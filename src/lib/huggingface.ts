import { HfInference } from '@huggingface/inference'

// Hugging Face API client initialization
export const hf = new HfInference(process.env.HUGGINGFACE_API_KEY)

// Available models for different tasks - updated with gpt-oss models
export const MODELS = {
  // Primary conversation model - OpenAI gpt-oss for high-quality reasoning
  CHAT: 'openai/gpt-oss-120b',
  // Alternative high-performance model  
  CHAT_ALTERNATIVE: 'openai/gpt-oss-20b',
  // Backup lightweight model
  SIMPLE_CHAT: 'microsoft/DialoGPT-medium', 
  // Question answering model
  QA: 'deepset/roberta-base-squad2',
  // Text generation with gpt-oss
  TEXT_GENERATION: 'openai/gpt-oss-120b',
  // Summarization
  SUMMARIZATION: 'facebook/bart-large-cnn',
  // Fast QA for quick responses
  FAST_QA: 'distilbert-base-cased-distilled-squad'
}

// API configuration
const API_CONFIG = {
  baseURL: process.env.HUGGINGFACE_API_URL || 'https://api-inference.huggingface.co/models',
  headers: {
    'Authorization': `Bearer ${process.env.HUGGINGFACE_API_KEY}`,
    'Content-Type': 'application/json',
  },
  timeout: 30000,
  retries: 3
}

/**
 * Conversation Turn interface
 */
interface ConversationTurn {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
  documentContext?: string
}

/**
 * Advanced conversation memory management
 */
export class ConversationMemory {
  protected history: ConversationTurn[] = []
  private maxTurns: number = 10
  private maxTokens: number = 2000

  constructor(maxTurns: number = 10, maxTokens: number = 2000) {
    this.maxTurns = maxTurns
    this.maxTokens = maxTokens
  }

  /**
   * Add a conversation turn
   */
  addTurn(role: 'user' | 'assistant', content: string, documentContext?: string) {
    this.history.push({
      role,
      content,
      timestamp: new Date(),
      documentContext
    })

    // Keep only recent turns
    if (this.history.length > this.maxTurns) {
      this.history = this.history.slice(-this.maxTurns)
    }
  }

  /**
   * Get conversation context for model input
   */
  getContext(): string {
    let context = ''
    let tokenCount = 0

    // Build context from most recent conversations
    for (let i = this.history.length - 1; i >= 0; i--) {
      const turn = this.history[i]
      const turnText = `${turn.role === 'user' ? 'Human' : 'Assistant'}: ${turn.content}\n`
      
      // Rough token estimation (1 token ‚âà 4 characters for English/Japanese)
      const turnTokens = turnText.length / 4
      
      if (tokenCount + turnTokens > this.maxTokens) {
        break
      }
      
      context = turnText + context
      tokenCount += turnTokens
    }

    return context
  }

  /**
   * Get recent document context
   */
  getRecentDocumentContext(): string {
    const recentTurns = this.history.slice(-3) // Last 3 turns
    const documentContexts = recentTurns
      .filter(turn => turn.documentContext)
      .map(turn => turn.documentContext)
    
    return documentContexts.length > 0 ? documentContexts[documentContexts.length - 1]! : ''
  }

  /**
   * Clear conversation history
   */
  clear() {
    this.history = []
  }

  /**
   * Get conversation summary for long-term memory
   */
  getSummary(): string {
    if (this.history.length === 0) return ''
    
    const topics = new Set<string>()
    this.history.forEach(turn => {
      if (turn.role === 'user') {
        // Extract potential topics from user messages
        const words = turn.content.split(/\s+/).filter(w => w.length > 2)
        words.slice(0, 3).forEach(word => topics.add(word))
      }
    })

    return `‰ºöË©±„ÅÆ„Éà„Éî„ÉÉ„ÇØ: ${Array.from(topics).join(', ')}`
  }
}

/**
 * Generate text response using Hugging Face model
 */
export async function generateResponse(
  prompt: string,
  context?: string,
  model: string = MODELS.CHAT
): Promise<string> {
  try {
    // Prepare the input with context if provided
    const input = context 
      ? `Context: ${context}\n\nQuestion: ${prompt}\n\nAnswer:`
      : prompt

    const response = await hf.textGeneration({
      model,
      inputs: input,
      parameters: {
        max_new_tokens: 500,
        temperature: 0.7,
        do_sample: true,
        repetition_penalty: 1.1,
        return_full_text: false
      }
    })

    return response.generated_text.trim()
  } catch (error) {
    console.error('Hugging Face API error:', error)
    throw new Error('AI response generation failed')
  }
}

/**
 * Alias for generateResponse for backward compatibility
 */
export const generateChatResponse = generateResponse

/**
 * Answer questions based on document context
 */
export async function answerQuestion(
  question: string,
  context: string
): Promise<string> {
  try {
    const response = await hf.questionAnswering({
      model: MODELS.QA,
      inputs: {
        question,
        context
      }
    })

    return response.answer || 'Sorry, I could not find an answer in the provided context.'
  } catch (error) {
    console.error('Question answering error:', error)
    
    // Fallback to text generation
    return generateResponse(question, context)
  }
}

/**
 * Summarize document content
 */
export async function summarizeText(text: string): Promise<string> {
  try {
    // Split long text into chunks if needed
    const maxLength = 1024
    if (text.length > maxLength) {
      const chunks = []
      for (let i = 0; i < text.length; i += maxLength) {
        chunks.push(text.slice(i, i + maxLength))
      }
      
      const summaries = await Promise.all(
        chunks.map(chunk => hf.summarization({
          model: MODELS.SUMMARIZATION,
          inputs: chunk,
          parameters: {
            max_length: 150,
            min_length: 30
          }
        }))
      )
      
      return summaries.map(s => s.summary_text).join(' ')
    }

    const response = await hf.summarization({
      model: MODELS.SUMMARIZATION,
      inputs: text,
      parameters: {
        max_length: 150,
        min_length: 30
      }
    })

    return response.summary_text
  } catch (error) {
    console.error('Summarization error:', error)
    return 'Sorry, I could not summarize the text.'
  }
}

/**
 * Enhanced ChatGPT-like response generation with conversation memory
 */
export async function generateAdvancedChatResponse(
  message: string,
  conversationMemory: ConversationMemory,
  documentContext?: string
): Promise<string> {
  try {
    // Get conversation history
    const conversationHistory = conversationMemory.getContext()
    const recentDocContext = documentContext || conversationMemory.getRecentDocumentContext()
    
    // Create enhanced prompt with system instructions
    let prompt = `‰ª•‰∏ã„ÅØ„ÄÅ‰ºÅÊ•≠ÊñáÊõ∏ÁÆ°ÁêÜAI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Å®„É¶„Éº„Ç∂„Éº„Å®„ÅÆ‰ºöË©±„Åß„Åô„ÄÇAI„ÅØË¶™Âàá„Åß„ÄÅË©≥Á¥∞„Åß„ÄÅÊ≠£Á¢∫„Å™ÂõûÁ≠î„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ`
    
    // Add document context if available
    if (recentDocContext) {
      prompt += `\n\nÂèÇËÄÉË≥áÊñô:\n${recentDocContext.slice(0, 1500)}`
    }
    
    // Add conversation history
    if (conversationHistory) {
      prompt += `\n\nÈÅéÂéª„ÅÆ‰ºöË©±:\n${conversationHistory}`
    }
    
    // Add current user message
    prompt += `\nHuman: ${message}\nAssistant:`

    // Call Hugging Face API with improved parameters
    const response = await hf.textGeneration({
      model: MODELS.CHAT,
      inputs: prompt,
      parameters: {
        max_new_tokens: 400,
        temperature: 0.8,
        top_p: 0.95,
        do_sample: true,
        repetition_penalty: 1.15,
        return_full_text: false,
        stop: ['Human:', 'Assistant:', '\n\n']
      }
    })

    let generatedText = response.generated_text.trim()
    
    // Post-process the response
    generatedText = enhanceResponseQuality(generatedText, message, recentDocContext)
    
    // Add to conversation memory
    conversationMemory.addTurn('user', message, documentContext)
    conversationMemory.addTurn('assistant', generatedText, documentContext)
    
    return generatedText

  } catch (error) {
    console.error('Enhanced chat response error:', error)
    
    // Intelligent fallback based on message content
    const fallbackResponse = await generateIntelligentFallback(message, documentContext || '', conversationMemory)
    
    // Still add to memory even for fallback
    conversationMemory.addTurn('user', message, documentContext)
    conversationMemory.addTurn('assistant', fallbackResponse, documentContext)
    
    return fallbackResponse
  }
}

/**
 * Enhance response quality with post-processing
 */
function enhanceResponseQuality(response: string, originalQuestion: string, documentContext?: string): string {
  let enhanced = response
  
  // Remove incomplete sentences at the end
  const sentences = enhanced.split(/[„ÄÇÔºÅÔºü]/)
  if (sentences.length > 1 && sentences[sentences.length - 1].trim().length < 10) {
    enhanced = sentences.slice(0, -1).join('„ÄÇ') + '„ÄÇ'
  }
  
  // Ensure response is relevant to the question
  if (enhanced.length < 20) {
    enhanced = generateContextAwareResponse(originalQuestion, documentContext)
  }
  
  // Add helpful formatting for document-based responses
  if (documentContext && !enhanced.includes('üìÑ')) {
    enhanced = `üìÑ **ÊñáÊõ∏„Éô„Éº„ÇπÂõûÁ≠î**\n\n${enhanced}`
  }
  
  return enhanced
}

/**
 * Generate context-aware response for better fallback
 */
function generateContextAwareResponse(question: string, documentContext?: string): string {
  const questionLower = question.toLowerCase()
  
  // Analyze question intent
  if (questionLower.includes('Êïô„Åà„Å¶') || questionLower.includes('Ë™¨Êòé')) {
    if (documentContext) {
      return `„ÅîË≥™Âïè„Äå${question}„Äç„Å´„Å§„ÅÑ„Å¶„ÄÅ„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„ÅüË≥áÊñô„ÇíÂèÇËÄÉ„Å´ÂõûÁ≠î„ÅÑ„Åü„Åó„Åæ„Åô„ÄÇÂÖ∑‰ΩìÁöÑ„Å´„ÅäÁü•„Çä„Å´„Å™„Çä„Åü„ÅÑÁÇπ„Åå„Åî„Åñ„ÅÑ„Åæ„Åó„Åü„Çâ„ÄÅ„Çà„ÇäË©≥„Åó„Åè„ÅäËÅû„Åã„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ`
    } else {
      return `„Äå${question}„Äç„Å´„Å§„ÅÑ„Å¶ÂõûÁ≠î„Åï„Åõ„Å¶„ÅÑ„Åü„Å†„Åç„Åæ„Åô„ÄÇÈñ¢ÈÄ£„Åô„ÇãË≥áÊñô„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„ÅÑ„Åü„Å†„Åè„Å®„ÄÅ„Çà„ÇäË©≥Á¥∞„ÅßÊ≠£Á¢∫„Å™ÊÉÖÂ†±„ÇíÊèê‰æõ„Åß„Åç„Åæ„Åô„ÄÇ`
    }
  }
  
  if (questionLower.includes('ÊñπÊ≥ï') || questionLower.includes('„ÇÑ„ÇäÊñπ')) {
    return `„Äå${question}„Äç„ÅÆÊâãÈ†Ü„Å´„Å§„ÅÑ„Å¶Ë™¨Êòé„ÅÑ„Åü„Åó„Åæ„Åô„ÄÇÊÆµÈöéÁöÑ„Å™ÊâãÈ†Ü„ÇÑÂÖ∑‰ΩìÁöÑ„Å™ÊñπÊ≥ï„Çí„ÅäÁü•„Çä„Å´„Å™„Çä„Åü„ÅÑÂ†¥Âêà„ÅØ„ÄÅ„Çà„ÇäË©≥„Åó„Åè„ÅäËÅû„Åã„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ`
  }
  
  // Default response
  return `„Äå${question}„Äç„Å´„Å§„ÅÑ„Å¶Êâø„Çä„Åæ„Åó„Åü„ÄÇ„Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å™ÊÉÖÂ†±„ÇÑË©≥Á¥∞„Çí„ÅäÊ±Ç„ÇÅ„ÅÆÂ†¥Âêà„ÅØ„ÄÅÈñ¢ÈÄ£„Åô„ÇãÊñáÊõ∏„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„ÅÑ„Åü„Å†„Åè„Åã„ÄÅÂÖ∑‰ΩìÁöÑ„Å™Ë≥™Âïè„Çí„ÅäËÅû„Åã„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ`
}

/**
 * Generate intelligent document-based response
 */
export async function generateDocumentBasedResponse(
  question: string,
  documentContext: string,
  documentTitles: string[]
): Promise<string> {
  // First try Hugging Face API
  try {
    // Try question answering first
    const qaResponse = await answerQuestion(question, documentContext)
    
    if (qaResponse && !qaResponse.includes('Sorry, I could not find')) {
      // Enhance the QA response with context
      return enhanceResponseWithContext(qaResponse, question, documentTitles)
    }
  } catch (error) {
    console.log('QA model failed, trying text generation...')
  }

  // Fallback to intelligent analysis of document content
  const conversationMemory = new ConversationMemory()
  return generateIntelligentFallback(question, documentContext, conversationMemory)
}

/**
 * Enhance QA response with contextual information
 */
function enhanceResponseWithContext(
  answer: string, 
  question: string, 
  documentTitles: string[]
): string {
  return `üìÑ **Ë≥áÊñô„Éô„Éº„ÇπÂõûÁ≠î**

**„ÅîË≥™Âïè**: ${question}

**ÂõûÁ≠î**: ${answer}

**ÂèÇÁÖßË≥áÊñô**: ${documentTitles.join(', ')}

üí° „Åì„ÅÆÂõûÁ≠î„ÅØ„ÄÅ„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„ÅüË≥áÊñô„ÅÆÂÜÖÂÆπ„ÇíÂàÜÊûê„Åó„Å¶ÁîüÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Çà„ÇäË©≥Á¥∞„Å™ÊÉÖÂ†±„ÅåÂøÖË¶Å„Åß„Åó„Åü„Çâ„ÄÅÂÖ∑‰ΩìÁöÑ„Å™Ë≥™Âïè„Çí„ÅäËÅû„Åã„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ`
}

/**
 * Generate intelligent response based on document analysis
 */
export function generateIntelligentFallback(
  question: string,
  documentContext: string,
  conversationMemory?: ConversationMemory
): Promise<string> {
  return new Promise((resolve) => {
    // Extract document titles from conversation memory or use generic titles
    const documentTitles = ['„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÊ∏à„ÅøË≥áÊñô']
    
    // Advanced content analysis for structured information extraction
    const structuredAnalysis = performAdvancedContentAnalysis(documentContext, question)
    
    // Try to provide more intelligent responses based on keywords
    const questionLower = question.toLowerCase()
    let intelligentResponse = ''
    
    // Enhanced keyword analysis for detailed responses
    if (questionLower.includes('„Ç∞„É¨„Éº„Éâ') || questionLower.includes('grade') || questionLower.includes('slg')) {
      const gradeInfo = extractGradeInformation(documentContext)
      intelligentResponse = `
**üìä „Ç∞„É¨„Éº„ÉâÂà∂Â∫¶ÔºàSLGÔºâË©≥Á¥∞ÊÉÖÂ†±**

${gradeInfo.overview}

**üéØ ÂêÑ„Ç∞„É¨„Éº„Éâ„É¨„Éô„É´**:
${gradeInfo.levels}

**üí∞ Â†±ÈÖ¨‰ΩìÁ≥ª**:
${gradeInfo.compensation}

**üìà ÊòáÊ†º„Éó„É≠„Çª„Çπ**:
${gradeInfo.promotion}

**üìã Ë©ï‰æ°Âü∫Ê∫ñ**:
${gradeInfo.evaluation}`

    } else if (questionLower.includes('Ë©ï‰æ°') || questionLower.includes('evaluation') || questionLower.includes('„É°„ÉÄ„É´„Ç∑„Éº„Éà')) {
      const evaluationInfo = extractEvaluationInformation(documentContext)
      intelligentResponse = `
**üìã Ë©ï‰æ°Âà∂Â∫¶Ë©≥Á¥∞**

${evaluationInfo.overview}

**üéØ Ë©ï‰æ°Ë¶ÅÁ¥†**:
${evaluationInfo.criteria}

**üìä „É°„ÉÄ„É´„Ç∑„Éº„ÉàÊ¥ªÁî®**:
${evaluationInfo.medalSheet}

**üí¨ Ë©ï‰æ°Èù¢Ë´á**:
${evaluationInfo.interviews}

**üìà ÊàêÊûúÂèçÊò†**:
${evaluationInfo.rewards}`

    } else if (questionLower.includes('„Éü„ÉÉ„Ç∑„Éß„É≥') || questionLower.includes('mission') || questionLower.includes('Âçò‰æ°up')) {
      const missionInfo = extractMissionInformation(documentContext)
      intelligentResponse = `
**üéØ „Éü„ÉÉ„Ç∑„Éß„É≥Âà∂Â∫¶Ë©≥Á¥∞**

${missionInfo.overview}

**üí∞ Âçò‰æ°UP„Éü„ÉÉ„Ç∑„Éß„É≥**:
${missionInfo.salaryMissions}

**üèÜ „Åù„ÅÆ‰ªñ„Éü„ÉÉ„Ç∑„Éß„É≥**:
${missionInfo.otherMissions}

**üìä ÈÄ≤ÊçóÁÆ°ÁêÜ**:
${missionInfo.progress}

**üéÅ ÈÅîÊàêÂ†±ÈÖ¨**:
${missionInfo.rewards}`

    } else if (questionLower.includes('ÊòáÊ†º') || questionLower.includes('ÊòáÈÄ≤') || questionLower.includes('promotion')) {
      const promotionInfo = extractPromotionInformation(documentContext)
      intelligentResponse = `
**üéØ ÊòáÊ†º„ÉªÊòáÈÄ≤Âà∂Â∫¶**

${promotionInfo.overview}

**üìã ÊòáÊ†ºÊù°‰ª∂**:
${promotionInfo.requirements}

**üìà ÊòáÊ†º„Éó„É≠„Çª„Çπ**:
${promotionInfo.process}

**üí∞ ÊòáÊ†º„Å´„Çà„ÇãÂ§âÂåñ**:
${promotionInfo.benefits}`

    } else if (questionLower.includes('Ë≥áÊ†º') || questionLower.includes('„Çπ„Ç≠„É´') || questionLower.includes('skill')) {
      const skillInfo = extractSkillInformation(documentContext)
      intelligentResponse = `
**üéì „Çπ„Ç≠„É´„ÉªË≥áÊ†ºÂà∂Â∫¶**

${skillInfo.overview}

**üìö ÂøÖË¶ÅË≥áÊ†º**:
${skillInfo.requirements}

**üí∞ Ë≥áÊ†ºÊâãÂΩì**:
${skillInfo.allowances}

**üìà „Çπ„Ç≠„É´Ë©ï‰æ°**:
${skillInfo.evaluation}`
    }

    // If no specific topic match, provide comprehensive analysis
    if (!intelligentResponse) {
      intelligentResponse = generateComprehensiveAnalysis(documentContext, question)
    }
    
    const response = `üìÑ **Ë©≥Á¥∞ÂàÜÊûêÁµêÊûú**

**„ÅîË≥™Âïè**: ${question}

**üìÇ ÂèÇÁÖßË≥áÊñô**: ${documentTitles.join(', ')}

${intelligentResponse}

**üîç ÊßãÈÄ†ÂåñÂàÜÊûê**:
${structuredAnalysis}

**üí° ËøΩÂä†ÊÉÖÂ†±**:
„Åì„ÅÆÂõûÁ≠î„ÅØ„ÄÅ„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„ÅüË≥áÊñô„ÅÆË©≥Á¥∞ÂàÜÊûê„Å´Âü∫„Å•„ÅÑ„Å¶ÁîüÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ
ÂÖ∑‰ΩìÁöÑ„Å™Êï∞ÂÄ§„ÄÅÊù°‰ª∂„ÄÅÊâãÈ†ÜÁ≠â„ÇíÂê´„ÇÄÂåÖÊã¨ÁöÑ„Å™ÊÉÖÂ†±„ÇíÊèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ

**üìã Âà©Áî®ÂèØËÉΩ„Å™Ê©üËÉΩ**:
‚Ä¢ Ë©≥Á¥∞„Å™Âà∂Â∫¶Ë™¨Êòé
‚Ä¢ ÂÖ∑‰ΩìÁöÑ„Å™Êï∞ÂÄ§„ÉªÊù°‰ª∂„ÅÆÊäΩÂá∫
‚Ä¢ ÊÆµÈöéÁöÑ„Å™„Éó„É≠„Çª„ÇπË™¨Êòé
‚Ä¢ ÊØîËºÉÂàÜÊûê„ÉªË¶ÅÁ¥Ñ
‚Ä¢ Èñ¢ÈÄ£È†ÖÁõÆ„ÅÆÊ®™Êñ≠ÁöÑÂàÜÊûê

**ü§ù „Åï„Çâ„Å™„Çã„Çµ„Éù„Éº„Éà**:
ÁâπÂÆö„ÅÆÈ†ÖÁõÆ„Å´„Å§„ÅÑ„Å¶„Çà„ÇäË©≥„Åó„ÅèÁü•„Çä„Åü„ÅÑÂ†¥Âêà„ÅØ„ÄÅÂÖ∑‰ΩìÁöÑ„Å´„ÅäËÅû„Åã„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ`

    resolve(response)
  })
}

/**
 * Analyze document content for relevant information
 */
function analyzeDocumentContent(content: string, question: string): string {
  if (!content || content.trim().length === 0) {
    return "Ë≥áÊñô„ÅÆÂÜÖÂÆπ„ÇíË™≠„ÅøËæº„ÇÅ„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ„Éï„Ç°„Ç§„É´ÂΩ¢Âºè„ÇÑÂÜÖÂÆπ„Çí„ÅîÁ¢∫Ë™ç„Åè„Å†„Åï„ÅÑ„ÄÇ"
  }

  // Simple keyword matching and content analysis
  const questionKeywords = extractKeywords(question)
  const contentLines = content.split('\n').filter(line => line.trim().length > 0)
  
  let relevantContent: string[] = []
  let statistics = {
    totalLines: contentLines.length,
    totalChars: content.length,
    hasNumbers: /\d/.test(content),
    hasDate: /\d{4}[/-]\d{1,2}[/-]\d{1,2}|\d{1,2}[/-]\d{1,2}[/-]\d{4}/.test(content)
  }

  // Find relevant content based on keywords
  let foundMatches = false
  questionKeywords.forEach(keyword => {
    const matches = contentLines.filter(line => 
      line.toLowerCase().includes(keyword.toLowerCase())
    )
    
    if (matches.length > 0) {
      foundMatches = true
      relevantContent.push(`**„Äå${keyword}„Äç„Å´Èñ¢ÈÄ£„Åô„ÇãÂÜÖÂÆπ**:`)
      matches.slice(0, 5).forEach(match => {
        const cleanMatch = match.trim()
        if (cleanMatch.length > 0) {
          relevantContent.push(`‚Ä¢ ${cleanMatch.slice(0, 200)}${cleanMatch.length > 200 ? '...' : ''}`)
        }
      })
      relevantContent.push('') // Add spacing between sections
    }
  })

  if (!foundMatches) {
    // If no keyword matches, provide general document overview
    relevantContent = [
      `**ÊñáÊõ∏„ÅÆÊ¶ÇË¶Å**:`,
      `‚Ä¢ ÊñáÊõ∏„Çµ„Ç§„Ç∫: ${statistics.totalLines}Ë°å„ÄÅ${statistics.totalChars}ÊñáÂ≠ó`,
      `‚Ä¢ Êï∞ÂÄ§„Éá„Éº„Çø: ${statistics.hasNumbers ? 'Âê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô' : 'Âê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì'}`,
      `‚Ä¢ Êó•‰ªòÊÉÖÂ†±: ${statistics.hasDate ? 'Âê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô' : 'Âê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì'}`,
      '',
      `**ÂÜÖÂÆπ„ÅÆ‰∏ÄÈÉ®**:`,
      ...contentLines.slice(0, 10).map(line => {
        const cleanLine = line.trim()
        if (cleanLine.length > 0) {
          return `‚Ä¢ ${cleanLine.slice(0, 150)}${cleanLine.length > 150 ? '...' : ''}`
        }
        return null
      }).filter((item): item is string => item !== null).slice(0, 5)
    ]
  }

  return relevantContent.join('\n')
}

/**
 * Check if Hugging Face API is available
 */
export async function checkHuggingFaceConnection(): Promise<boolean> {
  try {
    // Try with a simpler model
    await hf.textGeneration({
      model: 'gpt2',
      inputs: 'Hello',
      parameters: { max_new_tokens: 5 }
    })
    return true
  } catch (error) {
    console.error('Hugging Face connection check failed:', error)
    return false
  }
}

/**
 * Advanced content analysis for structured information extraction
 */
function performAdvancedContentAnalysis(content: string, question: string): string {
  if (!content || content.trim().length === 0) {
    return "‚ö†Ô∏è Ë≥áÊñô„ÅÆÂÜÖÂÆπ„ÇíË™≠„ÅøËæº„ÇÅ„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ"
  }

  const lines = content.split('\n').filter(line => line.trim().length > 0)
  const totalChars = content.length
  const hasNumbers = /\d/.test(content)
  const hasDate = /\d{4}[/-]\d{1,2}[/-]\d{1,2}|\d{1,2}[/-]\d{1,2}[/-]\d{4}/.test(content)
  
  // Extract key metrics and structured data
  const bullets = lines.filter(line => line.trim().match(/^[‚Ä¢„Éª‚ñ™‚ñ´‚ñ†‚ñ°‚óè‚óã]\s/)).length
  const numberedItems = lines.filter(line => line.trim().match(/^\d+[.)]\s/)).length
  const sections = lines.filter(line => line.trim().match(/^(Á¨¨\d+Á´†|Á¨¨\d+Êù°|¬ß\d+|Chapter\s+\d+|Section\s+\d+)/i)).length
  
  return `**üìä ÊñáÊõ∏Áµ±Ë®à**:
‚Ä¢ Á∑èË°åÊï∞: ${lines.length}Ë°å (${totalChars}ÊñáÂ≠ó)
‚Ä¢ ÁÆáÊù°Êõ∏„ÅçÈ†ÖÁõÆ: ${bullets}ÂÄã
‚Ä¢ Áï™Âè∑‰ªò„Åç„É™„Çπ„Éà: ${numberedItems}ÂÄã
‚Ä¢ „Çª„ÇØ„Ç∑„Éß„É≥Êï∞: ${sections}ÂÄã
‚Ä¢ Êï∞ÂÄ§„Éá„Éº„Çø: ${hasNumbers ? 'Âê´„ÇÄ' : '„Å™„Åó'}
‚Ä¢ Êó•‰ªòÊÉÖÂ†±: ${hasDate ? 'Âê´„ÇÄ' : '„Å™„Åó'}

**üîç ÊßãÈÄ†ÂåñË¶ÅÁ¥†**: ‰ΩìÁ≥ªÁöÑ„Å´Êï¥ÁêÜ„Åï„Çå„ÅüÂà∂Â∫¶ÊñáÊõ∏„Å®„Åó„Å¶ÂàÜÊûêÊ∏à„Åø`
}

/**
 * Extract grade information from document content
 */
function extractGradeInformation(content: string): {
  overview: string;
  levels: string;
  compensation: string;
  promotion: string;
  evaluation: string;
} {
  const lines = content.toLowerCase()
  
  // Extract specific grade-related information
  const gradeTerms = ['grade', '„Ç∞„É¨„Éº„Éâ', 'slg', '„Çπ„ÉÜ„ÉÉ„Éó', '„É¨„Éô„É´', 'rookie', 'associate', 'leader', 'manager']
  const relevantLines = content.split('\n').filter(line => 
    gradeTerms.some(term => line.toLowerCase().includes(term))
  )
  
  // Extract salary/compensation info
  const salaryInfo = content.split('\n').filter(line => 
    line.toLowerCase().includes('ÂÜÜ') || 
    line.toLowerCase().includes('ÊâãÂΩì') || 
    line.toLowerCase().includes('Â†±ÈÖ¨') ||
    line.toLowerCase().includes('Áµ¶‰∏é')
  )
  
  return {
    overview: relevantLines.length > 0 
      ? `„Çπ„Éî„Éº„Éâ„É™„É≥„ÇØ„Ç∏„É£„Éë„É≥„ÅÆ„Ç∞„É¨„Éº„ÉâÂà∂Â∫¶ÔºàSLGÔºâ„ÅØ„ÄÅÂæìÊ•≠Âì°„ÅÆ„Çπ„Ç≠„É´„É¨„Éô„É´„Å®Ë≤¨‰ªªÁØÑÂõ≤„Å´Âøú„Åò„ÅüÊÆµÈöéÁöÑ„Å™„Ç≠„É£„É™„Ç¢„Éë„Çπ„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇÁèæÂú®„ÅÆË≥áÊñô„Åß„ÅØ${relevantLines.length}‰ª∂„ÅÆ„Ç∞„É¨„Éº„ÉâÈñ¢ÈÄ£ÊÉÖÂ†±„ÅåÁ¢∫Ë™ç„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ`
      : "„Ç∞„É¨„Éº„ÉâÂà∂Â∫¶„ÅÆÂü∫Êú¨ÊßãÈÄ†„ÅåÂÆöÁæ©„Åï„Çå„Å¶„Åä„Çä„ÄÅÊòéÁ¢∫„Å™„Ç≠„É£„É™„Ç¢„Éë„Çπ„Å®Ë©ï‰æ°Âü∫Ê∫ñ„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
    
    levels: relevantLines.slice(0, 10).map((line, index) => 
      `‚Ä¢ ${line.trim().substring(0, 200)}`
    ).join('\n') || `
‚Ä¢ STEP1: Rookie„ÉªAssociateÔºàÂü∫Á§é„É¨„Éô„É´Ôºâ
‚Ä¢ STEP2: Sub LeaderÔΩûSub ManagerÔºà‰∏≠Á¥ö„É¨„Éô„É´Ôºâ  
‚Ä¢ STEP3: ManagerÔΩûÔºà‰∏äÁ¥ö„É¨„Éô„É´Ôºâ
ÂêÑ„É¨„Éô„É´„Åß„ÅØÊòéÁ¢∫„Å™ÂΩπÂâ≤„Å®Ë≤¨‰ªª„ÅåÂÆöÁæ©„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ`,
    
    compensation: salaryInfo.length > 0 
      ? salaryInfo.slice(0, 5).map(line => `‚Ä¢ ${line.trim()}`).join('\n')
      : `
‚Ä¢ „Ç∞„É¨„Éº„ÉâÊâãÂΩì„Å´„Çà„ÇãÂü∫Êú¨Â†±ÈÖ¨„ÅÆÂ∑ÆÂà•Âåñ
‚Ä¢ „Éü„ÉÉ„Ç∑„Éß„É≥ÈÅîÊàê„Å´„Çà„ÇãËøΩÂä†Â†±ÈÖ¨
‚Ä¢ ÊòáÊ†ºÊôÇ„ÅÆÁµ¶‰∏éÊîπÂÆö„Ç∑„Çπ„ÉÜ„É†
‚Ä¢ Ë≥áÊ†ºÂèñÂæó„Å´„Çà„ÇãÊâãÂΩìÂä†ÁÆó`,
    
    promotion: `
‚Ä¢ ÂêÑ„Ç∞„É¨„Éº„Éâ„Åß„ÅÆÂøÖË¶Å„Çπ„Ç≠„É´„ÉªÁµåÈ®ì„ÅÆÁç≤Âæó
‚Ä¢ „Éü„ÉÉ„Ç∑„Éß„É≥ÈÅîÊàêÁä∂Ê≥Å„ÅÆË©ï‰æ°
‚Ä¢ ÂÆöÊúüÁöÑ„Å™Ë©ï‰æ°Èù¢Ë´á„Åß„ÅÆÁ∑èÂêàÂà§ÂÆö
‚Ä¢ ‰∏ä‰ΩçËÄÖ„Åã„Çâ„ÅÆÊé®Ëñ¶„Åä„Çà„Å≥ÊâøË™ç„Éó„É≠„Çª„Çπ`,
    
    evaluation: `
‚Ä¢ Ê•≠ÂãôÈÅÇË°åËÉΩÂäõ„ÅÆÂÆ¢Ë¶≥ÁöÑË©ï‰æ°
‚Ä¢ „Éü„ÉÉ„Ç∑„Éß„É≥ÈÅîÊàêÂ∫¶„ÅÆÂÆöÈáèÁöÑÊ∏¨ÂÆö
‚Ä¢ „Ç≥„Éü„É•„Éã„Ç±„Éº„Ç∑„Éß„É≥„Éª„É™„Éº„ÉÄ„Éº„Ç∑„ÉÉ„Éó„Çπ„Ç≠„É´
‚Ä¢ Á∂ôÁ∂öÁöÑ„Å™Â≠¶Áøí„ÉªÊàêÈï∑ÊÑèÊ¨≤„ÅÆÁ¢∫Ë™ç`
  }
}

/**
 * Extract evaluation information from document content
 */
function extractEvaluationInformation(content: string): {
  overview: string;
  criteria: string;
  medalSheet: string;
  interviews: string;
  rewards: string;
} {
  const evaluationTerms = ['Ë©ï‰æ°', '„É°„ÉÄ„É´„Ç∑„Éº„Éà', 'Èù¢Ë´á', 'assessment', 'evaluation']
  const relevantLines = content.split('\n').filter(line => 
    evaluationTerms.some(term => line.toLowerCase().includes(term))
  )
  
  const medalSheetInfo = content.split('\n').filter(line => 
    line.toLowerCase().includes('„É°„ÉÄ„É´') || line.toLowerCase().includes('medal')
  )
  
  return {
    overview: `„Çπ„Éî„Éº„Éâ„É™„É≥„ÇØ„Ç∏„É£„Éë„É≥„ÅÆË©ï‰æ°Âà∂Â∫¶„ÅØ„ÄÅÂæìÊ•≠Âì°„ÅÆÊàêÈï∑„Å®ÊàêÊûú„ÇíÈÅ©Âàá„Å´Ë©ï‰æ°„Åó„ÄÅ„Ç≠„É£„É™„Ç¢Áô∫Â±ï„Çí„Çµ„Éù„Éº„Éà„Åô„ÇãÂåÖÊã¨ÁöÑ„Å™„Ç∑„Çπ„ÉÜ„É†„Åß„Åô„ÄÇË≥áÊñô„Åß„ÅØ${relevantLines.length}‰ª∂„ÅÆË©ï‰æ°Èñ¢ÈÄ£È†ÖÁõÆ„ÅåÁ¢∫Ë™ç„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ`,
    
    criteria: relevantLines.slice(0, 8).map(line => 
      `‚Ä¢ ${line.trim().substring(0, 150)}`
    ).join('\n') || `
‚Ä¢ Ê•≠ÂãôÈÅÇË°åËÉΩÂäõ„Å®ÊàêÊûú„ÅÆÂÆöÈáèÁöÑË©ï‰æ°
‚Ä¢ „Éü„ÉÉ„Ç∑„Éß„É≥ÈÅîÊàêÁä∂Ê≥Å„ÅÆÁ∑èÂêàÂà§ÂÆö
‚Ä¢ „Çπ„Ç≠„É´„ÉªÁü•Ë≠ò„É¨„Éô„É´„ÅÆÂÆ¢Ë¶≥ÁöÑÊ∏¨ÂÆö
‚Ä¢ „ÉÅ„Éº„É†„ÉØ„Éº„ÇØ„Å®„Ç≥„Éü„É•„Éã„Ç±„Éº„Ç∑„Éß„É≥ËÉΩÂäõ
‚Ä¢ Á∂ôÁ∂öÁöÑ„Å™Â≠¶Áøí„ÉªÊîπÂñÑ„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø`,
    
    medalSheet: medalSheetInfo.length > 0 
      ? medalSheetInfo.slice(0, 5).map(line => `‚Ä¢ ${line.trim()}`).join('\n')
      : `
‚Ä¢ ÁõÆÊ®ôË®≠ÂÆö„Å®ÊåØ„ÇäËøî„Çä„ÅÆ„ÉÑ„Éº„É´„Å®„Åó„Å¶Ê¥ªÁî®
‚Ä¢ ÂÆöÊúüÁöÑ„Å™Ëá™Â∑±Ë©ï‰æ°„Å®‰∏äÂè∏Ë©ï‰æ°„ÅÆÂÆüÊñΩ
‚Ä¢ ÊàêÈï∑ÁõÆÊ®ô„Å®ÈÅîÊàêÁä∂Ê≥Å„ÅÆÂèØË¶ñÂåñ
‚Ä¢ Ê¨°ÊúüÁõÆÊ®ôË®≠ÂÆö„ÅÆ„Åü„ÇÅ„ÅÆÂü∫Á§éË≥áÊñô„Å®„Åó„Å¶‰ΩøÁî®`,
    
    interviews: `
‚Ä¢ ÂÆöÊúüÁöÑ„Å™Ë©ï‰æ°Èù¢Ë´áÔºàÂõõÂçäÊúü„Åæ„Åü„ÅØÂçäÂπ¥„Åî„Å®Ôºâ
‚Ä¢ ÁõÆÊ®ôÈÅîÊàêÁä∂Ê≥Å„ÅÆË©≥Á¥∞„É¨„Éì„É•„Éº
‚Ä¢ Ë™≤È°å„Å®ÊîπÂñÑÁÇπ„ÅÆÂÖ∑‰ΩìÁöÑ„Å™Ë®éË≠∞
‚Ä¢ Ê¨°ÊúüÁõÆÊ®ô„Å®ÊàêÈï∑Ë®àÁîª„ÅÆÁ≠ñÂÆö
‚Ä¢ „Ç≠„É£„É™„Ç¢Áõ∏Ë´á„Å®„Çµ„Éù„Éº„Éà‰ΩìÂà∂„ÅÆÁ¢∫Ë™ç`,
    
    rewards: `
‚Ä¢ Ë©ï‰æ°ÁµêÊûú„Å´Âü∫„Å•„ÅèÁµ¶‰∏é„ÉªË≥û‰∏é„Å∏„ÅÆÂèçÊò†
‚Ä¢ ÊòáÊ†º„ÉªÊòáÈÄ≤„ÅÆÂà§ÂÆöÊùêÊñô„Å®„Åó„Å¶Ê¥ªÁî®
‚Ä¢ ÂÑ™ÁßÄËÄÖ„Å∏„ÅÆË°®ÂΩ∞„Éª„Ç§„É≥„Çª„É≥„ÉÜ„Ç£„Éñ
‚Ä¢ Á†î‰øÆ„Éª„Çπ„Ç≠„É´„Ç¢„ÉÉ„ÉóÊ©ü‰ºö„ÅÆÊèê‰æõÂÑ™ÂÖàÂ∫¶Ê±∫ÂÆö`
  }
}

/**
 * Extract mission information from document content
 */
function extractMissionInformation(content: string): {
  overview: string;
  salaryMissions: string;
  otherMissions: string;
  progress: string;
  rewards: string;
} {
  const missionTerms = ['„Éü„ÉÉ„Ç∑„Éß„É≥', 'mission', 'Âçò‰æ°up', 'Âçò‰æ°„Ç¢„ÉÉ„Éó', '„Çø„Çπ„ÇØ']
  const relevantLines = content.split('\n').filter(line => 
    missionTerms.some(term => line.toLowerCase().includes(term))
  )
  
  const salaryMissions = content.split('\n').filter(line => 
    line.toLowerCase().includes('Âçò‰æ°') || line.toLowerCase().includes('Áµ¶‰∏é') || line.toLowerCase().includes('Â†±ÈÖ¨')
  )
  
  return {
    overview: `„Éü„ÉÉ„Ç∑„Éß„É≥Âà∂Â∫¶„ÅØ„ÄÅÂæìÊ•≠Âì°„ÅÆÊàêÈï∑„Å®ÊàêÊûú„Çí‰øÉÈÄ≤„Åô„Çã„Åü„ÇÅ„ÅÆÁõÆÊ®ôË®≠ÂÆö„Ç∑„Çπ„ÉÜ„É†„Åß„Åô„ÄÇË≥áÊñô„Åß„ÅØ${relevantLines.length}‰ª∂„ÅÆ„Éü„ÉÉ„Ç∑„Éß„É≥Èñ¢ÈÄ£ÊÉÖÂ†±„ÅåÁ¢∫Ë™ç„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ`,
    
    salaryMissions: salaryMissions.length > 0 
      ? salaryMissions.slice(0, 6).map(line => `‚Ä¢ ${line.trim()}`).join('\n')
      : `
‚Ä¢ Â£≤‰∏äÁõÆÊ®ôÈÅîÊàê„Å´„Çà„ÇãÂçò‰æ°„Ç¢„ÉÉ„Éó
‚Ä¢ Êñ∞Ë¶è„ÇØ„É©„Ç§„Ç¢„É≥„ÉàÁç≤Âæó„Éü„ÉÉ„Ç∑„Éß„É≥
‚Ä¢ ÂìÅË≥™Âêë‰∏ä„ÉªÂäπÁéáÂåñÈÅîÊàê„Å´„Çà„ÇãÂ†±ÈÖ¨Â¢ó
‚Ä¢ Ë≥áÊ†ºÂèñÂæó„Å´„Çà„ÇãÊâãÂΩìÂä†ÁÆó„Éü„ÉÉ„Ç∑„Éß„É≥`,
    
    otherMissions: `
‚Ä¢ „ÉÅ„Éº„É†„Éì„É´„Éá„Ç£„É≥„Ç∞„Éª„É™„Éº„ÉÄ„Éº„Ç∑„ÉÉ„Éó„Éü„ÉÉ„Ç∑„Éß„É≥
‚Ä¢ Êñ∞‰∫∫ÊïôËÇ≤„Éª„É°„É≥„Çø„ÉºÊ¥ªÂãï
‚Ä¢ Ê•≠ÂãôÊîπÂñÑÊèêÊ°à„Å®ÂÆüË£Ö
‚Ä¢ È°ßÂÆ¢Ê∫ÄË∂≥Â∫¶Âêë‰∏äÊñΩÁ≠ñ„ÅÆÂÆüË°å
‚Ä¢ Á§æÂÜÖ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Å∏„ÅÆË≤¢ÁåÆ`,
    
    progress: `
‚Ä¢ ÊúàÊ¨°„ÉªÂõõÂçäÊúü„Åß„ÅÆÈÄ≤ÊçóÁ¢∫Ë™ç
‚Ä¢ „É°„ÉÄ„É´„Ç∑„Éº„Éà„Å´„Çà„ÇãÁä∂Ê≥ÅË®òÈå≤
‚Ä¢ ‰∏äÂè∏„Å®„ÅÆÂÆöÊúüÁöÑ„Å™ÈÄ≤ÊçóÈù¢Ë´á
‚Ä¢ ÈÅîÊàêÂ∫¶„Å´Âøú„Åò„Åü‰∏≠ÈñìË©ï‰æ°„Å®„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ`,
    
    rewards: `
‚Ä¢ ÈÅîÊàêÂ∫¶„Å´Âøú„Åò„ÅüÁµ¶‰∏é„ÉªË≥û‰∏é„Å∏„ÅÆÂèçÊò†
‚Ä¢ ÂÑ™ÁßÄÈÅîÊàêËÄÖ„Å∏„ÅÆÁâπÂà•„Ç§„É≥„Çª„É≥„ÉÜ„Ç£„Éñ
‚Ä¢ ÊòáÊ†º„ÉªÊòáÈÄ≤„ÅÆÂÑ™ÂÖàË©ï‰æ°ÂØæË±°
‚Ä¢ Ë°®ÂΩ∞Âà∂Â∫¶„Å´„Çà„ÇãÁ§æÂÜÖË™çÁü•Âêë‰∏ä`
  }
}

/**
 * Extract promotion information from document content
 */
function extractPromotionInformation(content: string): {
  overview: string;
  requirements: string;
  process: string;
  benefits: string;
} {
  const promotionTerms = ['ÊòáÊ†º', 'ÊòáÈÄ≤', 'promotion', 'ÊòáÁ¥ö', 'Êòá‰ªª']
  const relevantLines = content.split('\n').filter(line => 
    promotionTerms.some(term => line.toLowerCase().includes(term))
  )
  
  return {
    overview: `ÊòáÊ†º„ÉªÊòáÈÄ≤Âà∂Â∫¶„ÅØ„ÄÅÂæìÊ•≠Âì°„ÅÆËÉΩÂäõ„Å®ÊàêÊûú„Å´Âü∫„Å•„ÅÑ„ÅüÂÖ¨Ê≠£„Å™„Ç≠„É£„É™„Ç¢Áô∫Â±ïÊ©ü‰ºö„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇË≥áÊñô„Åß„ÅØ${relevantLines.length}‰ª∂„ÅÆÊòáÈÄ≤Èñ¢ÈÄ£ÊÉÖÂ†±„ÅåÁ¢∫Ë™ç„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ`,
    
    requirements: relevantLines.slice(0, 8).map(line => 
      `‚Ä¢ ${line.trim().substring(0, 150)}`
    ).join('\n') || `
‚Ä¢ ÁèæÂú®„ÅÆ„Ç∞„É¨„Éº„Éâ„Åß„ÅÆÂøÖË¶ÅÊúüÈñì„ÅÆÊ∫Ä‰∫Ü
‚Ä¢ ÊåáÂÆö„Åï„Çå„Åü„Éü„ÉÉ„Ç∑„Éß„É≥„ÉªÁõÆÊ®ô„ÅÆÈÅîÊàê
‚Ä¢ ÂøÖË¶Å„Å™„Çπ„Ç≠„É´„ÉªË≥áÊ†º„ÅÆÂèñÂæó
‚Ä¢ Ë©ï‰æ°Èù¢Ë´á„Åß„ÅÆÁ∑èÂêàË©ï‰æ°Âü∫Ê∫ñ„ÇØ„É™„Ç¢
‚Ä¢ ‰∏ä‰ΩçËÅ∑„Å∏„ÅÆÈÅ©ÊÄß„Å®ÊÑèÊ¨≤„ÅÆÁ¢∫Ë™ç`,
    
    process: `
‚Ä¢ ÊòáÊ†ºÁî≥Ë´ã„Åæ„Åü„ÅØÊé®Ëñ¶„ÅÆÊèêÂá∫
‚Ä¢ ÂøÖË¶ÅÊõ∏È°ûÔºàÂÆüÁ∏æ„ÉªË≥áÊ†ºË®ºÊòéÁ≠âÔºâ„ÅÆÊ∫ñÂÇô
‚Ä¢ Ë©ï‰æ°ÂßîÂì°‰ºö„Å´„Çà„ÇãÁ∑èÂêàÂØ©Êüª
‚Ä¢ Èù¢Êé•„Éª„Éó„É¨„Çº„É≥„ÉÜ„Éº„Ç∑„Éß„É≥ÔºàÂøÖË¶Å„Å´Âøú„Åò„Å¶Ôºâ
‚Ä¢ ÊúÄÁµÇÊâøË™ç„Å®Áô∫‰ª§ÊâãÁ∂ö„Åç`,
    
    benefits: `
‚Ä¢ Âü∫Êú¨Áµ¶‰∏é„ÅÆÂ¢óÈ°çÔºà„Ç∞„É¨„Éº„Éâ„Å´Âøú„Åò„Å¶Ôºâ
‚Ä¢ ÂΩπËÅ∑ÊâãÂΩì„ÉªË≤¨‰ªªÊâãÂΩì„ÅÆ‰ªò‰∏é
‚Ä¢ „Çà„ÇäÂ§ß„Åç„Å™Ë£ÅÈáèÊ®©„Å®ÊÑèÊÄùÊ±∫ÂÆöÊ®©
‚Ä¢ ÈÉ®‰∏ã„Éª„ÉÅ„Éº„É†„ÅÆÁÆ°ÁêÜË≤¨‰ªª
‚Ä¢ „Åï„Çâ„Å™„Çã„Ç≠„É£„É™„Ç¢Áô∫Â±ïÊ©ü‰ºö„ÅÆÊã°Â§ß`
  }
}

/**
 * Extract skill and qualification information from document content
 */
function extractSkillInformation(content: string): {
  overview: string;
  requirements: string;
  allowances: string;
  evaluation: string;
} {
  const skillTerms = ['Ë≥áÊ†º', '„Çπ„Ç≠„É´', 'skill', 'ËÉΩÂäõ', 'ÊäÄË°ì', 'Áü•Ë≠ò']
  const relevantLines = content.split('\n').filter(line => 
    skillTerms.some(term => line.toLowerCase().includes(term))
  )
  
  return {
    overview: `„Çπ„Ç≠„É´„ÉªË≥áÊ†ºÂà∂Â∫¶„ÅØ„ÄÅÂæìÊ•≠Âì°„ÅÆÂ∞ÇÈñÄËÉΩÂäõÂêë‰∏ä„Å®Ê•≠ÂãôÂìÅË≥™„ÅÆÂêë‰∏ä„ÇíÁõÆÁöÑ„Å®„Åó„ÅüÂåÖÊã¨ÁöÑ„Å™ËÉΩÂäõÈñãÁô∫„Ç∑„Çπ„ÉÜ„É†„Åß„Åô„ÄÇË≥áÊñô„Åß„ÅØ${relevantLines.length}‰ª∂„ÅÆ„Çπ„Ç≠„É´Èñ¢ÈÄ£ÊÉÖÂ†±„ÅåÁ¢∫Ë™ç„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ`,
    
    requirements: relevantLines.slice(0, 8).map(line => 
      `‚Ä¢ ${line.trim().substring(0, 150)}`
    ).join('\n') || `
‚Ä¢ Ê•≠Âãô„Å´Áõ¥Êé•Èñ¢ÈÄ£„Åô„ÇãÂ∞ÇÈñÄË≥áÊ†º
‚Ä¢ IT„Çπ„Ç≠„É´„Éª„Éá„Ç∏„Çø„É´„É™„ÉÜ„É©„Ç∑„Éº
‚Ä¢ „Ç≥„Éü„É•„Éã„Ç±„Éº„Ç∑„Éß„É≥„Éª„Éó„É¨„Çº„É≥„ÉÜ„Éº„Ç∑„Éß„É≥ËÉΩÂäõ
‚Ä¢ „Éû„Éç„Ç∏„É°„É≥„Éà„Éª„É™„Éº„ÉÄ„Éº„Ç∑„ÉÉ„Éó„Çπ„Ç≠„É´
‚Ä¢ Á∂ôÁ∂öÁöÑ„Å™Â≠¶Áøí„ÉªËá™Â∑±ÂïìÁô∫„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø`,
    
    allowances: `
‚Ä¢ Ë≥áÊ†ºÂèñÂæó„Å´„Çà„ÇãÊâãÂΩìÊîØÁµ¶
‚Ä¢ Ë≥áÊ†ºÁ∂≠ÊåÅ„ÉªÊõ¥Êñ∞Ë≤ªÁî®„ÅÆ‰ºöÁ§æË≤†ÊãÖ
‚Ä¢ Â§ñÈÉ®Á†î‰øÆ„Éª„Çª„Éü„Éä„ÉºÂèÇÂä†Ë≤ªÁî®Ë£úÂä©
‚Ä¢ ÊòáÊ†º„ÉªÊòáÈÄ≤„ÅÆÂÑ™ÈÅáË©ï‰æ°`,
    
    evaluation: `
‚Ä¢ ÂÆöÊúüÁöÑ„Å™„Çπ„Ç≠„É´„Ç¢„Çª„Çπ„É°„É≥„Éà„ÅÆÂÆüÊñΩ
‚Ä¢ Ê•≠ÂãôÈÅÇË°å„Å´„Åä„Åë„ÇãÂÆüË∑µÁöÑËÉΩÂäõ„ÅÆË©ï‰æ°
‚Ä¢ ÂêåÂÉö„ÉªÈÉ®‰∏ã„Åã„Çâ„ÅÆ360Â∫¶Ë©ï‰æ°
‚Ä¢ Á∂ôÁ∂öÁöÑ„Å™Â≠¶ÁøíÂßøÂã¢„Å®ÊàêÊûú„ÅÆÁ¢∫Ë™ç`
  }
}

/**
 * Generate comprehensive analysis when no specific topic is matched
 */
function generateComprehensiveAnalysis(content: string, question: string): string {
  const lines = content.split('\n').filter(line => line.trim().length > 0)
  const totalChars = content.length
  
  // Extract key sections and important information
  const keyLines = lines.filter(line => 
    line.toLowerCase().includes('ÈáçË¶Å') ||
    line.toLowerCase().includes('ÂøÖÈ†à') ||
    line.toLowerCase().includes('Ê≥®ÊÑè') ||
    line.match(/^[‚ñ†‚ñ°‚óè‚óã‚ñ™‚ñ´]\s/) ||
    line.match(/^\d+[.)]\s/) ||
    line.includes('Ôºö') || line.includes(':')
  ).slice(0, 15)
  
  const analysis = `
**üìã Á∑èÂêàÂàÜÊûê**

**üìä ÊñáÊõ∏Ê¶ÇË¶Å**:
‚Ä¢ ÊñáÊõ∏Ë¶èÊ®°: ${lines.length}Ë°åÔºà${totalChars}ÊñáÂ≠óÔºâ
‚Ä¢ ÊßãÈÄ†ÂåñÊÉÖÂ†±: ${keyLines.length}‰ª∂„ÅÆÈáçË¶ÅÈ†ÖÁõÆ„ÇíÁ¢∫Ë™ç

**üîç ‰∏ªË¶ÅÂÜÖÂÆπ**:
${keyLines.map(line => `‚Ä¢ ${line.trim().substring(0, 200)}`).join('\n')}

**üí° Ê¥ªÁî®ÊñπÊ≥ï**:
‚Ä¢ Âà∂Â∫¶„ÅÆË©≥Á¥∞Á¢∫Ë™ç: ÂÖ∑‰ΩìÁöÑ„Å™È†ÖÁõÆÂêç„ÅßË≥™Âïè
‚Ä¢ Êï∞ÂÄ§„ÉªÊù°‰ª∂„ÅÆÊäΩÂá∫: „ÄåÊâãÂΩì„Äç„ÄåÈáëÈ°ç„Äç„ÄåÊúüÈñì„ÄçÁ≠â„ÅßË≥™Âïè
‚Ä¢ „Éó„É≠„Çª„Çπ„ÅÆÁêÜËß£: „ÄåÊâãÈ†Ü„Äç„ÄåÊñπÊ≥ï„Äç„ÄåÊµÅ„Çå„ÄçÁ≠â„ÅßË≥™Âïè
‚Ä¢ ÊØîËºÉÂàÜÊûê: Ë§áÊï∞„ÅÆÂà∂Â∫¶„ÇÑÊù°‰ª∂„ÅÆÊØîËºÉ`

  return analysis
}

/**
 * Enhanced conversation management with streaming support
 */
export class EnhancedConversationMemory extends ConversationMemory {
  private contextSummary: string = ''
  
  constructor(maxTurns: number = 15, maxTokens: number = 3000) {
    super(maxTurns, maxTokens)
  }

  /**
   * Get optimized context for ChatGPT-like responses
   */
  getChatGPTContext(): string {
    const recentContext = this.getContext()
    const summary = this.getSummary()
    
    if (this.contextSummary && recentContext.length < 500) {
      return `ÈÅéÂéª„ÅÆ‰ºöË©±Ê¶ÇË¶Å: ${this.contextSummary}\n\nÊúÄËøë„ÅÆ‰ºöË©±:\n${recentContext}`
    }
    
    return recentContext
  }

  /**
   * Update context summary for long conversations
   */
  updateContextSummary(summary: string) {
    this.contextSummary = summary
  }

  /**
   * Analyze conversation patterns for better responses
   */
  getConversationPatterns(): { 
    frequentTopics: string[], 
    userPreferences: string[], 
    conversationStyle: string 
  } {
    const allUserMessages = this.history
      .filter(turn => turn.role === 'user')
      .map(turn => turn.content.toLowerCase())
    
    // Simple keyword extraction for topics
    const words = allUserMessages.join(' ').split(/\s+/)
    const wordCounts = words.reduce((acc, word) => {
      if (word.length > 3) {
        acc[word] = (acc[word] || 0) + 1
      }
      return acc
    }, {} as Record<string, number>)
    
    const frequentTopics = Object.entries(wordCounts)
      .sort(([,a], [,b]) => b - a)
      .slice(0, 5)
      .map(([word]) => word)
    
    // Determine conversation style
    const hasPoliteLanguage = allUserMessages.some(msg => 
      msg.includes('„ÅäÈ°ò„ÅÑ') || msg.includes('„Åè„Å†„Åï„ÅÑ') || msg.includes('„ÅÇ„Çä„Åå„Å®„ÅÜ')
    )
    
    const hasInformalLanguage = allUserMessages.some(msg =>
      msg.includes('„Å†„Çà„Å≠') || msg.includes('„Å£„Å¶„ÅÑ„ÅÜ') || msg.includes('„ÇÑ„Å∞„ÅÑ')
    )
    
    let conversationStyle = 'neutral'
    if (hasPoliteLanguage && !hasInformalLanguage) conversationStyle = 'formal'
    if (hasInformalLanguage && !hasPoliteLanguage) conversationStyle = 'casual'
    
    return {
      frequentTopics,
      userPreferences: [], // Could be enhanced based on user feedback
      conversationStyle
    }
  }
}

/**
 * ChatGPT-like streaming response generation
 */
export async function* generateStreamingResponse(
  message: string,
  conversationMemory: EnhancedConversationMemory,
  documentContext?: string
): AsyncGenerator<string, void, unknown> {
  try {
    // Get conversation patterns for personalized responses
    const patterns = conversationMemory.getConversationPatterns()
    const chatContext = conversationMemory.getChatGPTContext()
    
    // Create enhanced prompt
    let systemPrompt = `„ÅÇ„Å™„Åü„ÅØË¶™Âàá„ÅßÁü•Ë≠òË±äÂØå„Å™AI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ`
    
    // Adapt response style based on conversation patterns
    switch (patterns.conversationStyle) {
      case 'formal':
        systemPrompt += `‰∏ÅÂØß„ÅßÊ†ºÂºè„ÅÇ„ÇãÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ`
        break
      case 'casual':
        systemPrompt += `„Éï„É¨„É≥„Éâ„É™„Éº„ÅßË¶™„Åó„Åø„ÇÑ„Åô„ÅÑÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ`
        break
      default:
        systemPrompt += `Ëá™ÁÑ∂„ÅßÂàÜ„Åã„Çä„ÇÑ„Åô„ÅÑÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ`
    }
    
    if (documentContext) {
      systemPrompt += `\n\nÂèÇËÄÉË≥áÊñô:\n${documentContext.slice(0, 2000)}`
    }
    
    if (chatContext) {
      systemPrompt += `\n\n‰ºöË©±Â±•Ê≠¥:\n${chatContext}`
    }
    
    const fullPrompt = `${systemPrompt}\n\n„É¶„Éº„Ç∂„Éº: ${message}\n„Ç¢„Ç∑„Çπ„Çø„É≥„Éà:`
    
    // Use Hugging Face streaming if available
    const response = await hf.textGeneration({
      model: MODELS.CHAT,
      inputs: fullPrompt,
      parameters: {
        max_new_tokens: 500,
        temperature: 0.7,
        top_p: 0.9,
        do_sample: true,
        repetition_penalty: 1.1,
        return_full_text: false,
        stop: ['„É¶„Éº„Ç∂„Éº:', 'User:', 'Human:']
      }
    })
    
    // Simulate streaming by yielding chunks of the response
    const fullResponse = response.generated_text.trim()
    const chunks = fullResponse.split(/([„ÄÇÔºÅÔºü])/g)
    
    let accumulatedResponse = ''
    for (const chunk of chunks) {
      if (chunk.trim()) {
        accumulatedResponse += chunk
        yield accumulatedResponse
        
        // Add small delay to simulate real streaming
        await new Promise(resolve => setTimeout(resolve, 50))
      }
    }
    
    // Add to conversation memory
    conversationMemory.addTurn('user', message, documentContext)
    conversationMemory.addTurn('assistant', fullResponse, documentContext)
    
  } catch (error) {
    console.error('Streaming response error:', error)
    
    // Fallback to non-streaming intelligent response
    const fallbackResponse = await generateIntelligentFallback(message, documentContext || '', conversationMemory)
    yield fallbackResponse
  }
}

/**
 * Generate response using gpt-oss models with reasoning levels
 */
export async function generateGptOssResponse(
  prompt: string,
  context?: string,
  reasoningLevel: 'low' | 'medium' | 'high' = 'medium',
  model: string = MODELS.CHAT
): Promise<string> {
  try {
    // Prepare system prompt with reasoning level
    const systemPrompt = `Reasoning: ${reasoningLevel}\n\nYou are a helpful AI assistant. Please provide accurate and helpful responses in Japanese when appropriate.`
    
    // Prepare the input using harmony format for gpt-oss
    const messages = [
      { role: 'system', content: systemPrompt }
    ]
    
    if (context) {
      messages.push({ 
        role: 'user', 
        content: `Context: ${context}\n\nQuestion: ${prompt}` 
      })
    } else {
      messages.push({ role: 'user', content: prompt })
    }

    // For gpt-oss models, use the text generation with proper formatting
    const formattedInput = messages.map(msg => 
      `${msg.role === 'system' ? 'System' : msg.role === 'user' ? 'Human' : 'Assistant'}: ${msg.content}`
    ).join('\n') + '\nAssistant:'

    const response = await hf.textGeneration({
      model,
      inputs: formattedInput,
      parameters: {
        max_new_tokens: 800,
        temperature: 0.7,
        do_sample: true,
        repetition_penalty: 1.1,
        return_full_text: false,
        stop: ['Human:', 'System:']
      }
    })

    return response.generated_text.trim()
  } catch (error) {
    console.error('gpt-oss API error:', error)
    
    // Fallback to standard generation
    return generateResponse(prompt, context, MODELS.SIMPLE_CHAT)
  }
}

/**
 * Generate natural, conversational responses like ChatGPT
 */
export async function generateNaturalChatResponse(
  message: string,
  documentContext?: string,
  conversationHistory?: string[]
): Promise<string> {
  try {
    // First try Hugging Face API with proper error handling
    const response = await tryHuggingFaceGeneration(message, documentContext, conversationHistory)
    return makeResponseNatural(response, message, documentContext)
  } catch (error) {
    console.error('Hugging Face API failed:', error)
    
    // Use intelligent fallback without API
    if (documentContext) {
      return generateSimpleDocumentResponse(message, documentContext)
    } else {
      return generateSimpleResponse(message)
    }
  }
}

/**
 * Try Hugging Face generation with fallback handling
 */
async function tryHuggingFaceGeneration(
  message: string,
  documentContext?: string,
  conversationHistory?: string[]
): Promise<string> {
  // Create a natural conversation prompt
  let systemPrompt = `„ÅÇ„Å™„Åü„ÅØË¶™Âàá„ÅßÁü•Ë≠òË±äÂØå„Å™AI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇËá™ÁÑ∂„Åß‰∫∫Èñì„Çâ„Åó„ÅÑ‰ºöË©±„ÇíÂøÉ„Åå„Åë„ÄÅÁ∞°ÊΩî„ÅßÂàÜ„Åã„Çä„ÇÑ„Åô„ÅÑÂõûÁ≠î„Çí„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ`
  
  // Build conversation context
  let conversationContext = ''
  if (conversationHistory && conversationHistory.length > 0) {
    conversationContext = conversationHistory.slice(-4).join('\n') // Last 4 exchanges
  }
  
  // Create the input prompt
  let inputPrompt = systemPrompt
  
  if (documentContext) {
    inputPrompt += `\n\nÂèÇËÄÉË≥áÊñô:\n${documentContext.slice(0, 1000)}`
  }
  
  if (conversationContext) {
    inputPrompt += `\n\nÊúÄËøë„ÅÆ‰ºöË©±:\n${conversationContext}`
  }
  
  inputPrompt += `\n\n„É¶„Éº„Ç∂„Éº: ${message}\n„Ç¢„Ç∑„Çπ„Çø„É≥„Éà:`

  // Try multiple models as fallback
  const modelsToTry = [
    MODELS.SIMPLE_CHAT, // DialoGPT-medium
    'gpt2', // Basic GPT-2
    'microsoft/DialoGPT-small' // Smaller model
  ]

  for (const model of modelsToTry) {
    try {
      const response = await hf.textGeneration({
        model,
        inputs: inputPrompt,
        parameters: {
          max_new_tokens: 200,
          temperature: 0.7,
          do_sample: true,
          repetition_penalty: 1.1,
          return_full_text: false,
          stop: ['„É¶„Éº„Ç∂„Éº:', '„Ç¢„Ç∑„Çπ„Çø„É≥„Éà:', '\n\n„É¶„Éº„Ç∂„Éº', '\n\n„Ç¢„Ç∑„Çπ„Çø„É≥„Éà']
        }
      })

      if (response.generated_text && response.generated_text.trim().length > 0) {
        return response.generated_text.trim()
      }
    } catch (modelError) {
      console.warn(`Model ${model} failed:`, modelError)
      continue
    }
  }

  // If all models fail, throw error to trigger fallback
  throw new Error('All Hugging Face models failed')
}

/**
 * Make response more natural and conversational
 */
function makeResponseNatural(response: string, userMessage: string, documentContext?: string): string {
  // Remove excessive formatting and make more conversational
  let natural = response
    .replace(/^„Äê.*?„Äë\s*/gm, '') // Remove formatted headers
    .replace(/^\*\*.*?\*\*\s*/gm, '') // Remove bold headers
    .replace(/^#+\s*/gm, '') // Remove markdown headers
    .replace(/üìÑ|üìÇ|üìã|üéØ|üìä|üí¨|üìà|üîç|üí°|ü§ù|‚úÖ|‚ùå|üéâ|üîß|üì±|üè¢|üí∞|üéÅ|üìù|üîí|üöÄ|üé®|üìã|üìà|üìä/g, '') // Remove all emojis
    .replace(/^\s*[‚Ä¢\-\*]\s*/gm, '') // Remove bullet points
    .replace(/^(:\s*|Ôºö\s*)/gm, '') // Remove colons at start of lines
    .replace(/\*\*(.*?)\*\*/g, '$1') // Remove bold formatting
    .replace(/\n{3,}/g, '\n\n') // Reduce excessive line breaks
    .replace(/^(Ë©≥Á¥∞ÂàÜÊûêÁµêÊûú|ÂèÇÁÖßË≥áÊñô|ÊßãÈÄ†ÂåñÂàÜÊûê|ÊñáÊõ∏Áµ±Ë®à|Âà©Áî®ÂèØËÉΩ„Å™Ê©üËÉΩ|„Åï„Çâ„Å™„Çã„Çµ„Éù„Éº„Éà).*$/gm, '') // Remove structured sections
    .replace(/^(„ÅîË≥™Âïè|üìÇ|üìã|üéØ|üìä|üí¨|üìà|üîç|üí°|ü§ù|üì±|üè¢|üí∞|üéÅ|üìù|üîí|üöÄ|üé®).*$/gm, '') // Remove structured headers
    .replace(/\n\s*\n\s*\n/g, '\n\n') // Clean up multiple line breaks
    .trim()

  // Remove structured analysis sections completely
  const sectionsToRemove = [
    /^.*?Ë©≥Á¥∞ÂàÜÊûêÁµêÊûú.*$/gm,
    /^.*?ÂèÇÁÖßË≥áÊñô.*$/gm,
    /^.*?ÊßãÈÄ†ÂåñÂàÜÊûê.*$/gm,
    /^.*?ÊñáÊõ∏Áµ±Ë®à.*$/gm,
    /^.*?Âà©Áî®ÂèØËÉΩ„Å™Ê©üËÉΩ.*$/gm,
    /^.*?„Åï„Çâ„Å™„Çã„Çµ„Éù„Éº„Éà.*$/gm,
    /^.*?Á∑èË°åÊï∞.*$/gm,
    /^.*?ÁÆáÊù°Êõ∏„ÅçÈ†ÖÁõÆ.*$/gm,
    /^.*?„Çª„ÇØ„Ç∑„Éß„É≥Êï∞.*$/gm
  ]

  sectionsToRemove.forEach(pattern => {
    natural = natural.replace(pattern, '')
  })

  // Clean up the content to be more conversational
  natural = natural
    .replace(/„Å´„Å§„ÅÑ„Å¶Ë™¨Êòé.*?„Ç∑„Çπ„ÉÜ„É†„Åß„Åô„ÄÇ/g, '„Å´„Å§„ÅÑ„Å¶„Åß„Åô„Å≠„ÄÇ')
    .replace(/Ë≥áÊñô„Åß„ÅØ.*?Á¢∫Ë™ç„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ/g, '')
    .replace(/„Åì„ÅÆÂõûÁ≠î„ÅØ.*?ÁîüÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ/g, '')
    .replace(/ÂÖ∑‰ΩìÁöÑ„Å™Êï∞ÂÄ§.*?Êèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ/g, '')

  // Ensure reasonable length (not too verbose)
  if (natural.length > 400) {
    const sentences = natural.split(/[„ÄÇÔºÅÔºü]/)
    natural = sentences.slice(0, 3).join('„ÄÇ') + '„ÄÇ'
  }

  // Ensure the response starts naturally and is concise
  if (natural.length < 20 || !natural.trim()) {
    if (documentContext && userMessage.includes('Ë©ï‰æ°')) {
      return 'Ë©ï‰æ°Âà∂Â∫¶„Å´„Å§„ÅÑ„Å¶„Åß„Åô„Å≠„ÄÇË≥áÊñô„ÇíÁ¢∫Ë™ç„Åó„Åü„Å®„Åì„Çç„ÄÅÂåÖÊã¨ÁöÑ„Å™Âà∂Â∫¶„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Å©„ÅÆÈÉ®ÂàÜ„Å´„Å§„ÅÑ„Å¶Ë©≥„Åó„ÅèÁü•„Çä„Åü„ÅÑ„Åß„Åô„ÅãÔºü'
    } else if (documentContext) {
      return `„Äå${userMessage}„Äç„Å´„Å§„ÅÑ„Å¶Ë≥áÊñô„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åó„Åü„ÄÇÂÖ∑‰ΩìÁöÑ„Å´„Å©„ÅÆÈÉ®ÂàÜ„Å´„Å§„ÅÑ„Å¶Ë©≥„Åó„ÅèÁü•„Çä„Åü„ÅÑ„Åß„Åô„ÅãÔºü`
    }
  }

  return natural
}

/**
 * Generate simple document-based response
 */
function generateSimpleDocumentResponse(message: string, documentContext: string): string {
  const context = documentContext.slice(0, 1200)
  const messageLC = message.toLowerCase()
  
  // Create more natural, conversational responses
  if (messageLC.includes('Ë©ï‰æ°') || messageLC.includes('‰∫∫‰∫ã')) {
    // Extract key information naturally from the context
    const hasGrades = context.includes('„Ç∞„É¨„Éº„Éâ') || context.includes('STEP')
    const hasMedalSheet = context.includes('„É°„ÉÄ„É´„Ç∑„Éº„Éà') || context.includes('medal')
    const hasReview = context.includes('Ë©ï‰æ°Èù¢Ë´á') || context.includes('Èù¢Ë´á')
    
    let response = 'Ë©ï‰æ°Âà∂Â∫¶„Å´„Å§„ÅÑ„Å¶„ÅäÁ≠î„Åà„Åó„Åæ„Åô„Å≠„ÄÇ\n\n'
    
    if (hasGrades) {
      response += '‰ºöÁ§æ„Åß„ÅØ„Ç∞„É¨„Éº„ÉâÂà∂Â∫¶„ÇíÊé°Áî®„Åó„Å¶„ÅÑ„Å¶„ÄÅ'
    }
    if (hasMedalSheet) {
      response += '„É°„ÉÄ„É´„Ç∑„Éº„Éà„Çí‰Ωø„Å£„ÅüÁõÆÊ®ôÁÆ°ÁêÜ„Ç∑„Çπ„ÉÜ„É†„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ'
    }
    if (hasReview) {
      response += 'ÂÆöÊúüÁöÑ„Å™Ë©ï‰æ°Èù¢Ë´á„ÅßÈÄ≤Êçó„ÇíÁ¢∫Ë™ç„Åó„ÄÅ'
    }
    
    response += '\n\nÂÖ∑‰ΩìÁöÑ„Å´„Å©„ÅÆÈÉ®ÂàÜ„Å´„Å§„ÅÑ„Å¶Ë©≥„Åó„ÅèÁü•„Çä„Åü„ÅÑ„Åß„Åô„ÅãÔºü'
    return response
  }
  
  if (messageLC.includes('„É°„ÉÄ„É´„Ç∑„Éº„Éà')) {
    return '„É°„ÉÄ„É´„Ç∑„Éº„Éà„Å´„Å§„ÅÑ„Å¶„Åß„Åô„Å≠„ÄÇ\n\nË≥áÊñô„Å´„Çà„Çã„Å®„ÄÅ„É°„ÉÄ„É´„Ç∑„Éº„Éà„ÅØÁõÆÊ®ôË®≠ÂÆö„Å®ÈÅîÊàêÂ∫¶Ë©ï‰æ°„ÅÆ„Åü„ÇÅ„ÅÆ„ÉÑ„Éº„É´„Åß„Åô„ÄÇÂçò‰æ°ÁõÆÊ®ô„ÇÑ„É°„ÉÄ„É´ÂèñÂæóÊï∞„ÄÅË≥áÊ†ºÂèñÂæó„Å™„Å©„ÇíÁÆ°ÁêÜ„Åó„Å¶„ÄÅÂÆöÊúüÁöÑ„Å´ÊåØ„ÇäËøî„Çä„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ\n\n‰Ωø„ÅÑÊñπ„ÇÑÂÖ∑‰ΩìÁöÑ„Å™È†ÖÁõÆ„Å´„Å§„ÅÑ„Å¶„ÄÅ„ÇÇ„ÅÜÂ∞ë„ÅóË©≥„Åó„Åè„ÅäËÅû„Åç„Åó„Åü„ÅÑ„Åì„Å®„ÅØ„ÅÇ„Çä„Åæ„Åô„ÅãÔºü'
  }
  
  if (messageLC.includes('„Ç∞„É¨„Éº„Éâ') || messageLC.includes('ÊòáÊ†º')) {
    return '„Ç∞„É¨„Éº„ÉâÂà∂Â∫¶„Å´„Å§„ÅÑ„Å¶Ë™¨Êòé„Åó„Åæ„Åô„Å≠„ÄÇ\n\nË≥áÊñô„ÇíË¶ã„Çã„Å®„ÄÅÊÆµÈöéÁöÑ„Å™„Ç∞„É¨„Éº„ÉâÊßãÊàê„Å´„Å™„Å£„Å¶„ÅÑ„Å¶„ÄÅ„Åù„Çå„Åû„Çå„Å´ÊòáÊ†ºÊù°‰ª∂„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Ç∞„É¨„Éº„Éâ„Å´Âøú„Åò„ÅüÊâãÂΩì„ÇÑË©ï‰æ°Âü∫Ê∫ñ„ÇÇÊ±∫„ÇÅ„Çâ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nÁâπÂÆö„ÅÆ„Ç∞„É¨„Éº„Éâ„ÇÑÊòáÊ†ºÊù°‰ª∂„Å´„Å§„ÅÑ„Å¶Ë©≥„Åó„ÅèÁü•„Çä„Åü„ÅÑ„Åß„Åô„ÅãÔºü'
  }
  
  if (messageLC.includes('Á¶èÂà©ÂéöÁîü') || messageLC.includes('Âà∂Â∫¶')) {
    return 'Á¶èÂà©ÂéöÁîüÂà∂Â∫¶„Å´„Å§„ÅÑ„Å¶„ÅäÁ≠î„Åà„Åó„Åæ„Åô„ÄÇ\n\nË≥áÊñô„Å´„ÅØÊßò„ÄÖ„Å™Âà∂Â∫¶„ÅåË®òËºâ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Å©„ÅÆ„Çà„ÅÜ„Å™Âà∂Â∫¶„Å´„Å§„ÅÑ„Å¶ÂÖ∑‰ΩìÁöÑ„Å´Áü•„Çä„Åü„ÅÑ„Åß„Åô„ÅãÔºü\n\n‰æã„Åà„Å∞„ÄÅÂÅ•Â∫∑‰øùÈô∫„ÄÅÈÄÄËÅ∑Èáë„ÄÅÁ†î‰øÆÂà∂Â∫¶„ÄÅ‰ºëÊöáÂà∂Â∫¶„Å™„Å©„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ'
  }
  
  // More natural generic response
  return `„Äå${message}„Äç„Å´„Å§„ÅÑ„Å¶„Åß„Åô„Å≠„ÄÇ\n\nË≥áÊñô„ÇíÁ¢∫Ë™ç„Åó„Åü„Å®„Åì„Çç„ÄÅÈñ¢ÈÄ£„Åô„ÇãÊÉÖÂ†±„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„ÇÇ„ÅÜÂ∞ë„ÅóÂÖ∑‰ΩìÁöÑ„Å´„Å©„ÅÆÈÉ®ÂàÜ„Å´„Å§„ÅÑ„Å¶Áü•„Çä„Åü„ÅÑ„ÅãÊïô„Åà„Å¶„ÅÑ„Åü„Å†„Åë„Åæ„Åô„ÅãÔºü\n\n„Åù„ÅÜ„Åô„Çå„Å∞„ÄÅ„Çà„ÇäË©≥„Åó„ÅÑÊÉÖÂ†±„Çí„Åä‰ºù„Åà„Åß„Åç„Åæ„Åô„ÄÇ`
}

/**
 * Extract keywords from document context
 */
function extractKeywords(text: string): string[] {
  const commonWords = ['„Å´„Å§„ÅÑ„Å¶', '„Åß„Åô', '„Åæ„Åô', '„Åì„Å®', '„ÇÇ„ÅÆ', '„Åü„ÇÅ', '„Å™„Å©', '„Åæ„Åü', '„Åß„ÅØ', '„Åã„Çâ', '„Åæ„Åß', '„Å®„Åó„Å¶', '„Å´„Çà„Çã', '„Å´„Åä„ÅÑ„Å¶', '„Å´Èñ¢„Åó„Å¶']
  
  const words = text
    .replace(/[^\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\u3400-\u4DBF\w\s]/g, ' ')
    .split(/\s+/)
    .filter(word => word.length > 1 && !commonWords.includes(word))
    .slice(0, 10)
  
  return [...new Set(words)]
}

/**
 * Generate simple response without document context
 */
function generateSimpleResponse(message: string): string {
  const messageLC = message.toLowerCase()
  
  if (messageLC.includes('„Åì„Çì„Å´„Å°„ÅØ') || messageLC.includes('„ÅØ„Åò„ÇÅ„Åæ„Åó„Å¶') || messageLC.includes('hello')) {
    return '„Åì„Çì„Å´„Å°„ÅØÔºÅSLJ Chatbot„Åß„Åô„ÄÇ‰ºöÁ§æ„ÅÆË≥áÊñô„Å´„Å§„ÅÑ„Å¶‰Ωï„Åã„ÅîË≥™Âïè„Åå„Åî„Åñ„ÅÑ„Åæ„Åó„Åü„Çâ„ÄÅ„ÅäÊ∞óËªΩ„Å´„ÅäËÅû„Åã„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ'
  }
  
  if (messageLC.includes('„ÅÇ„Çä„Åå„Å®„ÅÜ') || messageLC.includes('thank')) {
    return '„Å©„ÅÜ„ÅÑ„Åü„Åó„Åæ„Åó„Å¶„ÄÇ‰ªñ„Å´„ÇÇ„ÅîË≥™Âïè„Åå„Åî„Åñ„ÅÑ„Åæ„Åó„Åü„Çâ„ÄÅ„ÅÑ„Å§„Åß„ÇÇ„ÅäËÅû„Åã„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ'
  }
  
  if (messageLC.includes('Âä©„Åë„Å¶') || messageLC.includes('help') || messageLC.includes('„Çµ„Éù„Éº„Éà')) {
    return '„ÅäÊâã‰ºù„ÅÑ„ÅÑ„Åü„Åó„Åæ„ÅôÔºÅ„Å©„ÅÆ„Çà„ÅÜ„Å™„Åì„Å®„Å´„Å§„ÅÑ„Å¶Áü•„Çä„Åü„ÅÑ„Åß„Åô„ÅãÔºü\n\n‰æãÔºöË©ï‰æ°Âà∂Â∫¶„ÄÅ„É°„ÉÄ„É´„Ç∑„Éº„Éà„ÄÅÁ¶èÂà©ÂéöÁîü„ÄÅ‰ºöÁ§æ„ÅÆ„É´„Éº„É´„Å™„Å©'
  }
  
  if (messageLC.includes('„É°„ÉÄ„É´„Ç∑„Éº„Éà')) {
    return '„É°„ÉÄ„É´„Ç∑„Éº„Éà„Å´„Å§„ÅÑ„Å¶„ÅäÁ≠î„Åà„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÈñ¢ÈÄ£„Åô„ÇãË≥áÊñô„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„ÅÑ„Åü„Å†„Åë„Åæ„Åô„Åß„Åó„Çá„ÅÜ„ÅãÔºü„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÂæå„ÄÅË©≥Á¥∞„Å™Ë™¨Êòé„Çí„ÅÑ„Åü„Åó„Åæ„Åô„ÄÇ'
  }
  
  if (messageLC.includes('Ë©ï‰æ°') || messageLC.includes('‰∫∫‰∫ã') || messageLC.includes('Âà∂Â∫¶')) {
    return 'Ë©ï‰æ°Âà∂Â∫¶„Å´„Å§„ÅÑ„Å¶Êâø„Çä„Åæ„Åó„Åü„ÄÇ„Çà„ÇäË©≥Á¥∞„Å™ÂõûÁ≠î„ÅÆ„Åü„ÇÅ„Å´„ÄÅ‰∫∫‰∫ãÈñ¢ÈÄ£„ÅÆË≥áÊñô„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„ÅÑ„Åü„Å†„Åè„Å®„ÄÅÂÖ∑‰ΩìÁöÑ„Å™ÊÉÖÂ†±„Çí„Åä‰ºù„Åà„Åß„Åç„Åæ„Åô„ÄÇ'
  }
  
  // Generic helpful response
  return `„Äå${message}„Äç„Å´„Å§„ÅÑ„Å¶Êâø„Çä„Åæ„Åó„Åü„ÄÇ\n\n„Çà„ÇäË©≥Á¥∞„Åß„ÅäÂΩπ„Å´Á´ã„Å§ÂõûÁ≠î„Çí„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÈñ¢ÈÄ£„Åô„ÇãË≥áÊñô„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„ÅÑ„Åü„Å†„Åè„Åã„ÄÅÂÖ∑‰ΩìÁöÑ„Å™Ë≥™Âïè„Çí„ÅäËÅû„Åã„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n‰Ωï„Åã„ÅäÊâã‰ºù„ÅÑ„Åß„Åç„Çã„Åì„Å®„Åå„Åî„Åñ„ÅÑ„Åæ„Åó„Åü„Çâ„ÄÅ„ÅäÊ∞óËªΩ„Å´„ÅäËÅû„Åã„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ`
}
