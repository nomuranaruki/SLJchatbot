import { HfInference } from '@huggingface/inference'

// Hugging Face API client initialization
export const hf = new HfInference(process.env.HUGGINGFACE_API_KEY)

// Available models for different tasks - updated for better Japanese support
export const MODELS = {
  // Primary conversation model - better for Japanese
  CHAT: 'microsoft/DialoGPT-medium',
  // Backup lightweight model
  SIMPLE_CHAT: 'microsoft/DialoGPT-small', 
  // Question answering model
  QA: 'deepset/roberta-base-squad2',
  // Text generation
  TEXT_GENERATION: 'gpt2',
  // Summarization
  SUMMARIZATION: 'facebook/bart-large-cnn',
  // Fast QA for quick responses
  FAST_QA: 'distilbert-base-cased-distilled-squad'
}

// API configuration
const API_CONFIG = {
  baseURL: process.env.HUGGINGFACE_API_URL || 'https://api-inference.huggingface.co/models',
  headers: {
    'Authorization': `Bearer ${process.env.HUGGINGFACE_API_KEY}`,
    'Content-Type': 'application/json',
  },
  timeout: 30000,
  retries: 3
}

/**
 * Conversation Turn interface
 */
interface ConversationTurn {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
  documentContext?: string
}

/**
 * Advanced conversation memory management
 */
export class ConversationMemory {
  protected history: ConversationTurn[] = []
  private maxTurns: number = 10
  private maxTokens: number = 2000

  constructor(maxTurns: number = 10, maxTokens: number = 2000) {
    this.maxTurns = maxTurns
    this.maxTokens = maxTokens
  }

  /**
   * Add a conversation turn
   */
  addTurn(role: 'user' | 'assistant', content: string, documentContext?: string) {
    this.history.push({
      role,
      content,
      timestamp: new Date(),
      documentContext
    })

    // Keep only recent turns
    if (this.history.length > this.maxTurns) {
      this.history = this.history.slice(-this.maxTurns)
    }
  }

  /**
   * Get conversation context for model input
   */
  getContext(): string {
    let context = ''
    let tokenCount = 0

    // Build context from most recent conversations
    for (let i = this.history.length - 1; i >= 0; i--) {
      const turn = this.history[i]
      const turnText = `${turn.role === 'user' ? 'Human' : 'Assistant'}: ${turn.content}\n`
      
      // Rough token estimation (1 token ‚âà 4 characters for English/Japanese)
      const turnTokens = turnText.length / 4
      
      if (tokenCount + turnTokens > this.maxTokens) {
        break
      }
      
      context = turnText + context
      tokenCount += turnTokens
    }

    return context
  }

  /**
   * Get recent document context
   */
  getRecentDocumentContext(): string {
    const recentTurns = this.history.slice(-3) // Last 3 turns
    const documentContexts = recentTurns
      .filter(turn => turn.documentContext)
      .map(turn => turn.documentContext)
    
    return documentContexts.length > 0 ? documentContexts[documentContexts.length - 1]! : ''
  }

  /**
   * Clear conversation history
   */
  clear() {
    this.history = []
  }

  /**
   * Get conversation summary for long-term memory
   */
  getSummary(): string {
    if (this.history.length === 0) return ''
    
    const topics = new Set<string>()
    this.history.forEach(turn => {
      if (turn.role === 'user') {
        // Extract potential topics from user messages
        const words = turn.content.split(/\s+/).filter(w => w.length > 2)
        words.slice(0, 3).forEach(word => topics.add(word))
      }
    })

    return `‰ºöË©±„ÅÆ„Éà„Éî„ÉÉ„ÇØ: ${Array.from(topics).join(', ')}`
  }
}

/**
 * Generate text response using Hugging Face model
 */
export async function generateResponse(
  prompt: string,
  context?: string,
  model: string = MODELS.CHAT
): Promise<string> {
  try {
    // Prepare the input with context if provided
    const input = context 
      ? `Context: ${context}\n\nQuestion: ${prompt}\n\nAnswer:`
      : prompt

    const response = await hf.textGeneration({
      model,
      inputs: input,
      parameters: {
        max_new_tokens: 500,
        temperature: 0.7,
        do_sample: true,
        repetition_penalty: 1.1,
        return_full_text: false
      }
    })

    return response.generated_text.trim()
  } catch (error) {
    console.error('Hugging Face API error:', error)
    throw new Error('AI response generation failed')
  }
}

/**
 * Alias for generateResponse for backward compatibility
 */
export const generateChatResponse = generateResponse

/**
 * Answer questions based on document context
 */
export async function answerQuestion(
  question: string,
  context: string
): Promise<string> {
  try {
    const response = await hf.questionAnswering({
      model: MODELS.QA,
      inputs: {
        question,
        context
      }
    })

    return response.answer || 'Sorry, I could not find an answer in the provided context.'
  } catch (error) {
    console.error('Question answering error:', error)
    
    // Fallback to text generation
    return generateResponse(question, context)
  }
}

/**
 * Summarize document content
 */
export async function summarizeText(text: string): Promise<string> {
  try {
    // Split long text into chunks if needed
    const maxLength = 1024
    if (text.length > maxLength) {
      const chunks = []
      for (let i = 0; i < text.length; i += maxLength) {
        chunks.push(text.slice(i, i + maxLength))
      }
      
      const summaries = await Promise.all(
        chunks.map(chunk => hf.summarization({
          model: MODELS.SUMMARIZATION,
          inputs: chunk,
          parameters: {
            max_length: 150,
            min_length: 30
          }
        }))
      )
      
      return summaries.map(s => s.summary_text).join(' ')
    }

    const response = await hf.summarization({
      model: MODELS.SUMMARIZATION,
      inputs: text,
      parameters: {
        max_length: 150,
        min_length: 30
      }
    })

    return response.summary_text
  } catch (error) {
    console.error('Summarization error:', error)
    return 'Sorry, I could not summarize the text.'
  }
}

/**
 * Enhanced ChatGPT-like response generation with conversation memory
 */
export async function generateAdvancedChatResponse(
  message: string,
  conversationMemory: ConversationMemory,
  documentContext?: string
): Promise<string> {
  try {
    // Get conversation history
    const conversationHistory = conversationMemory.getContext()
    const recentDocContext = documentContext || conversationMemory.getRecentDocumentContext()
    
    // Create enhanced prompt with system instructions
    let prompt = `‰ª•‰∏ã„ÅØ„ÄÅ‰ºÅÊ•≠ÊñáÊõ∏ÁÆ°ÁêÜAI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Å®„É¶„Éº„Ç∂„Éº„Å®„ÅÆ‰ºöË©±„Åß„Åô„ÄÇAI„ÅØË¶™Âàá„Åß„ÄÅË©≥Á¥∞„Åß„ÄÅÊ≠£Á¢∫„Å™ÂõûÁ≠î„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ`
    
    // Add document context if available
    if (recentDocContext) {
      prompt += `\n\nÂèÇËÄÉË≥áÊñô:\n${recentDocContext.slice(0, 1500)}`
    }
    
    // Add conversation history
    if (conversationHistory) {
      prompt += `\n\nÈÅéÂéª„ÅÆ‰ºöË©±:\n${conversationHistory}`
    }
    
    // Add current user message
    prompt += `\nHuman: ${message}\nAssistant:`

    // Call Hugging Face API with improved parameters
    const response = await hf.textGeneration({
      model: MODELS.CHAT,
      inputs: prompt,
      parameters: {
        max_new_tokens: 400,
        temperature: 0.8,
        top_p: 0.95,
        do_sample: true,
        repetition_penalty: 1.15,
        return_full_text: false,
        stop: ['Human:', 'Assistant:', '\n\n']
      }
    })

    let generatedText = response.generated_text.trim()
    
    // Post-process the response
    generatedText = enhanceResponseQuality(generatedText, message, recentDocContext)
    
    // Add to conversation memory
    conversationMemory.addTurn('user', message, documentContext)
    conversationMemory.addTurn('assistant', generatedText, documentContext)
    
    return generatedText

  } catch (error) {
    console.error('Enhanced chat response error:', error)
    
    // Intelligent fallback based on message content
    const fallbackResponse = await generateIntelligentFallback(message, documentContext || '', conversationMemory)
    
    // Still add to memory even for fallback
    conversationMemory.addTurn('user', message, documentContext)
    conversationMemory.addTurn('assistant', fallbackResponse, documentContext)
    
    return fallbackResponse
  }
}

/**
 * Enhance response quality with post-processing
 */
function enhanceResponseQuality(response: string, originalQuestion: string, documentContext?: string): string {
  let enhanced = response
  
  // Remove incomplete sentences at the end
  const sentences = enhanced.split(/[„ÄÇÔºÅÔºü]/)
  if (sentences.length > 1 && sentences[sentences.length - 1].trim().length < 10) {
    enhanced = sentences.slice(0, -1).join('„ÄÇ') + '„ÄÇ'
  }
  
  // Ensure response is relevant to the question
  if (enhanced.length < 20) {
    enhanced = generateContextAwareResponse(originalQuestion, documentContext)
  }
  
  // Add helpful formatting for document-based responses
  if (documentContext && !enhanced.includes('üìÑ')) {
    enhanced = `üìÑ **ÊñáÊõ∏„Éô„Éº„ÇπÂõûÁ≠î**\n\n${enhanced}`
  }
  
  return enhanced
}

/**
 * Generate context-aware response for better fallback
 */
function generateContextAwareResponse(question: string, documentContext?: string): string {
  const questionLower = question.toLowerCase()
  
  // Analyze question intent
  if (questionLower.includes('Êïô„Åà„Å¶') || questionLower.includes('Ë™¨Êòé')) {
    if (documentContext) {
      return `„ÅîË≥™Âïè„Äå${question}„Äç„Å´„Å§„ÅÑ„Å¶„ÄÅ„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„ÅüË≥áÊñô„ÇíÂèÇËÄÉ„Å´ÂõûÁ≠î„ÅÑ„Åü„Åó„Åæ„Åô„ÄÇÂÖ∑‰ΩìÁöÑ„Å´„ÅäÁü•„Çä„Å´„Å™„Çä„Åü„ÅÑÁÇπ„Åå„Åî„Åñ„ÅÑ„Åæ„Åó„Åü„Çâ„ÄÅ„Çà„ÇäË©≥„Åó„Åè„ÅäËÅû„Åã„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ`
    } else {
      return `„Äå${question}„Äç„Å´„Å§„ÅÑ„Å¶ÂõûÁ≠î„Åï„Åõ„Å¶„ÅÑ„Åü„Å†„Åç„Åæ„Åô„ÄÇÈñ¢ÈÄ£„Åô„ÇãË≥áÊñô„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„ÅÑ„Åü„Å†„Åè„Å®„ÄÅ„Çà„ÇäË©≥Á¥∞„ÅßÊ≠£Á¢∫„Å™ÊÉÖÂ†±„ÇíÊèê‰æõ„Åß„Åç„Åæ„Åô„ÄÇ`
    }
  }
  
  if (questionLower.includes('ÊñπÊ≥ï') || questionLower.includes('„ÇÑ„ÇäÊñπ')) {
    return `„Äå${question}„Äç„ÅÆÊâãÈ†Ü„Å´„Å§„ÅÑ„Å¶Ë™¨Êòé„ÅÑ„Åü„Åó„Åæ„Åô„ÄÇÊÆµÈöéÁöÑ„Å™ÊâãÈ†Ü„ÇÑÂÖ∑‰ΩìÁöÑ„Å™ÊñπÊ≥ï„Çí„ÅäÁü•„Çä„Å´„Å™„Çä„Åü„ÅÑÂ†¥Âêà„ÅØ„ÄÅ„Çà„ÇäË©≥„Åó„Åè„ÅäËÅû„Åã„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ`
  }
  
  // Default response
  return `„Äå${question}„Äç„Å´„Å§„ÅÑ„Å¶Êâø„Çä„Åæ„Åó„Åü„ÄÇ„Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å™ÊÉÖÂ†±„ÇÑË©≥Á¥∞„Çí„ÅäÊ±Ç„ÇÅ„ÅÆÂ†¥Âêà„ÅØ„ÄÅÈñ¢ÈÄ£„Åô„ÇãÊñáÊõ∏„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„ÅÑ„Åü„Å†„Åè„Åã„ÄÅÂÖ∑‰ΩìÁöÑ„Å™Ë≥™Âïè„Çí„ÅäËÅû„Åã„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ`
}

/**
 * Generate intelligent document-based response
 */
export async function generateDocumentBasedResponse(
  question: string,
  documentContext: string,
  documentTitles: string[]
): Promise<string> {
  // First try Hugging Face API
  try {
    // Try question answering first
    const qaResponse = await answerQuestion(question, documentContext)
    
    if (qaResponse && !qaResponse.includes('Sorry, I could not find')) {
      // Enhance the QA response with context
      return enhanceResponseWithContext(qaResponse, question, documentTitles)
    }
  } catch (error) {
    console.log('QA model failed, trying text generation...')
  }

  // Fallback to intelligent analysis of document content
  const conversationMemory = new ConversationMemory()
  return generateIntelligentFallback(question, documentContext, conversationMemory)
}

/**
 * Enhance QA response with contextual information
 */
function enhanceResponseWithContext(
  answer: string, 
  question: string, 
  documentTitles: string[]
): string {
  return `üìÑ **Ë≥áÊñô„Éô„Éº„ÇπÂõûÁ≠î**

**„ÅîË≥™Âïè**: ${question}

**ÂõûÁ≠î**: ${answer}

**ÂèÇÁÖßË≥áÊñô**: ${documentTitles.join(', ')}

üí° „Åì„ÅÆÂõûÁ≠î„ÅØ„ÄÅ„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„ÅüË≥áÊñô„ÅÆÂÜÖÂÆπ„ÇíÂàÜÊûê„Åó„Å¶ÁîüÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Çà„ÇäË©≥Á¥∞„Å™ÊÉÖÂ†±„ÅåÂøÖË¶Å„Åß„Åó„Åü„Çâ„ÄÅÂÖ∑‰ΩìÁöÑ„Å™Ë≥™Âïè„Çí„ÅäËÅû„Åã„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ`
}

/**
 * Generate intelligent response based on document analysis
 */
function generateIntelligentFallback(
  question: string,
  documentContext: string,
  conversationMemory: ConversationMemory
): Promise<string> {
  return new Promise((resolve) => {
    // Extract document titles from conversation memory or use generic titles
    const documentTitles = ['„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÊ∏à„ÅøË≥áÊñô']
    
    // Analyze document content for relevant information
    const contentAnalysis = analyzeDocumentContent(documentContext, question)
    
    // Try to provide more intelligent responses based on keywords
    const questionLower = question.toLowerCase()
    let intelligentResponse = ''
    
    // Check for specific topics based on question content
    if (questionLower.includes('„Ç∞„É¨„Éº„Éâ') || questionLower.includes('grade')) {
      intelligentResponse = `
**üìä „Ç∞„É¨„Éº„ÉâÂà∂Â∫¶„Å´„Å§„ÅÑ„Å¶**:
„Åì„ÅÆË≥áÊñô„Å´„ÅØ„ÄÅ„Çπ„Éî„Éº„Éâ„É™„É≥„ÇØ„Ç∏„É£„Éë„É≥„ÅÆ„Ç∞„É¨„Éº„ÉâÂà∂Â∫¶ÔºàSLGÔºâ„Å´Èñ¢„Åô„ÇãË©≥Á¥∞„Å™ÊÉÖÂ†±„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ

**‰∏ªË¶Å„Å™„Ç∞„É¨„Éº„ÉâÊßãÊàê**:
‚Ä¢ STEP1: Rookie„ÉªAssociate
‚Ä¢ STEP2: Sub LeaderÔΩûSub Manager  
‚Ä¢ STEP3: ManagerÔΩû

**Âà∂Â∫¶„ÅÆÁâπÂæ¥**:
‚Ä¢ ÂêÑ„Ç∞„É¨„Éº„Éâ„Å´„ÅØÊòéÁ¢∫„Å™ÊòáÊ†ºÊù°‰ª∂„ÅåË®≠ÂÆö
‚Ä¢ „Ç∞„É¨„Éº„ÉâÊâãÂΩì„Å´„Çà„ÇãÂ†±ÈÖ¨‰ΩìÁ≥ª
‚Ä¢ „Éü„ÉÉ„Ç∑„Éß„É≥ÈÅîÊàê„Å´„Çà„ÇãÊòáÈÄ≤„Ç∑„Çπ„ÉÜ„É†
‚Ä¢ ÂÆöÊúüÁöÑ„Å™Ë©ï‰æ°Èù¢Ë´á„Å´„Çà„ÇãÈÄ≤ÊçóÁ¢∫Ë™ç

Ë©≥Á¥∞„Å™ÊòáÊ†ºÊù°‰ª∂„ÇÑÂ†±ÈÖ¨‰ΩìÁ≥ª„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅË≥áÊñôÂÜÖ„ÅÆË©≤ÂΩì„Çª„ÇØ„Ç∑„Éß„É≥„Çí„ÅîÁ¢∫Ë™ç„Åè„Å†„Åï„ÅÑ„ÄÇ`

    } else if (questionLower.includes('Ë©ï‰æ°') || questionLower.includes('evaluation')) {
      intelligentResponse = `
**üìã Ë©ï‰æ°Âà∂Â∫¶„Å´„Å§„ÅÑ„Å¶**:
„Åì„ÅÆË≥áÊñô„Åß„ÅØ„ÄÅ„Çπ„Éî„Éº„Éâ„É™„É≥„ÇØ„Ç∏„É£„Éë„É≥„ÅÆÂåÖÊã¨ÁöÑ„Å™Ë©ï‰æ°Âà∂Â∫¶„Å´„Å§„ÅÑ„Å¶Ë™¨Êòé„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ

**Ë©ï‰æ°Âà∂Â∫¶„ÅÆË¶ÅÁ¥†**:
‚Ä¢ Ë©ï‰æ°Âü∫Ê∫ñ„ÅÆË©≥Á¥∞ÂÆöÁæ©
‚Ä¢ Ë©ï‰æ°‰ΩìÂà∂ÔºàË©ï‰æ°ËÄÖ„ÉªË©ï‰æ°Èù¢Ë´áÔºâ
‚Ä¢ „Éü„ÉÉ„Ç∑„Éß„É≥ÈÅîÊàê„ÅÆÂà§Êñ≠Âü∫Ê∫ñ
‚Ä¢ Ë©ï‰æ°ÁµêÊûú„ÅÆÂ†±ÈÖ¨„Å∏„ÅÆÂèçÊò†ÊñπÊ≥ï

**Ë©ï‰æ°„Éó„É≠„Çª„Çπ**:
‚Ä¢ ÂÆöÊúüÁöÑ„Å™Ë©ï‰æ°Èù¢Ë´á„ÅÆÂÆüÊñΩ
‚Ä¢ „É°„ÉÄ„É´„Ç∑„Éº„Éà„Å´„Çà„ÇãÁõÆÊ®ôË®≠ÂÆö„ÉªÊåØ„ÇäËøî„Çä
‚Ä¢ Âçò‰æ°UP„Éü„ÉÉ„Ç∑„Éß„É≥„ÅÆÈÄ≤ÊçóÁÆ°ÁêÜ
‚Ä¢ Ë≥áÊ†ºÂèñÂæó„Å´„Çà„ÇãËÉΩÂäõË©ï‰æ°

„Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å™Ë©ï‰æ°Âü∫Ê∫ñ„ÇÑÊâãÈ†Ü„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅË≥áÊñô„ÅÆË©≥Á¥∞„Çª„ÇØ„Ç∑„Éß„É≥„Çí„ÅîÂèÇÁÖß„Åè„Å†„Åï„ÅÑ„ÄÇ`

    } else if (questionLower.includes('ÊòáÊ†º') || questionLower.includes('ÊòáÈÄ≤')) {
      intelligentResponse = `
**üéØ ÊòáÊ†º„ÉªÊòáÈÄ≤„Å´„Å§„ÅÑ„Å¶**:
Ë≥áÊñô„Åß„ÅØ„ÄÅÊòéÁ¢∫„Å™ÊòáÊ†ºÊù°‰ª∂„Å®ÊòáÈÄ≤„Éó„É≠„Çª„Çπ„ÅåÂÆö„ÇÅ„Çâ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ

**ÊòáÊ†º„ÅÆÊù°‰ª∂**:
‚Ä¢ ÂêÑ„Ç∞„É¨„Éº„Éâ„É¨„Éô„É´„Åß„ÅÆÂøÖË¶Å„Çπ„Ç≠„É´Áç≤Âæó
‚Ä¢ „Éü„ÉÉ„Ç∑„Éß„É≥ÈÅîÊàêÁä∂Ê≥Å
‚Ä¢ Ë©ï‰æ°Èù¢Ë´á„Åß„ÅÆÁ∑èÂêàÂà§Êñ≠
‚Ä¢ Ë≥áÊ†ºÂèñÂæóÁä∂Ê≥Å

ÊòáÊ†º„Å´Èñ¢„Åô„ÇãË©≥Á¥∞„Å™Ë¶Å‰ª∂„ÅØ„ÄÅÂêÑ„Ç∞„É¨„Éº„Éâ„É¨„Éô„É´„ÅÆË™¨Êòé„Çª„ÇØ„Ç∑„Éß„É≥„ÅßÁ¢∫Ë™ç„Åß„Åç„Åæ„Åô„ÄÇ`
    }
    
    const response = `üìÑ **Ë≥áÊñôÂàÜÊûêÁµêÊûú**

**„ÅîË≥™Âïè**: ${question}

**Èñ¢ÈÄ£Ë≥áÊñô**: ${documentTitles.join(', ')}

${intelligentResponse}

**ÂàÜÊûêÂÜÖÂÆπ**:
${contentAnalysis}

üîç **Ë©≥Á¥∞ÊÉÖÂ†±**:
„Åì„ÅÆÂõûÁ≠î„ÅØ„ÄÅ„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„ÅüË≥áÊñôÔºà${documentTitles.length}‰ª∂Ôºâ„ÅÆÂÜÖÂÆπ„ÇíÂàÜÊûê„Åó„Å¶ÁîüÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ

üí° **„Åï„Çâ„Å´Ë©≥„Åó„ÅèÁü•„Çä„Åü„ÅÑÂ†¥Âêà**:
- „Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å™Ë≥™Âïè„Çí„Åó„Å¶„ÅÑ„Åü„Å†„Åè„Å®„ÄÅË©≤ÂΩìÈÉ®ÂàÜ„ÇíË©≥Á¥∞„Å´ÂàÜÊûê„Åß„Åç„Åæ„Åô
- ÁâπÂÆö„ÅÆÈ†ÖÁõÆ„ÇÑÊï∞ÂÄ§„Å´„Å§„ÅÑ„Å¶„ÅäËÅû„Åç„Åè„Å†„Åï„ÅÑ
- Ë¶ÅÁ¥Ñ„ÇÑÊØîËºÉÂàÜÊûê„ÇÇÂèØËÉΩ„Åß„Åô

**Âà©Áî®ÂèØËÉΩ„Å™Ê©üËÉΩ**: ÂÜÖÂÆπË¶ÅÁ¥Ñ„ÄÅ„Ç≠„Éº„ÉØ„Éº„ÉâÊ§úÁ¥¢„ÄÅ„Éá„Éº„ÇøÂàÜÊûê„ÄÅÊØîËºÉÊ§úË®é`

    resolve(response)
  })
}

/**
 * Analyze document content for relevant information
 */
function analyzeDocumentContent(content: string, question: string): string {
  if (!content || content.trim().length === 0) {
    return "Ë≥áÊñô„ÅÆÂÜÖÂÆπ„ÇíË™≠„ÅøËæº„ÇÅ„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ„Éï„Ç°„Ç§„É´ÂΩ¢Âºè„ÇÑÂÜÖÂÆπ„Çí„ÅîÁ¢∫Ë™ç„Åè„Å†„Åï„ÅÑ„ÄÇ"
  }

  // Simple keyword matching and content analysis
  const questionKeywords = extractKeywords(question)
  const contentLines = content.split('\n').filter(line => line.trim().length > 0)
  
  let relevantContent: string[] = []
  let statistics = {
    totalLines: contentLines.length,
    totalChars: content.length,
    hasNumbers: /\d/.test(content),
    hasDate: /\d{4}[/-]\d{1,2}[/-]\d{1,2}|\d{1,2}[/-]\d{1,2}[/-]\d{4}/.test(content)
  }

  // Find relevant content based on keywords
  let foundMatches = false
  questionKeywords.forEach(keyword => {
    const matches = contentLines.filter(line => 
      line.toLowerCase().includes(keyword.toLowerCase())
    )
    
    if (matches.length > 0) {
      foundMatches = true
      relevantContent.push(`**„Äå${keyword}„Äç„Å´Èñ¢ÈÄ£„Åô„ÇãÂÜÖÂÆπ**:`)
      matches.slice(0, 5).forEach(match => {
        const cleanMatch = match.trim()
        if (cleanMatch.length > 0) {
          relevantContent.push(`‚Ä¢ ${cleanMatch.slice(0, 200)}${cleanMatch.length > 200 ? '...' : ''}`)
        }
      })
      relevantContent.push('') // Add spacing between sections
    }
  })

  if (!foundMatches) {
    // If no keyword matches, provide general document overview
    relevantContent = [
      `**ÊñáÊõ∏„ÅÆÊ¶ÇË¶Å**:`,
      `‚Ä¢ ÊñáÊõ∏„Çµ„Ç§„Ç∫: ${statistics.totalLines}Ë°å„ÄÅ${statistics.totalChars}ÊñáÂ≠ó`,
      `‚Ä¢ Êï∞ÂÄ§„Éá„Éº„Çø: ${statistics.hasNumbers ? 'Âê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô' : 'Âê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì'}`,
      `‚Ä¢ Êó•‰ªòÊÉÖÂ†±: ${statistics.hasDate ? 'Âê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô' : 'Âê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì'}`,
      '',
      `**ÂÜÖÂÆπ„ÅÆ‰∏ÄÈÉ®**:`,
      ...contentLines.slice(0, 10).map(line => {
        const cleanLine = line.trim()
        if (cleanLine.length > 0) {
          return `‚Ä¢ ${cleanLine.slice(0, 150)}${cleanLine.length > 150 ? '...' : ''}`
        }
        return null
      }).filter((item): item is string => item !== null).slice(0, 5)
    ]
  }

  return relevantContent.join('\n')
}

/**
 * Extract keywords from question
 */
function extractKeywords(question: string): string[] {
  // Remove common Japanese particles and extract meaningful words
  const stopWords = ['„ÅØ', '„Åå', '„Çí', '„Å´', '„Åß', '„Å®', '„ÅÆ', '„Åã„Çâ', '„Åæ„Åß', '„Å´„Å§„ÅÑ„Å¶', '„Å®„ÅÑ„ÅÜ', '„Åß„Åô', '„Åæ„Åô', '„Åß„ÅÇ„Çã', '„Å©„ÅÆ„Çà„ÅÜ„Å™', '„Å©„Çì„Å™', '„Å™„Å´', '„Å™„Åú', '„ÅÑ„Å§', '„Å©„Åì', '„Å†„Çå', '„Å©„ÅÜ„ÇÑ„Å£„Å¶']
  
  // Split by various separators and clean up
  const words = question
    .replace(/[ÔºüÔºÅ„ÄÇ„ÄÅ,Ôºå]/g, ' ')
    .split(/[\s\u3000]+/)
    .filter(word => word.length > 1 && !stopWords.includes(word))
    .map(word => word.replace(/[ÔºüÔºÅ„ÄÇ„ÄÅ]/g, ''))
    .filter(word => word.length > 0)
  
  return Array.from(new Set(words)).slice(0, 5) // Remove duplicates and limit to 5 keywords
}

/**
 * Check if Hugging Face API is available
 */
export async function checkHuggingFaceConnection(): Promise<boolean> {
  try {
    // Try with a simpler model
    await hf.textGeneration({
      model: 'gpt2',
      inputs: 'Hello',
      parameters: { max_new_tokens: 5 }
    })
    return true
  } catch (error) {
    console.error('Hugging Face connection check failed:', error)
    return false
  }
}

/**
 * Enhanced conversation management with streaming support
 */
export class EnhancedConversationMemory extends ConversationMemory {
  private contextSummary: string = ''
  
  constructor(maxTurns: number = 15, maxTokens: number = 3000) {
    super(maxTurns, maxTokens)
  }

  /**
   * Get optimized context for ChatGPT-like responses
   */
  getChatGPTContext(): string {
    const recentContext = this.getContext()
    const summary = this.getSummary()
    
    if (this.contextSummary && recentContext.length < 500) {
      return `ÈÅéÂéª„ÅÆ‰ºöË©±Ê¶ÇË¶Å: ${this.contextSummary}\n\nÊúÄËøë„ÅÆ‰ºöË©±:\n${recentContext}`
    }
    
    return recentContext
  }

  /**
   * Update context summary for long conversations
   */
  updateContextSummary(summary: string) {
    this.contextSummary = summary
  }

  /**
   * Analyze conversation patterns for better responses
   */
  getConversationPatterns(): { 
    frequentTopics: string[], 
    userPreferences: string[], 
    conversationStyle: string 
  } {
    const allUserMessages = this.history
      .filter(turn => turn.role === 'user')
      .map(turn => turn.content.toLowerCase())
    
    // Simple keyword extraction for topics
    const words = allUserMessages.join(' ').split(/\s+/)
    const wordCounts = words.reduce((acc, word) => {
      if (word.length > 3) {
        acc[word] = (acc[word] || 0) + 1
      }
      return acc
    }, {} as Record<string, number>)
    
    const frequentTopics = Object.entries(wordCounts)
      .sort(([,a], [,b]) => b - a)
      .slice(0, 5)
      .map(([word]) => word)
    
    // Determine conversation style
    const hasPoliteLanguage = allUserMessages.some(msg => 
      msg.includes('„ÅäÈ°ò„ÅÑ') || msg.includes('„Åè„Å†„Åï„ÅÑ') || msg.includes('„ÅÇ„Çä„Åå„Å®„ÅÜ')
    )
    
    const hasInformalLanguage = allUserMessages.some(msg =>
      msg.includes('„Å†„Çà„Å≠') || msg.includes('„Å£„Å¶„ÅÑ„ÅÜ') || msg.includes('„ÇÑ„Å∞„ÅÑ')
    )
    
    let conversationStyle = 'neutral'
    if (hasPoliteLanguage && !hasInformalLanguage) conversationStyle = 'formal'
    if (hasInformalLanguage && !hasPoliteLanguage) conversationStyle = 'casual'
    
    return {
      frequentTopics,
      userPreferences: [], // Could be enhanced based on user feedback
      conversationStyle
    }
  }
}

/**
 * ChatGPT-like streaming response generation
 */
export async function* generateStreamingResponse(
  message: string,
  conversationMemory: EnhancedConversationMemory,
  documentContext?: string
): AsyncGenerator<string, void, unknown> {
  try {
    // Get conversation patterns for personalized responses
    const patterns = conversationMemory.getConversationPatterns()
    const chatContext = conversationMemory.getChatGPTContext()
    
    // Create enhanced prompt
    let systemPrompt = `„ÅÇ„Å™„Åü„ÅØË¶™Âàá„ÅßÁü•Ë≠òË±äÂØå„Å™AI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ`
    
    // Adapt response style based on conversation patterns
    switch (patterns.conversationStyle) {
      case 'formal':
        systemPrompt += `‰∏ÅÂØß„ÅßÊ†ºÂºè„ÅÇ„ÇãÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ`
        break
      case 'casual':
        systemPrompt += `„Éï„É¨„É≥„Éâ„É™„Éº„ÅßË¶™„Åó„Åø„ÇÑ„Åô„ÅÑÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ`
        break
      default:
        systemPrompt += `Ëá™ÁÑ∂„ÅßÂàÜ„Åã„Çä„ÇÑ„Åô„ÅÑÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ`
    }
    
    if (documentContext) {
      systemPrompt += `\n\nÂèÇËÄÉË≥áÊñô:\n${documentContext.slice(0, 2000)}`
    }
    
    if (chatContext) {
      systemPrompt += `\n\n‰ºöË©±Â±•Ê≠¥:\n${chatContext}`
    }
    
    const fullPrompt = `${systemPrompt}\n\n„É¶„Éº„Ç∂„Éº: ${message}\n„Ç¢„Ç∑„Çπ„Çø„É≥„Éà:`
    
    // Use Hugging Face streaming if available
    const response = await hf.textGeneration({
      model: MODELS.CHAT,
      inputs: fullPrompt,
      parameters: {
        max_new_tokens: 500,
        temperature: 0.7,
        top_p: 0.9,
        do_sample: true,
        repetition_penalty: 1.1,
        return_full_text: false,
        stop: ['„É¶„Éº„Ç∂„Éº:', 'User:', 'Human:']
      }
    })
    
    // Simulate streaming by yielding chunks of the response
    const fullResponse = response.generated_text.trim()
    const chunks = fullResponse.split(/([„ÄÇÔºÅÔºü])/g)
    
    let accumulatedResponse = ''
    for (const chunk of chunks) {
      if (chunk.trim()) {
        accumulatedResponse += chunk
        yield accumulatedResponse
        
        // Add small delay to simulate real streaming
        await new Promise(resolve => setTimeout(resolve, 50))
      }
    }
    
    // Add to conversation memory
    conversationMemory.addTurn('user', message, documentContext)
    conversationMemory.addTurn('assistant', fullResponse, documentContext)
    
  } catch (error) {
    console.error('Streaming response error:', error)
    
    // Fallback to non-streaming intelligent response
    const fallbackResponse = await generateIntelligentFallback(message, documentContext || '', conversationMemory)
    yield fallbackResponse
  }
}
