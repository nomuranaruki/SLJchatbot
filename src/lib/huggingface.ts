import { HfInference } from '@huggingface/inference'

// Hugging Face API client initialization
export const hf = new HfInference(process.env.HUGGINGFACE_API_KEY)

// Available models for different tasks - updated with gpt-oss models
export const MODELS = {
  // Primary conversation model - OpenAI gpt-oss for high-quality reasoning
  CHAT: 'openai/gpt-oss-120b',
  // Alternative high-performance model  
  CHAT_ALTERNATIVE: 'openai/gpt-oss-20b',
  // Backup lightweight model
  SIMPLE_CHAT: 'microsoft/DialoGPT-medium', 
  // Question answering model
  QA: 'deepset/roberta-base-squad2',
  // Text generation with gpt-oss
  TEXT_GENERATION: 'openai/gpt-oss-120b',
  // Summarization
  SUMMARIZATION: 'facebook/bart-large-cnn',
  // Fast QA for quick responses
  FAST_QA: 'distilbert-base-cased-distilled-squad'
}

// API configuration
const API_CONFIG = {
  baseURL: process.env.HUGGINGFACE_API_URL || 'https://api-inference.huggingface.co/models',
  headers: {
    'Authorization': `Bearer ${process.env.HUGGINGFACE_API_KEY}`,
    'Content-Type': 'application/json',
  },
  timeout: 30000,
  retries: 3
}

/**
 * Conversation Turn interface
 */
interface ConversationTurn {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
  documentContext?: string
}

/**
 * Advanced conversation memory management
 */
export class ConversationMemory {
  protected history: ConversationTurn[] = []
  private maxTurns: number = 10
  private maxTokens: number = 2000

  constructor(maxTurns: number = 10, maxTokens: number = 2000) {
    this.maxTurns = maxTurns
    this.maxTokens = maxTokens
  }

  /**
   * Add a conversation turn
   */
  addTurn(role: 'user' | 'assistant', content: string, documentContext?: string) {
    this.history.push({
      role,
      content,
      timestamp: new Date(),
      documentContext
    })

    // Keep only recent turns
    if (this.history.length > this.maxTurns) {
      this.history = this.history.slice(-this.maxTurns)
    }
  }

  /**
   * Get conversation context for model input
   */
  getContext(): string {
    let context = ''
    let tokenCount = 0

    // Build context from most recent conversations
    for (let i = this.history.length - 1; i >= 0; i--) {
      const turn = this.history[i]
      const turnText = `${turn.role === 'user' ? 'Human' : 'Assistant'}: ${turn.content}\n`
      
      // Rough token estimation (1 token â‰ˆ 4 characters for English/Japanese)
      const turnTokens = turnText.length / 4
      
      if (tokenCount + turnTokens > this.maxTokens) {
        break
      }
      
      context = turnText + context
      tokenCount += turnTokens
    }

    return context
  }

  /**
   * Get recent document context
   */
  getRecentDocumentContext(): string {
    const recentTurns = this.history.slice(-3) // Last 3 turns
    const documentContexts = recentTurns
      .filter(turn => turn.documentContext)
      .map(turn => turn.documentContext)
    
    return documentContexts.length > 0 ? documentContexts[documentContexts.length - 1]! : ''
  }

  /**
   * Clear conversation history
   */
  clear() {
    this.history = []
  }

  /**
   * Get conversation summary for long-term memory
   */
  getSummary(): string {
    if (this.history.length === 0) return ''
    
    const topics = new Set<string>()
    this.history.forEach(turn => {
      if (turn.role === 'user') {
        // Extract potential topics from user messages
        const words = turn.content.split(/\s+/).filter(w => w.length > 2)
        words.slice(0, 3).forEach(word => topics.add(word))
      }
    })

    return `ä¼šè©±ã®ãƒˆãƒ”ãƒƒã‚¯: ${Array.from(topics).join(', ')}`
  }
}

/**
 * Generate text response using Hugging Face model
 */
export async function generateResponse(
  prompt: string,
  context?: string,
  model: string = MODELS.CHAT
): Promise<string> {
  try {
    // Prepare the input with context if provided
    const input = context 
      ? `Context: ${context}\n\nQuestion: ${prompt}\n\nAnswer:`
      : prompt

    const response = await hf.textGeneration({
      model,
      inputs: input,
      parameters: {
        max_new_tokens: 500,
        temperature: 0.7,
        do_sample: true,
        repetition_penalty: 1.1,
        return_full_text: false
      }
    })

    return response.generated_text.trim()
  } catch (error) {
    console.error('Hugging Face API error:', error)
    throw new Error('AI response generation failed')
  }
}

/**
 * Alias for generateResponse for backward compatibility
 */
export const generateChatResponse = generateResponse

/**
 * Answer questions based on document context
 */
export async function answerQuestion(
  question: string,
  context: string
): Promise<string> {
  try {
    const response = await hf.questionAnswering({
      model: MODELS.QA,
      inputs: {
        question,
        context
      }
    })

    return response.answer || 'Sorry, I could not find an answer in the provided context.'
  } catch (error) {
    console.error('Question answering error:', error)
    
    // Fallback to text generation
    return generateResponse(question, context)
  }
}

/**
 * Summarize document content
 */
export async function summarizeText(text: string): Promise<string> {
  try {
    // Split long text into chunks if needed
    const maxLength = 1024
    if (text.length > maxLength) {
      const chunks = []
      for (let i = 0; i < text.length; i += maxLength) {
        chunks.push(text.slice(i, i + maxLength))
      }
      
      const summaries = await Promise.all(
        chunks.map(chunk => hf.summarization({
          model: MODELS.SUMMARIZATION,
          inputs: chunk,
          parameters: {
            max_length: 150,
            min_length: 30
          }
        }))
      )
      
      return summaries.map(s => s.summary_text).join(' ')
    }

    const response = await hf.summarization({
      model: MODELS.SUMMARIZATION,
      inputs: text,
      parameters: {
        max_length: 150,
        min_length: 30
      }
    })

    return response.summary_text
  } catch (error) {
    console.error('Summarization error:', error)
    return 'Sorry, I could not summarize the text.'
  }
}

/**
 * Enhanced ChatGPT-like response generation with conversation memory
 */
export async function generateAdvancedChatResponse(
  message: string,
  conversationMemory: ConversationMemory,
  documentContext?: string
): Promise<string> {
  try {
    // Get conversation history
    const conversationHistory = conversationMemory.getContext()
    const recentDocContext = documentContext || conversationMemory.getRecentDocumentContext()
    
    // Create enhanced prompt with system instructions
    let prompt = `ä»¥ä¸‹ã¯ã€ä¼æ¥­æ–‡æ›¸ç®¡ç†AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ä¼šè©±ã§ã™ã€‚AIã¯è¦ªåˆ‡ã§ã€è©³ç´°ã§ã€æ­£ç¢ºãªå›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚`
    
    // Add document context if available
    if (recentDocContext) {
      prompt += `\n\nå‚è€ƒè³‡æ–™:\n${recentDocContext.slice(0, 1500)}`
    }
    
    // Add conversation history
    if (conversationHistory) {
      prompt += `\n\néå»ã®ä¼šè©±:\n${conversationHistory}`
    }
    
    // Add current user message
    prompt += `\nHuman: ${message}\nAssistant:`

    // Call Hugging Face API with improved parameters
    const response = await hf.textGeneration({
      model: MODELS.CHAT,
      inputs: prompt,
      parameters: {
        max_new_tokens: 400,
        temperature: 0.8,
        top_p: 0.95,
        do_sample: true,
        repetition_penalty: 1.15,
        return_full_text: false,
        stop: ['Human:', 'Assistant:', '\n\n']
      }
    })

    let generatedText = response.generated_text.trim()
    
    // Post-process the response
    generatedText = enhanceResponseQuality(generatedText, message, recentDocContext)
    
    // Add to conversation memory
    conversationMemory.addTurn('user', message, documentContext)
    conversationMemory.addTurn('assistant', generatedText, documentContext)
    
    return generatedText

  } catch (error) {
    console.error('Enhanced chat response error:', error)
    
    // Intelligent fallback based on message content
    const fallbackResponse = await generateIntelligentFallback(message, documentContext || '', conversationMemory)
    
    // Still add to memory even for fallback
    conversationMemory.addTurn('user', message, documentContext)
    conversationMemory.addTurn('assistant', fallbackResponse, documentContext)
    
    return fallbackResponse
  }
}

/**
 * Enhance response quality with post-processing
 */
function enhanceResponseQuality(response: string, originalQuestion: string, documentContext?: string): string {
  let enhanced = response
  
  // Remove incomplete sentences at the end
  const sentences = enhanced.split(/[ã€‚ï¼ï¼Ÿ]/)
  if (sentences.length > 1 && sentences[sentences.length - 1].trim().length < 10) {
    enhanced = sentences.slice(0, -1).join('ã€‚') + 'ã€‚'
  }
  
  // Ensure response is relevant to the question
  if (enhanced.length < 20) {
    enhanced = generateContextAwareResponse(originalQuestion, documentContext)
  }
  
  // Add helpful formatting for document-based responses
  if (documentContext && !enhanced.includes('ğŸ“„')) {
    enhanced = `ğŸ“„ **æ–‡æ›¸ãƒ™ãƒ¼ã‚¹å›ç­”**\n\n${enhanced}`
  }
  
  return enhanced
}

/**
 * Generate context-aware response for better fallback
 */
function generateContextAwareResponse(question: string, documentContext?: string): string {
  const questionLower = question.toLowerCase()
  
  // Analyze question intent
  if (questionLower.includes('æ•™ãˆã¦') || questionLower.includes('èª¬æ˜')) {
    if (documentContext) {
      return `ã”è³ªå•ã€Œ${question}ã€ã«ã¤ã„ã¦ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸè³‡æ–™ã‚’å‚è€ƒã«å›ç­”ã„ãŸã—ã¾ã™ã€‚å…·ä½“çš„ã«ãŠçŸ¥ã‚Šã«ãªã‚ŠãŸã„ç‚¹ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ã‚ˆã‚Šè©³ã—ããŠèã‹ã›ãã ã•ã„ã€‚`
    } else {
      return `ã€Œ${question}ã€ã«ã¤ã„ã¦å›ç­”ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚é–¢é€£ã™ã‚‹è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ãŸã ãã¨ã€ã‚ˆã‚Šè©³ç´°ã§æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã§ãã¾ã™ã€‚`
    }
  }
  
  if (questionLower.includes('æ–¹æ³•') || questionLower.includes('ã‚„ã‚Šæ–¹')) {
    return `ã€Œ${question}ã€ã®æ‰‹é †ã«ã¤ã„ã¦èª¬æ˜ã„ãŸã—ã¾ã™ã€‚æ®µéšçš„ãªæ‰‹é †ã‚„å…·ä½“çš„ãªæ–¹æ³•ã‚’ãŠçŸ¥ã‚Šã«ãªã‚ŠãŸã„å ´åˆã¯ã€ã‚ˆã‚Šè©³ã—ããŠèã‹ã›ãã ã•ã„ã€‚`
  }
  
  // Default response
  return `ã€Œ${question}ã€ã«ã¤ã„ã¦æ‰¿ã‚Šã¾ã—ãŸã€‚ã‚ˆã‚Šå…·ä½“çš„ãªæƒ…å ±ã‚„è©³ç´°ã‚’ãŠæ±‚ã‚ã®å ´åˆã¯ã€é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ãŸã ãã‹ã€å…·ä½“çš„ãªè³ªå•ã‚’ãŠèã‹ã›ãã ã•ã„ã€‚`
}

/**
 * Generate intelligent document-based response
 */
export async function generateDocumentBasedResponse(
  question: string,
  documentContext: string,
  documentTitles: string[]
): Promise<string> {
  // First try Hugging Face API
  try {
    // Try question answering first
    const qaResponse = await answerQuestion(question, documentContext)
    
    if (qaResponse && !qaResponse.includes('Sorry, I could not find')) {
      // Enhance the QA response with context
      return enhanceResponseWithContext(qaResponse, question, documentTitles)
    }
  } catch (error) {
    console.log('QA model failed, trying text generation...')
  }

  // Fallback to intelligent analysis of document content
  const conversationMemory = new ConversationMemory()
  return generateIntelligentFallback(question, documentContext, conversationMemory)
}

/**
 * Enhance QA response with contextual information
 */
function enhanceResponseWithContext(
  answer: string, 
  question: string, 
  documentTitles: string[]
): string {
  return `ğŸ“„ **è³‡æ–™ãƒ™ãƒ¼ã‚¹å›ç­”**

**ã”è³ªå•**: ${question}

**å›ç­”**: ${answer}

**å‚ç…§è³‡æ–™**: ${documentTitles.join(', ')}

ğŸ’¡ ã“ã®å›ç­”ã¯ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸè³‡æ–™ã®å†…å®¹ã‚’åˆ†æã—ã¦ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ãŒå¿…è¦ã§ã—ãŸã‚‰ã€å…·ä½“çš„ãªè³ªå•ã‚’ãŠèã‹ã›ãã ã•ã„ã€‚`
}

/**
 * Generate intelligent response based on document analysis
 */
export function generateIntelligentFallback(
  question: string,
  documentContext: string,
  conversationMemory?: ConversationMemory
): Promise<string> {
  return new Promise((resolve) => {
    // Extract document titles from conversation memory or use generic titles
    const documentTitles = ['ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿è³‡æ–™']
    
    // Advanced content analysis for structured information extraction
    const structuredAnalysis = performAdvancedContentAnalysis(documentContext, question)
    
    // Try to provide more intelligent responses based on keywords
    const questionLower = question.toLowerCase()
    let intelligentResponse = ''
    
    // Enhanced keyword analysis for detailed responses
    if (questionLower.includes('ã‚°ãƒ¬ãƒ¼ãƒ‰') || questionLower.includes('grade') || questionLower.includes('slg')) {
      const gradeInfo = extractGradeInformation(documentContext)
      intelligentResponse = `
**ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¶åº¦ï¼ˆSLGï¼‰è©³ç´°æƒ…å ±**

${gradeInfo.overview}

**ğŸ¯ å„ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«**:
${gradeInfo.levels}

**ğŸ’° å ±é…¬ä½“ç³»**:
${gradeInfo.compensation}

**ğŸ“ˆ æ˜‡æ ¼ãƒ—ãƒ­ã‚»ã‚¹**:
${gradeInfo.promotion}

**ğŸ“‹ è©•ä¾¡åŸºæº–**:
${gradeInfo.evaluation}`

    } else if (questionLower.includes('è©•ä¾¡') || questionLower.includes('evaluation') || questionLower.includes('ãƒ¡ãƒ€ãƒ«ã‚·ãƒ¼ãƒˆ')) {
      const evaluationInfo = extractEvaluationInformation(documentContext)
      intelligentResponse = `
**ğŸ“‹ è©•ä¾¡åˆ¶åº¦è©³ç´°**

${evaluationInfo.overview}

**ğŸ¯ è©•ä¾¡è¦ç´ **:
${evaluationInfo.criteria}

**ğŸ“Š ãƒ¡ãƒ€ãƒ«ã‚·ãƒ¼ãƒˆæ´»ç”¨**:
${evaluationInfo.medalSheet}

**ğŸ’¬ è©•ä¾¡é¢è«‡**:
${evaluationInfo.interviews}

**ğŸ“ˆ æˆæœåæ˜ **:
${evaluationInfo.rewards}`

    } else if (questionLower.includes('ãƒŸãƒƒã‚·ãƒ§ãƒ³') || questionLower.includes('mission') || questionLower.includes('å˜ä¾¡up')) {
      const missionInfo = extractMissionInformation(documentContext)
      intelligentResponse = `
**ğŸ¯ ãƒŸãƒƒã‚·ãƒ§ãƒ³åˆ¶åº¦è©³ç´°**

${missionInfo.overview}

**ğŸ’° å˜ä¾¡UPãƒŸãƒƒã‚·ãƒ§ãƒ³**:
${missionInfo.salaryMissions}

**ğŸ† ãã®ä»–ãƒŸãƒƒã‚·ãƒ§ãƒ³**:
${missionInfo.otherMissions}

**ğŸ“Š é€²æ—ç®¡ç†**:
${missionInfo.progress}

**ğŸ é”æˆå ±é…¬**:
${missionInfo.rewards}`

    } else if (questionLower.includes('æ˜‡æ ¼') || questionLower.includes('æ˜‡é€²') || questionLower.includes('promotion')) {
      const promotionInfo = extractPromotionInformation(documentContext)
      intelligentResponse = `
**ğŸ¯ æ˜‡æ ¼ãƒ»æ˜‡é€²åˆ¶åº¦**

${promotionInfo.overview}

**ğŸ“‹ æ˜‡æ ¼æ¡ä»¶**:
${promotionInfo.requirements}

**ğŸ“ˆ æ˜‡æ ¼ãƒ—ãƒ­ã‚»ã‚¹**:
${promotionInfo.process}

**ğŸ’° æ˜‡æ ¼ã«ã‚ˆã‚‹å¤‰åŒ–**:
${promotionInfo.benefits}`

    } else if (questionLower.includes('è³‡æ ¼') || questionLower.includes('ã‚¹ã‚­ãƒ«') || questionLower.includes('skill')) {
      const skillInfo = extractSkillInformation(documentContext)
      intelligentResponse = `
**ğŸ“ ã‚¹ã‚­ãƒ«ãƒ»è³‡æ ¼åˆ¶åº¦**

${skillInfo.overview}

**ğŸ“š å¿…è¦è³‡æ ¼**:
${skillInfo.requirements}

**ğŸ’° è³‡æ ¼æ‰‹å½“**:
${skillInfo.allowances}

**ğŸ“ˆ ã‚¹ã‚­ãƒ«è©•ä¾¡**:
${skillInfo.evaluation}`
    }

    // If no specific topic match, provide comprehensive analysis
    if (!intelligentResponse) {
      intelligentResponse = generateComprehensiveAnalysis(documentContext, question)
    }
    
    const response = `ğŸ“„ **è©³ç´°åˆ†æçµæœ**

**ã”è³ªå•**: ${question}

**ğŸ“‚ å‚ç…§è³‡æ–™**: ${documentTitles.join(', ')}

${intelligentResponse}

**ğŸ” æ§‹é€ åŒ–åˆ†æ**:
${structuredAnalysis}

**ğŸ’¡ è¿½åŠ æƒ…å ±**:
ã“ã®å›ç­”ã¯ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸè³‡æ–™ã®è©³ç´°åˆ†æã«åŸºã¥ã„ã¦ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ã€‚
å…·ä½“çš„ãªæ•°å€¤ã€æ¡ä»¶ã€æ‰‹é †ç­‰ã‚’å«ã‚€åŒ…æ‹¬çš„ãªæƒ…å ±ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

**ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½**:
â€¢ è©³ç´°ãªåˆ¶åº¦èª¬æ˜
â€¢ å…·ä½“çš„ãªæ•°å€¤ãƒ»æ¡ä»¶ã®æŠ½å‡º
â€¢ æ®µéšçš„ãªãƒ—ãƒ­ã‚»ã‚¹èª¬æ˜
â€¢ æ¯”è¼ƒåˆ†æãƒ»è¦ç´„
â€¢ é–¢é€£é …ç›®ã®æ¨ªæ–­çš„åˆ†æ

**ğŸ¤ ã•ã‚‰ãªã‚‹ã‚µãƒãƒ¼ãƒˆ**:
ç‰¹å®šã®é …ç›®ã«ã¤ã„ã¦ã‚ˆã‚Šè©³ã—ãçŸ¥ã‚ŠãŸã„å ´åˆã¯ã€å…·ä½“çš„ã«ãŠèã‹ã›ãã ã•ã„ã€‚`

    resolve(response)
  })
}

/**
 * Analyze document content for relevant information
 */
function analyzeDocumentContent(content: string, question: string): string {
  if (!content || content.trim().length === 0) {
    return "è³‡æ–™ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚„å†…å®¹ã‚’ã”ç¢ºèªãã ã•ã„ã€‚"
  }

  // Simple keyword matching and content analysis
  const questionKeywords = extractKeywords(question)
  const contentLines = content.split('\n').filter(line => line.trim().length > 0)
  
  let relevantContent: string[] = []
  let statistics = {
    totalLines: contentLines.length,
    totalChars: content.length,
    hasNumbers: /\d/.test(content),
    hasDate: /\d{4}[/-]\d{1,2}[/-]\d{1,2}|\d{1,2}[/-]\d{1,2}[/-]\d{4}/.test(content)
  }

  // Find relevant content based on keywords
  let foundMatches = false
  questionKeywords.forEach(keyword => {
    const matches = contentLines.filter(line => 
      line.toLowerCase().includes(keyword.toLowerCase())
    )
    
    if (matches.length > 0) {
      foundMatches = true
      relevantContent.push(`**ã€Œ${keyword}ã€ã«é–¢é€£ã™ã‚‹å†…å®¹**:`)
      matches.slice(0, 5).forEach(match => {
        const cleanMatch = match.trim()
        if (cleanMatch.length > 0) {
          relevantContent.push(`â€¢ ${cleanMatch.slice(0, 200)}${cleanMatch.length > 200 ? '...' : ''}`)
        }
      })
      relevantContent.push('') // Add spacing between sections
    }
  })

  if (!foundMatches) {
    // If no keyword matches, provide general document overview
    relevantContent = [
      `**æ–‡æ›¸ã®æ¦‚è¦**:`,
      `â€¢ æ–‡æ›¸ã‚µã‚¤ã‚º: ${statistics.totalLines}è¡Œã€${statistics.totalChars}æ–‡å­—`,
      `â€¢ æ•°å€¤ãƒ‡ãƒ¼ã‚¿: ${statistics.hasNumbers ? 'å«ã¾ã‚Œã¦ã„ã¾ã™' : 'å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“'}`,
      `â€¢ æ—¥ä»˜æƒ…å ±: ${statistics.hasDate ? 'å«ã¾ã‚Œã¦ã„ã¾ã™' : 'å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“'}`,
      '',
      `**å†…å®¹ã®ä¸€éƒ¨**:`,
      ...contentLines.slice(0, 10).map(line => {
        const cleanLine = line.trim()
        if (cleanLine.length > 0) {
          return `â€¢ ${cleanLine.slice(0, 150)}${cleanLine.length > 150 ? '...' : ''}`
        }
        return null
      }).filter((item): item is string => item !== null).slice(0, 5)
    ]
  }

  return relevantContent.join('\n')
}

/**
 * Check if Hugging Face API is available
 */
export async function checkHuggingFaceConnection(): Promise<boolean> {
  try {
    // Try with a simpler model
    await hf.textGeneration({
      model: 'gpt2',
      inputs: 'Hello',
      parameters: { max_new_tokens: 5 }
    })
    return true
  } catch (error) {
    console.error('Hugging Face connection check failed:', error)
    return false
  }
}

/**
 * Advanced content analysis for structured information extraction
 */
function performAdvancedContentAnalysis(content: string, question: string): string {
  if (!content || content.trim().length === 0) {
    return "âš ï¸ è³‡æ–™ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚"
  }

  const lines = content.split('\n').filter(line => line.trim().length > 0)
  const totalChars = content.length
  const hasNumbers = /\d/.test(content)
  const hasDate = /\d{4}[/-]\d{1,2}[/-]\d{1,2}|\d{1,2}[/-]\d{1,2}[/-]\d{4}/.test(content)
  
  // Extract key metrics and structured data
  const bullets = lines.filter(line => line.trim().match(/^[â€¢ãƒ»â–ªâ–«â– â–¡â—â—‹]\s/)).length
  const numberedItems = lines.filter(line => line.trim().match(/^\d+[.)]\s/)).length
  const sections = lines.filter(line => line.trim().match(/^(ç¬¬\d+ç« |ç¬¬\d+æ¡|Â§\d+|Chapter\s+\d+|Section\s+\d+)/i)).length
  
  return `**ğŸ“Š æ–‡æ›¸çµ±è¨ˆ**:
â€¢ ç·è¡Œæ•°: ${lines.length}è¡Œ (${totalChars}æ–‡å­—)
â€¢ ç®‡æ¡æ›¸ãé …ç›®: ${bullets}å€‹
â€¢ ç•ªå·ä»˜ããƒªã‚¹ãƒˆ: ${numberedItems}å€‹
â€¢ ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: ${sections}å€‹
â€¢ æ•°å€¤ãƒ‡ãƒ¼ã‚¿: ${hasNumbers ? 'å«ã‚€' : 'ãªã—'}
â€¢ æ—¥ä»˜æƒ…å ±: ${hasDate ? 'å«ã‚€' : 'ãªã—'}

**ğŸ” æ§‹é€ åŒ–è¦ç´ **: ä½“ç³»çš„ã«æ•´ç†ã•ã‚ŒãŸåˆ¶åº¦æ–‡æ›¸ã¨ã—ã¦åˆ†ææ¸ˆã¿`
}

/**
 * Extract grade information from document content
 */
function extractGradeInformation(content: string): {
  overview: string;
  levels: string;
  compensation: string;
  promotion: string;
  evaluation: string;
} {
  const lines = content.toLowerCase()
  
  // Extract specific grade-related information
  const gradeTerms = ['grade', 'ã‚°ãƒ¬ãƒ¼ãƒ‰', 'slg', 'ã‚¹ãƒ†ãƒƒãƒ—', 'ãƒ¬ãƒ™ãƒ«', 'rookie', 'associate', 'leader', 'manager']
  const relevantLines = content.split('\n').filter(line => 
    gradeTerms.some(term => line.toLowerCase().includes(term))
  )
  
  // Extract salary/compensation info
  const salaryInfo = content.split('\n').filter(line => 
    line.toLowerCase().includes('å††') || 
    line.toLowerCase().includes('æ‰‹å½“') || 
    line.toLowerCase().includes('å ±é…¬') ||
    line.toLowerCase().includes('çµ¦ä¸')
  )
  
  return {
    overview: relevantLines.length > 0 
      ? `ã‚¹ãƒ”ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã‚¸ãƒ£ãƒ‘ãƒ³ã®ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¶åº¦ï¼ˆSLGï¼‰ã¯ã€å¾“æ¥­å“¡ã®ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«ã¨è²¬ä»»ç¯„å›²ã«å¿œã˜ãŸæ®µéšçš„ãªã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚ç¾åœ¨ã®è³‡æ–™ã§ã¯${relevantLines.length}ä»¶ã®ã‚°ãƒ¬ãƒ¼ãƒ‰é–¢é€£æƒ…å ±ãŒç¢ºèªã•ã‚Œã¦ã„ã¾ã™ã€‚`
      : "ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¶åº¦ã®åŸºæœ¬æ§‹é€ ãŒå®šç¾©ã•ã‚Œã¦ãŠã‚Šã€æ˜ç¢ºãªã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ã¨è©•ä¾¡åŸºæº–ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚",
    
    levels: relevantLines.slice(0, 10).map((line, index) => 
      `â€¢ ${line.trim().substring(0, 200)}`
    ).join('\n') || `
â€¢ STEP1: Rookieãƒ»Associateï¼ˆåŸºç¤ãƒ¬ãƒ™ãƒ«ï¼‰
â€¢ STEP2: Sub Leaderï½Sub Managerï¼ˆä¸­ç´šãƒ¬ãƒ™ãƒ«ï¼‰  
â€¢ STEP3: Managerï½ï¼ˆä¸Šç´šãƒ¬ãƒ™ãƒ«ï¼‰
å„ãƒ¬ãƒ™ãƒ«ã§ã¯æ˜ç¢ºãªå½¹å‰²ã¨è²¬ä»»ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã€‚`,
    
    compensation: salaryInfo.length > 0 
      ? salaryInfo.slice(0, 5).map(line => `â€¢ ${line.trim()}`).join('\n')
      : `
â€¢ ã‚°ãƒ¬ãƒ¼ãƒ‰æ‰‹å½“ã«ã‚ˆã‚‹åŸºæœ¬å ±é…¬ã®å·®åˆ¥åŒ–
â€¢ ãƒŸãƒƒã‚·ãƒ§ãƒ³é”æˆã«ã‚ˆã‚‹è¿½åŠ å ±é…¬
â€¢ æ˜‡æ ¼æ™‚ã®çµ¦ä¸æ”¹å®šã‚·ã‚¹ãƒ†ãƒ 
â€¢ è³‡æ ¼å–å¾—ã«ã‚ˆã‚‹æ‰‹å½“åŠ ç®—`,
    
    promotion: `
â€¢ å„ã‚°ãƒ¬ãƒ¼ãƒ‰ã§ã®å¿…è¦ã‚¹ã‚­ãƒ«ãƒ»çµŒé¨“ã®ç²å¾—
â€¢ ãƒŸãƒƒã‚·ãƒ§ãƒ³é”æˆçŠ¶æ³ã®è©•ä¾¡
â€¢ å®šæœŸçš„ãªè©•ä¾¡é¢è«‡ã§ã®ç·åˆåˆ¤å®š
â€¢ ä¸Šä½è€…ã‹ã‚‰ã®æ¨è–¦ãŠã‚ˆã³æ‰¿èªãƒ—ãƒ­ã‚»ã‚¹`,
    
    evaluation: `
â€¢ æ¥­å‹™é‚è¡Œèƒ½åŠ›ã®å®¢è¦³çš„è©•ä¾¡
â€¢ ãƒŸãƒƒã‚·ãƒ§ãƒ³é”æˆåº¦ã®å®šé‡çš„æ¸¬å®š
â€¢ ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã‚¹ã‚­ãƒ«
â€¢ ç¶™ç¶šçš„ãªå­¦ç¿’ãƒ»æˆé•·æ„æ¬²ã®ç¢ºèª`
  }
}

/**
 * Extract evaluation information from document content
 */
function extractEvaluationInformation(content: string): {
  overview: string;
  criteria: string;
  medalSheet: string;
  interviews: string;
  rewards: string;
} {
  const evaluationTerms = ['è©•ä¾¡', 'ãƒ¡ãƒ€ãƒ«ã‚·ãƒ¼ãƒˆ', 'é¢è«‡', 'assessment', 'evaluation']
  const relevantLines = content.split('\n').filter(line => 
    evaluationTerms.some(term => line.toLowerCase().includes(term))
  )
  
  const medalSheetInfo = content.split('\n').filter(line => 
    line.toLowerCase().includes('ãƒ¡ãƒ€ãƒ«') || line.toLowerCase().includes('medal')
  )
  
  return {
    overview: `ã‚¹ãƒ”ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã‚¸ãƒ£ãƒ‘ãƒ³ã®è©•ä¾¡åˆ¶åº¦ã¯ã€å¾“æ¥­å“¡ã®æˆé•·ã¨æˆæœã‚’é©åˆ‡ã«è©•ä¾¡ã—ã€ã‚­ãƒ£ãƒªã‚¢ç™ºå±•ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹åŒ…æ‹¬çš„ãªã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚è³‡æ–™ã§ã¯${relevantLines.length}ä»¶ã®è©•ä¾¡é–¢é€£é …ç›®ãŒç¢ºèªã•ã‚Œã¦ã„ã¾ã™ã€‚`,
    
    criteria: relevantLines.slice(0, 8).map(line => 
      `â€¢ ${line.trim().substring(0, 150)}`
    ).join('\n') || `
â€¢ æ¥­å‹™é‚è¡Œèƒ½åŠ›ã¨æˆæœã®å®šé‡çš„è©•ä¾¡
â€¢ ãƒŸãƒƒã‚·ãƒ§ãƒ³é”æˆçŠ¶æ³ã®ç·åˆåˆ¤å®š
â€¢ ã‚¹ã‚­ãƒ«ãƒ»çŸ¥è­˜ãƒ¬ãƒ™ãƒ«ã®å®¢è¦³çš„æ¸¬å®š
â€¢ ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³èƒ½åŠ›
â€¢ ç¶™ç¶šçš„ãªå­¦ç¿’ãƒ»æ”¹å–„ã¸ã®å–ã‚Šçµ„ã¿`,
    
    medalSheet: medalSheetInfo.length > 0 
      ? medalSheetInfo.slice(0, 5).map(line => `â€¢ ${line.trim()}`).join('\n')
      : `
â€¢ ç›®æ¨™è¨­å®šã¨æŒ¯ã‚Šè¿”ã‚Šã®ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦æ´»ç”¨
â€¢ å®šæœŸçš„ãªè‡ªå·±è©•ä¾¡ã¨ä¸Šå¸è©•ä¾¡ã®å®Ÿæ–½
â€¢ æˆé•·ç›®æ¨™ã¨é”æˆçŠ¶æ³ã®å¯è¦–åŒ–
â€¢ æ¬¡æœŸç›®æ¨™è¨­å®šã®ãŸã‚ã®åŸºç¤è³‡æ–™ã¨ã—ã¦ä½¿ç”¨`,
    
    interviews: `
â€¢ å®šæœŸçš„ãªè©•ä¾¡é¢è«‡ï¼ˆå››åŠæœŸã¾ãŸã¯åŠå¹´ã”ã¨ï¼‰
â€¢ ç›®æ¨™é”æˆçŠ¶æ³ã®è©³ç´°ãƒ¬ãƒ“ãƒ¥ãƒ¼
â€¢ èª²é¡Œã¨æ”¹å–„ç‚¹ã®å…·ä½“çš„ãªè¨è­°
â€¢ æ¬¡æœŸç›®æ¨™ã¨æˆé•·è¨ˆç”»ã®ç­–å®š
â€¢ ã‚­ãƒ£ãƒªã‚¢ç›¸è«‡ã¨ã‚µãƒãƒ¼ãƒˆä½“åˆ¶ã®ç¢ºèª`,
    
    rewards: `
â€¢ è©•ä¾¡çµæœã«åŸºã¥ãçµ¦ä¸ãƒ»è³ä¸ã¸ã®åæ˜ 
â€¢ æ˜‡æ ¼ãƒ»æ˜‡é€²ã®åˆ¤å®šææ–™ã¨ã—ã¦æ´»ç”¨
â€¢ å„ªç§€è€…ã¸ã®è¡¨å½°ãƒ»ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–
â€¢ ç ”ä¿®ãƒ»ã‚¹ã‚­ãƒ«ã‚¢ãƒƒãƒ—æ©Ÿä¼šã®æä¾›å„ªå…ˆåº¦æ±ºå®š`
  }
}

/**
 * Extract mission information from document content
 */
function extractMissionInformation(content: string): {
  overview: string;
  salaryMissions: string;
  otherMissions: string;
  progress: string;
  rewards: string;
} {
  const missionTerms = ['ãƒŸãƒƒã‚·ãƒ§ãƒ³', 'mission', 'å˜ä¾¡up', 'å˜ä¾¡ã‚¢ãƒƒãƒ—', 'ã‚¿ã‚¹ã‚¯']
  const relevantLines = content.split('\n').filter(line => 
    missionTerms.some(term => line.toLowerCase().includes(term))
  )
  
  const salaryMissions = content.split('\n').filter(line => 
    line.toLowerCase().includes('å˜ä¾¡') || line.toLowerCase().includes('çµ¦ä¸') || line.toLowerCase().includes('å ±é…¬')
  )
  
  return {
    overview: `ãƒŸãƒƒã‚·ãƒ§ãƒ³åˆ¶åº¦ã¯ã€å¾“æ¥­å“¡ã®æˆé•·ã¨æˆæœã‚’ä¿ƒé€²ã™ã‚‹ãŸã‚ã®ç›®æ¨™è¨­å®šã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚è³‡æ–™ã§ã¯${relevantLines.length}ä»¶ã®ãƒŸãƒƒã‚·ãƒ§ãƒ³é–¢é€£æƒ…å ±ãŒç¢ºèªã•ã‚Œã¦ã„ã¾ã™ã€‚`,
    
    salaryMissions: salaryMissions.length > 0 
      ? salaryMissions.slice(0, 6).map(line => `â€¢ ${line.trim()}`).join('\n')
      : `
â€¢ å£²ä¸Šç›®æ¨™é”æˆã«ã‚ˆã‚‹å˜ä¾¡ã‚¢ãƒƒãƒ—
â€¢ æ–°è¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç²å¾—ãƒŸãƒƒã‚·ãƒ§ãƒ³
â€¢ å“è³ªå‘ä¸Šãƒ»åŠ¹ç‡åŒ–é”æˆã«ã‚ˆã‚‹å ±é…¬å¢—
â€¢ è³‡æ ¼å–å¾—ã«ã‚ˆã‚‹æ‰‹å½“åŠ ç®—ãƒŸãƒƒã‚·ãƒ§ãƒ³`,
    
    otherMissions: `
â€¢ ãƒãƒ¼ãƒ ãƒ“ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ãƒ»ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ãƒŸãƒƒã‚·ãƒ§ãƒ³
â€¢ æ–°äººæ•™è‚²ãƒ»ãƒ¡ãƒ³ã‚¿ãƒ¼æ´»å‹•
â€¢ æ¥­å‹™æ”¹å–„ææ¡ˆã¨å®Ÿè£…
â€¢ é¡§å®¢æº€è¶³åº¦å‘ä¸Šæ–½ç­–ã®å®Ÿè¡Œ
â€¢ ç¤¾å†…ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®è²¢çŒ®`,
    
    progress: `
â€¢ æœˆæ¬¡ãƒ»å››åŠæœŸã§ã®é€²æ—ç¢ºèª
â€¢ ãƒ¡ãƒ€ãƒ«ã‚·ãƒ¼ãƒˆã«ã‚ˆã‚‹çŠ¶æ³è¨˜éŒ²
â€¢ ä¸Šå¸ã¨ã®å®šæœŸçš„ãªé€²æ—é¢è«‡
â€¢ é”æˆåº¦ã«å¿œã˜ãŸä¸­é–“è©•ä¾¡ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯`,
    
    rewards: `
â€¢ é”æˆåº¦ã«å¿œã˜ãŸçµ¦ä¸ãƒ»è³ä¸ã¸ã®åæ˜ 
â€¢ å„ªç§€é”æˆè€…ã¸ã®ç‰¹åˆ¥ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–
â€¢ æ˜‡æ ¼ãƒ»æ˜‡é€²ã®å„ªå…ˆè©•ä¾¡å¯¾è±¡
â€¢ è¡¨å½°åˆ¶åº¦ã«ã‚ˆã‚‹ç¤¾å†…èªçŸ¥å‘ä¸Š`
  }
}

/**
 * Extract promotion information from document content
 */
function extractPromotionInformation(content: string): {
  overview: string;
  requirements: string;
  process: string;
  benefits: string;
} {
  const promotionTerms = ['æ˜‡æ ¼', 'æ˜‡é€²', 'promotion', 'æ˜‡ç´š', 'æ˜‡ä»»']
  const relevantLines = content.split('\n').filter(line => 
    promotionTerms.some(term => line.toLowerCase().includes(term))
  )
  
  return {
    overview: `æ˜‡æ ¼ãƒ»æ˜‡é€²åˆ¶åº¦ã¯ã€å¾“æ¥­å“¡ã®èƒ½åŠ›ã¨æˆæœã«åŸºã¥ã„ãŸå…¬æ­£ãªã‚­ãƒ£ãƒªã‚¢ç™ºå±•æ©Ÿä¼šã‚’æä¾›ã—ã¾ã™ã€‚è³‡æ–™ã§ã¯${relevantLines.length}ä»¶ã®æ˜‡é€²é–¢é€£æƒ…å ±ãŒç¢ºèªã•ã‚Œã¦ã„ã¾ã™ã€‚`,
    
    requirements: relevantLines.slice(0, 8).map(line => 
      `â€¢ ${line.trim().substring(0, 150)}`
    ).join('\n') || `
â€¢ ç¾åœ¨ã®ã‚°ãƒ¬ãƒ¼ãƒ‰ã§ã®å¿…è¦æœŸé–“ã®æº€äº†
â€¢ æŒ‡å®šã•ã‚ŒãŸãƒŸãƒƒã‚·ãƒ§ãƒ³ãƒ»ç›®æ¨™ã®é”æˆ
â€¢ å¿…è¦ãªã‚¹ã‚­ãƒ«ãƒ»è³‡æ ¼ã®å–å¾—
â€¢ è©•ä¾¡é¢è«‡ã§ã®ç·åˆè©•ä¾¡åŸºæº–ã‚¯ãƒªã‚¢
â€¢ ä¸Šä½è·ã¸ã®é©æ€§ã¨æ„æ¬²ã®ç¢ºèª`,
    
    process: `
â€¢ æ˜‡æ ¼ç”³è«‹ã¾ãŸã¯æ¨è–¦ã®æå‡º
â€¢ å¿…è¦æ›¸é¡ï¼ˆå®Ÿç¸¾ãƒ»è³‡æ ¼è¨¼æ˜ç­‰ï¼‰ã®æº–å‚™
â€¢ è©•ä¾¡å§”å“¡ä¼šã«ã‚ˆã‚‹ç·åˆå¯©æŸ»
â€¢ é¢æ¥ãƒ»ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
â€¢ æœ€çµ‚æ‰¿èªã¨ç™ºä»¤æ‰‹ç¶šã`,
    
    benefits: `
â€¢ åŸºæœ¬çµ¦ä¸ã®å¢—é¡ï¼ˆã‚°ãƒ¬ãƒ¼ãƒ‰ã«å¿œã˜ã¦ï¼‰
â€¢ å½¹è·æ‰‹å½“ãƒ»è²¬ä»»æ‰‹å½“ã®ä»˜ä¸
â€¢ ã‚ˆã‚Šå¤§ããªè£é‡æ¨©ã¨æ„æ€æ±ºå®šæ¨©
â€¢ éƒ¨ä¸‹ãƒ»ãƒãƒ¼ãƒ ã®ç®¡ç†è²¬ä»»
â€¢ ã•ã‚‰ãªã‚‹ã‚­ãƒ£ãƒªã‚¢ç™ºå±•æ©Ÿä¼šã®æ‹¡å¤§`
  }
}

/**
 * Extract skill and qualification information from document content
 */
function extractSkillInformation(content: string): {
  overview: string;
  requirements: string;
  allowances: string;
  evaluation: string;
} {
  const skillTerms = ['è³‡æ ¼', 'ã‚¹ã‚­ãƒ«', 'skill', 'èƒ½åŠ›', 'æŠ€è¡“', 'çŸ¥è­˜']
  const relevantLines = content.split('\n').filter(line => 
    skillTerms.some(term => line.toLowerCase().includes(term))
  )
  
  return {
    overview: `ã‚¹ã‚­ãƒ«ãƒ»è³‡æ ¼åˆ¶åº¦ã¯ã€å¾“æ¥­å“¡ã®å°‚é–€èƒ½åŠ›å‘ä¸Šã¨æ¥­å‹™å“è³ªã®å‘ä¸Šã‚’ç›®çš„ã¨ã—ãŸåŒ…æ‹¬çš„ãªèƒ½åŠ›é–‹ç™ºã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚è³‡æ–™ã§ã¯${relevantLines.length}ä»¶ã®ã‚¹ã‚­ãƒ«é–¢é€£æƒ…å ±ãŒç¢ºèªã•ã‚Œã¦ã„ã¾ã™ã€‚`,
    
    requirements: relevantLines.slice(0, 8).map(line => 
      `â€¢ ${line.trim().substring(0, 150)}`
    ).join('\n') || `
â€¢ æ¥­å‹™ã«ç›´æ¥é–¢é€£ã™ã‚‹å°‚é–€è³‡æ ¼
â€¢ ITã‚¹ã‚­ãƒ«ãƒ»ãƒ‡ã‚¸ã‚¿ãƒ«ãƒªãƒ†ãƒ©ã‚·ãƒ¼
â€¢ ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³èƒ½åŠ›
â€¢ ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆãƒ»ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã‚¹ã‚­ãƒ«
â€¢ ç¶™ç¶šçš„ãªå­¦ç¿’ãƒ»è‡ªå·±å•“ç™ºã¸ã®å–ã‚Šçµ„ã¿`,
    
    allowances: `
â€¢ è³‡æ ¼å–å¾—ã«ã‚ˆã‚‹æ‰‹å½“æ”¯çµ¦
â€¢ è³‡æ ¼ç¶­æŒãƒ»æ›´æ–°è²»ç”¨ã®ä¼šç¤¾è² æ‹…
â€¢ å¤–éƒ¨ç ”ä¿®ãƒ»ã‚»ãƒŸãƒŠãƒ¼å‚åŠ è²»ç”¨è£œåŠ©
â€¢ æ˜‡æ ¼ãƒ»æ˜‡é€²ã®å„ªé‡è©•ä¾¡`,
    
    evaluation: `
â€¢ å®šæœŸçš„ãªã‚¹ã‚­ãƒ«ã‚¢ã‚»ã‚¹ãƒ¡ãƒ³ãƒˆã®å®Ÿæ–½
â€¢ æ¥­å‹™é‚è¡Œã«ãŠã‘ã‚‹å®Ÿè·µçš„èƒ½åŠ›ã®è©•ä¾¡
â€¢ åŒåƒšãƒ»éƒ¨ä¸‹ã‹ã‚‰ã®360åº¦è©•ä¾¡
â€¢ ç¶™ç¶šçš„ãªå­¦ç¿’å§¿å‹¢ã¨æˆæœã®ç¢ºèª`
  }
}

/**
 * Generate comprehensive analysis when no specific topic is matched
 */
function generateComprehensiveAnalysis(content: string, question: string): string {
  const lines = content.split('\n').filter(line => line.trim().length > 0)
  const totalChars = content.length
  
  // Extract key sections and important information
  const keyLines = lines.filter(line => 
    line.toLowerCase().includes('é‡è¦') ||
    line.toLowerCase().includes('å¿…é ˆ') ||
    line.toLowerCase().includes('æ³¨æ„') ||
    line.match(/^[â– â–¡â—â—‹â–ªâ–«]\s/) ||
    line.match(/^\d+[.)]\s/) ||
    line.includes('ï¼š') || line.includes(':')
  ).slice(0, 15)
  
  const analysis = `
**ğŸ“‹ ç·åˆåˆ†æ**

**ğŸ“Š æ–‡æ›¸æ¦‚è¦**:
â€¢ æ–‡æ›¸è¦æ¨¡: ${lines.length}è¡Œï¼ˆ${totalChars}æ–‡å­—ï¼‰
â€¢ æ§‹é€ åŒ–æƒ…å ±: ${keyLines.length}ä»¶ã®é‡è¦é …ç›®ã‚’ç¢ºèª

**ğŸ” ä¸»è¦å†…å®¹**:
${keyLines.map(line => `â€¢ ${line.trim().substring(0, 200)}`).join('\n')}

**ğŸ’¡ æ´»ç”¨æ–¹æ³•**:
â€¢ åˆ¶åº¦ã®è©³ç´°ç¢ºèª: å…·ä½“çš„ãªé …ç›®åã§è³ªå•
â€¢ æ•°å€¤ãƒ»æ¡ä»¶ã®æŠ½å‡º: ã€Œæ‰‹å½“ã€ã€Œé‡‘é¡ã€ã€ŒæœŸé–“ã€ç­‰ã§è³ªå•
â€¢ ãƒ—ãƒ­ã‚»ã‚¹ã®ç†è§£: ã€Œæ‰‹é †ã€ã€Œæ–¹æ³•ã€ã€Œæµã‚Œã€ç­‰ã§è³ªå•
â€¢ æ¯”è¼ƒåˆ†æ: è¤‡æ•°ã®åˆ¶åº¦ã‚„æ¡ä»¶ã®æ¯”è¼ƒ`

  return analysis
}

/**
 * Enhanced conversation management with streaming support
 */
export class EnhancedConversationMemory extends ConversationMemory {
  private contextSummary: string = ''
  
  constructor(maxTurns: number = 15, maxTokens: number = 3000) {
    super(maxTurns, maxTokens)
  }

  /**
   * Get optimized context for ChatGPT-like responses
   */
  getChatGPTContext(): string {
    const recentContext = this.getContext()
    const summary = this.getSummary()
    
    if (this.contextSummary && recentContext.length < 500) {
      return `éå»ã®ä¼šè©±æ¦‚è¦: ${this.contextSummary}\n\næœ€è¿‘ã®ä¼šè©±:\n${recentContext}`
    }
    
    return recentContext
  }

  /**
   * Update context summary for long conversations
   */
  updateContextSummary(summary: string) {
    this.contextSummary = summary
  }

  /**
   * Analyze conversation patterns for better responses
   */
  getConversationPatterns(): { 
    frequentTopics: string[], 
    userPreferences: string[], 
    conversationStyle: string 
  } {
    const allUserMessages = this.history
      .filter(turn => turn.role === 'user')
      .map(turn => turn.content.toLowerCase())
    
    // Simple keyword extraction for topics
    const words = allUserMessages.join(' ').split(/\s+/)
    const wordCounts = words.reduce((acc, word) => {
      if (word.length > 3) {
        acc[word] = (acc[word] || 0) + 1
      }
      return acc
    }, {} as Record<string, number>)
    
    const frequentTopics = Object.entries(wordCounts)
      .sort(([,a], [,b]) => b - a)
      .slice(0, 5)
      .map(([word]) => word)
    
    // Determine conversation style
    const hasPoliteLanguage = allUserMessages.some(msg => 
      msg.includes('ãŠé¡˜ã„') || msg.includes('ãã ã•ã„') || msg.includes('ã‚ã‚ŠãŒã¨ã†')
    )
    
    const hasInformalLanguage = allUserMessages.some(msg =>
      msg.includes('ã ã‚ˆã­') || msg.includes('ã£ã¦ã„ã†') || msg.includes('ã‚„ã°ã„')
    )
    
    let conversationStyle = 'neutral'
    if (hasPoliteLanguage && !hasInformalLanguage) conversationStyle = 'formal'
    if (hasInformalLanguage && !hasPoliteLanguage) conversationStyle = 'casual'
    
    return {
      frequentTopics,
      userPreferences: [], // Could be enhanced based on user feedback
      conversationStyle
    }
  }
}

/**
 * ChatGPT-like streaming response generation
 */
export async function* generateStreamingResponse(
  message: string,
  conversationMemory: EnhancedConversationMemory,
  documentContext?: string
): AsyncGenerator<string, void, unknown> {
  try {
    // Get conversation patterns for personalized responses
    const patterns = conversationMemory.getConversationPatterns()
    const chatContext = conversationMemory.getChatGPTContext()
    
    // Create enhanced prompt
    let systemPrompt = `ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚`
    
    // Adapt response style based on conversation patterns
    switch (patterns.conversationStyle) {
      case 'formal':
        systemPrompt += `ä¸å¯§ã§æ ¼å¼ã‚ã‚‹æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`
        break
      case 'casual':
        systemPrompt += `ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§è¦ªã—ã¿ã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`
        break
      default:
        systemPrompt += `è‡ªç„¶ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`
    }
    
    if (documentContext) {
      systemPrompt += `\n\nå‚è€ƒè³‡æ–™:\n${documentContext.slice(0, 2000)}`
    }
    
    if (chatContext) {
      systemPrompt += `\n\nä¼šè©±å±¥æ­´:\n${chatContext}`
    }
    
    const fullPrompt = `${systemPrompt}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼: ${message}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:`
    
    // Use Hugging Face streaming if available
    const response = await hf.textGeneration({
      model: MODELS.CHAT,
      inputs: fullPrompt,
      parameters: {
        max_new_tokens: 500,
        temperature: 0.7,
        top_p: 0.9,
        do_sample: true,
        repetition_penalty: 1.1,
        return_full_text: false,
        stop: ['ãƒ¦ãƒ¼ã‚¶ãƒ¼:', 'User:', 'Human:']
      }
    })
    
    // Simulate streaming by yielding chunks of the response
    const fullResponse = response.generated_text.trim()
    const chunks = fullResponse.split(/([ã€‚ï¼ï¼Ÿ])/g)
    
    let accumulatedResponse = ''
    for (const chunk of chunks) {
      if (chunk.trim()) {
        accumulatedResponse += chunk
        yield accumulatedResponse
        
        // Add small delay to simulate real streaming
        await new Promise(resolve => setTimeout(resolve, 50))
      }
    }
    
    // Add to conversation memory
    conversationMemory.addTurn('user', message, documentContext)
    conversationMemory.addTurn('assistant', fullResponse, documentContext)
    
  } catch (error) {
    console.error('Streaming response error:', error)
    
    // Fallback to non-streaming intelligent response
    const fallbackResponse = await generateIntelligentFallback(message, documentContext || '', conversationMemory)
    yield fallbackResponse
  }
}

/**
 * Generate response using gpt-oss models with reasoning levels
 */
export async function generateGptOssResponse(
  prompt: string,
  context?: string,
  reasoningLevel: 'low' | 'medium' | 'high' = 'medium',
  model: string = MODELS.CHAT
): Promise<string> {
  try {
    // Prepare system prompt with reasoning level
    const systemPrompt = `Reasoning: ${reasoningLevel}\n\nYou are a helpful AI assistant. Please provide accurate and helpful responses in Japanese when appropriate.`
    
    // Prepare the input using harmony format for gpt-oss
    const messages = [
      { role: 'system', content: systemPrompt }
    ]
    
    if (context) {
      messages.push({ 
        role: 'user', 
        content: `Context: ${context}\n\nQuestion: ${prompt}` 
      })
    } else {
      messages.push({ role: 'user', content: prompt })
    }

    // For gpt-oss models, use the text generation with proper formatting
    const formattedInput = messages.map(msg => 
      `${msg.role === 'system' ? 'System' : msg.role === 'user' ? 'Human' : 'Assistant'}: ${msg.content}`
    ).join('\n') + '\nAssistant:'

    const response = await hf.textGeneration({
      model,
      inputs: formattedInput,
      parameters: {
        max_new_tokens: 800,
        temperature: 0.7,
        do_sample: true,
        repetition_penalty: 1.1,
        return_full_text: false,
        stop: ['Human:', 'System:']
      }
    })

    return response.generated_text.trim()
  } catch (error) {
    console.error('gpt-oss API error:', error)
    
    // Fallback to standard generation
    return generateResponse(prompt, context, MODELS.SIMPLE_CHAT)
  }
}

/**
 * Generate natural, conversational responses like ChatGPT
 */
export async function generateNaturalChatResponse(
  message: string,
  documentContext?: string,
  conversationHistory?: string[]
): Promise<string> {
  try {
    // First try Hugging Face API with proper error handling
    const response = await tryHuggingFaceGeneration(message, documentContext, conversationHistory)
    return makeResponseNatural(response, message, documentContext)
  } catch (error) {
    console.error('Hugging Face API failed:', error)
    
    // Use intelligent fallback without API
    if (documentContext) {
      return generateSimpleDocumentResponse(message, documentContext)
    } else {
      return generateSimpleResponse(message)
    }
  }
}

/**
 * Try Hugging Face generation with fallback handling
 */
async function tryHuggingFaceGeneration(
  message: string,
  documentContext?: string,
  conversationHistory?: string[]
): Promise<string> {
  // Create a natural conversation prompt
  let systemPrompt = `ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è‡ªç„¶ã§äººé–“ã‚‰ã—ã„ä¼šè©±ã‚’å¿ƒãŒã‘ã€ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚`
  
  // Build conversation context
  let conversationContext = ''
  if (conversationHistory && conversationHistory.length > 0) {
    conversationContext = conversationHistory.slice(-4).join('\n') // Last 4 exchanges
  }
  
  // Create the input prompt
  let inputPrompt = systemPrompt
  
  if (documentContext) {
    inputPrompt += `\n\nå‚è€ƒè³‡æ–™:\n${documentContext.slice(0, 1000)}`
  }
  
  if (conversationContext) {
    inputPrompt += `\n\næœ€è¿‘ã®ä¼šè©±:\n${conversationContext}`
  }
  
  inputPrompt += `\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼: ${message}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:`

  // Try multiple models as fallback
  const modelsToTry = [
    MODELS.SIMPLE_CHAT, // DialoGPT-medium
    'gpt2', // Basic GPT-2
    'microsoft/DialoGPT-small' // Smaller model
  ]

  for (const model of modelsToTry) {
    try {
      const response = await hf.textGeneration({
        model,
        inputs: inputPrompt,
        parameters: {
          max_new_tokens: 200,
          temperature: 0.7,
          do_sample: true,
          repetition_penalty: 1.1,
          return_full_text: false,
          stop: ['ãƒ¦ãƒ¼ã‚¶ãƒ¼:', 'ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:', '\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼', '\n\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ']
        }
      })

      if (response.generated_text && response.generated_text.trim().length > 0) {
        return response.generated_text.trim()
      }
    } catch (modelError) {
      console.warn(`Model ${model} failed:`, modelError)
      continue
    }
  }

  // If all models fail, throw error to trigger fallback
  throw new Error('All Hugging Face models failed')
}

/**
 * Make response more natural and conversational
 */
function makeResponseNatural(response: string, userMessage: string, documentContext?: string): string {
  // Remove excessive formatting and make more conversational
  let natural = response
    .replace(/^ã€.*?ã€‘\s*/gm, '') // Remove formatted headers
    .replace(/^\*\*.*?\*\*\s*/gm, '') // Remove bold headers
    .replace(/^#+\s*/gm, '') // Remove markdown headers
    .replace(/ğŸ“„|ğŸ“‚|ğŸ“‹|ğŸ¯|ğŸ“Š|ğŸ’¬|ğŸ“ˆ|ğŸ”|ğŸ’¡|ğŸ¤|âœ…|âŒ|ğŸ‰|ğŸ”§|ğŸ“±|ğŸ¢|ğŸ’°|ğŸ|ğŸ“|ğŸ”’|ğŸš€|ğŸ¨|ğŸ“‹|ğŸ“ˆ|ğŸ“Š/g, '') // Remove all emojis
    .replace(/^\s*[â€¢\-\*]\s*/gm, '') // Remove bullet points
    .replace(/^(:\s*|ï¼š\s*)/gm, '') // Remove colons at start of lines
    .replace(/\*\*(.*?)\*\*/g, '$1') // Remove bold formatting
    .replace(/\n{3,}/g, '\n\n') // Reduce excessive line breaks
    .replace(/^(è©³ç´°åˆ†æçµæœ|å‚ç…§è³‡æ–™|æ§‹é€ åŒ–åˆ†æ|æ–‡æ›¸çµ±è¨ˆ|åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½|ã•ã‚‰ãªã‚‹ã‚µãƒãƒ¼ãƒˆ).*$/gm, '') // Remove structured sections
    .replace(/^(ã”è³ªå•|ğŸ“‚|ğŸ“‹|ğŸ¯|ğŸ“Š|ğŸ’¬|ğŸ“ˆ|ğŸ”|ğŸ’¡|ğŸ¤|ğŸ“±|ğŸ¢|ğŸ’°|ğŸ|ğŸ“|ğŸ”’|ğŸš€|ğŸ¨).*$/gm, '') // Remove structured headers
    .replace(/\n\s*\n\s*\n/g, '\n\n') // Clean up multiple line breaks
    .trim()

  // Remove structured analysis sections completely
  const sectionsToRemove = [
    /^.*?è©³ç´°åˆ†æçµæœ.*$/gm,
    /^.*?å‚ç…§è³‡æ–™.*$/gm,
    /^.*?æ§‹é€ åŒ–åˆ†æ.*$/gm,
    /^.*?æ–‡æ›¸çµ±è¨ˆ.*$/gm,
    /^.*?åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½.*$/gm,
    /^.*?ã•ã‚‰ãªã‚‹ã‚µãƒãƒ¼ãƒˆ.*$/gm,
    /^.*?ç·è¡Œæ•°.*$/gm,
    /^.*?ç®‡æ¡æ›¸ãé …ç›®.*$/gm,
    /^.*?ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°.*$/gm
  ]

  sectionsToRemove.forEach(pattern => {
    natural = natural.replace(pattern, '')
  })

  // Clean up the content to be more conversational
  natural = natural
    .replace(/ã«ã¤ã„ã¦èª¬æ˜.*?ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚/g, 'ã«ã¤ã„ã¦ã§ã™ã­ã€‚')
    .replace(/è³‡æ–™ã§ã¯.*?ç¢ºèªã•ã‚Œã¦ã„ã¾ã™ã€‚/g, '')
    .replace(/ã“ã®å›ç­”ã¯.*?ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ã€‚/g, '')
    .replace(/å…·ä½“çš„ãªæ•°å€¤.*?æä¾›ã—ã¦ã„ã¾ã™ã€‚/g, '')

  // Ensure reasonable length (not too verbose)
  if (natural.length > 400) {
    const sentences = natural.split(/[ã€‚ï¼ï¼Ÿ]/)
    natural = sentences.slice(0, 3).join('ã€‚') + 'ã€‚'
  }

  // Ensure the response starts naturally and is concise
  if (natural.length < 20 || !natural.trim()) {
    if (documentContext && userMessage.includes('è©•ä¾¡')) {
      return 'è©•ä¾¡åˆ¶åº¦ã«ã¤ã„ã¦ã§ã™ã­ã€‚è³‡æ–™ã‚’ç¢ºèªã—ãŸã¨ã“ã‚ã€åŒ…æ‹¬çš„ãªåˆ¶åº¦ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚ã©ã®éƒ¨åˆ†ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ'
    } else if (documentContext) {
      return `ã€Œ${userMessage}ã€ã«ã¤ã„ã¦è³‡æ–™ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚å…·ä½“çš„ã«ã©ã®éƒ¨åˆ†ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ`
    }
  }

  return natural
}

/**
 * Generate simple document-based response
 */
function generateSimpleDocumentResponse(message: string, documentContext: string): string {
  const context = documentContext.slice(0, 1200)
  const messageLC = message.toLowerCase()
  
  // Create more natural, conversational responses
  if (messageLC.includes('è©•ä¾¡') || messageLC.includes('äººäº‹')) {
    // Extract key information naturally from the context
    const hasGrades = context.includes('ã‚°ãƒ¬ãƒ¼ãƒ‰') || context.includes('STEP')
    const hasMedalSheet = context.includes('ãƒ¡ãƒ€ãƒ«ã‚·ãƒ¼ãƒˆ') || context.includes('medal')
    const hasReview = context.includes('è©•ä¾¡é¢è«‡') || context.includes('é¢è«‡')
    
    let response = 'è©•ä¾¡åˆ¶åº¦ã«ã¤ã„ã¦ãŠç­”ãˆã—ã¾ã™ã­ã€‚\n\n'
    
    if (hasGrades) {
      response += 'ä¼šç¤¾ã§ã¯ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¶åº¦ã‚’æ¡ç”¨ã—ã¦ã„ã¦ã€'
    }
    if (hasMedalSheet) {
      response += 'ãƒ¡ãƒ€ãƒ«ã‚·ãƒ¼ãƒˆã‚’ä½¿ã£ãŸç›®æ¨™ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãŒã‚ã‚Šã¾ã™ã€‚'
    }
    if (hasReview) {
      response += 'å®šæœŸçš„ãªè©•ä¾¡é¢è«‡ã§é€²æ—ã‚’ç¢ºèªã—ã€'
    }
    
    response += '\n\nå…·ä½“çš„ã«ã©ã®éƒ¨åˆ†ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ'
    return response
  }
  
  if (messageLC.includes('ãƒ¡ãƒ€ãƒ«ã‚·ãƒ¼ãƒˆ')) {
    return 'ãƒ¡ãƒ€ãƒ«ã‚·ãƒ¼ãƒˆã«ã¤ã„ã¦ã§ã™ã­ã€‚\n\nè³‡æ–™ã«ã‚ˆã‚‹ã¨ã€ãƒ¡ãƒ€ãƒ«ã‚·ãƒ¼ãƒˆã¯ç›®æ¨™è¨­å®šã¨é”æˆåº¦è©•ä¾¡ã®ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚å˜ä¾¡ç›®æ¨™ã‚„ãƒ¡ãƒ€ãƒ«å–å¾—æ•°ã€è³‡æ ¼å–å¾—ãªã©ã‚’ç®¡ç†ã—ã¦ã€å®šæœŸçš„ã«æŒ¯ã‚Šè¿”ã‚Šã‚’è¡Œã„ã¾ã™ã€‚\n\nä½¿ã„æ–¹ã‚„å…·ä½“çš„ãªé …ç›®ã«ã¤ã„ã¦ã€ã‚‚ã†å°‘ã—è©³ã—ããŠèãã—ãŸã„ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ'
  }
  
  if (messageLC.includes('ã‚°ãƒ¬ãƒ¼ãƒ‰') || messageLC.includes('æ˜‡æ ¼')) {
    return 'ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¶åº¦ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã­ã€‚\n\nè³‡æ–™ã‚’è¦‹ã‚‹ã¨ã€æ®µéšçš„ãªã‚°ãƒ¬ãƒ¼ãƒ‰æ§‹æˆã«ãªã£ã¦ã„ã¦ã€ãã‚Œãã‚Œã«æ˜‡æ ¼æ¡ä»¶ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚°ãƒ¬ãƒ¼ãƒ‰ã«å¿œã˜ãŸæ‰‹å½“ã‚„è©•ä¾¡åŸºæº–ã‚‚æ±ºã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚\n\nç‰¹å®šã®ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚„æ˜‡æ ¼æ¡ä»¶ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ'
  }
  
  if (messageLC.includes('ç¦åˆ©åšç”Ÿ') || messageLC.includes('åˆ¶åº¦')) {
    return 'ç¦åˆ©åšç”Ÿåˆ¶åº¦ã«ã¤ã„ã¦ãŠç­”ãˆã—ã¾ã™ã€‚\n\nè³‡æ–™ã«ã¯æ§˜ã€…ãªåˆ¶åº¦ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚ã©ã®ã‚ˆã†ãªåˆ¶åº¦ã«ã¤ã„ã¦å…·ä½“çš„ã«çŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ\n\nä¾‹ãˆã°ã€å¥åº·ä¿é™ºã€é€€è·é‡‘ã€ç ”ä¿®åˆ¶åº¦ã€ä¼‘æš‡åˆ¶åº¦ãªã©ãŒã‚ã‚Šã¾ã™ã€‚'
  }
  
  // More natural generic response
  return `ã€Œ${message}ã€ã«ã¤ã„ã¦ã§ã™ã­ã€‚\n\nè³‡æ–™ã‚’ç¢ºèªã—ãŸã¨ã“ã‚ã€é–¢é€£ã™ã‚‹æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã‚‚ã†å°‘ã—å…·ä½“çš„ã«ã©ã®éƒ¨åˆ†ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„ã‹æ•™ãˆã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ\n\nãã†ã™ã‚Œã°ã€ã‚ˆã‚Šè©³ã—ã„æƒ…å ±ã‚’ãŠä¼ãˆã§ãã¾ã™ã€‚`
}

/**
 * Extract keywords from document context
 */
function extractKeywords(text: string): string[] {
  const commonWords = ['ã«ã¤ã„ã¦', 'ã§ã™', 'ã¾ã™', 'ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ãªã©', 'ã¾ãŸ', 'ã§ã¯', 'ã‹ã‚‰', 'ã¾ã§', 'ã¨ã—ã¦', 'ã«ã‚ˆã‚‹', 'ã«ãŠã„ã¦', 'ã«é–¢ã—ã¦']
  
  const words = text
    .replace(/[^\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\u3400-\u4DBF\w\s]/g, ' ')
    .split(/\s+/)
    .filter(word => word.length > 1 && !commonWords.includes(word))
    .slice(0, 10)
  
  return [...new Set(words)]
}

/**
 * Generate simple response without document context
 */
function generateSimpleResponse(message: string): string {
  const messageLC = message.toLowerCase()
  
  if (messageLC.includes('ã“ã‚“ã«ã¡ã¯') || messageLC.includes('ã¯ã˜ã‚ã¾ã—ã¦') || messageLC.includes('hello')) {
    return 'ã“ã‚“ã«ã¡ã¯ï¼SLJ Chatbotã§ã™ã€‚ä¼šç¤¾ã®è³‡æ–™ã«ã¤ã„ã¦ä½•ã‹ã”è³ªå•ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ãŠèã‹ã›ãã ã•ã„ã€‚'
  }
  
  if (messageLC.includes('ã‚ã‚ŠãŒã¨ã†') || messageLC.includes('thank')) {
    return 'ã©ã†ã„ãŸã—ã¾ã—ã¦ã€‚ä»–ã«ã‚‚ã”è³ªå•ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ã„ã¤ã§ã‚‚ãŠèã‹ã›ãã ã•ã„ã€‚'
  }
  
  if (messageLC.includes('åŠ©ã‘ã¦') || messageLC.includes('help') || messageLC.includes('ã‚µãƒãƒ¼ãƒˆ')) {
    return 'ãŠæ‰‹ä¼ã„ã„ãŸã—ã¾ã™ï¼ã©ã®ã‚ˆã†ãªã“ã¨ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ\n\nä¾‹ï¼šè©•ä¾¡åˆ¶åº¦ã€ãƒ¡ãƒ€ãƒ«ã‚·ãƒ¼ãƒˆã€ç¦åˆ©åšç”Ÿã€ä¼šç¤¾ã®ãƒ«ãƒ¼ãƒ«ãªã©'
  }
  
  if (messageLC.includes('ãƒ¡ãƒ€ãƒ«ã‚·ãƒ¼ãƒˆ')) {
    return 'ãƒ¡ãƒ€ãƒ«ã‚·ãƒ¼ãƒˆã«ã¤ã„ã¦ãŠç­”ãˆã™ã‚‹ãŸã‚ã«ã€é–¢é€£ã™ã‚‹è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ï¼Ÿã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã€è©³ç´°ãªèª¬æ˜ã‚’ã„ãŸã—ã¾ã™ã€‚'
  }
  
  if (messageLC.includes('è©•ä¾¡') || messageLC.includes('äººäº‹') || messageLC.includes('åˆ¶åº¦')) {
    return 'è©•ä¾¡åˆ¶åº¦ã«ã¤ã„ã¦æ‰¿ã‚Šã¾ã—ãŸã€‚ã‚ˆã‚Šè©³ç´°ãªå›ç­”ã®ãŸã‚ã«ã€äººäº‹é–¢é€£ã®è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ãŸã ãã¨ã€å…·ä½“çš„ãªæƒ…å ±ã‚’ãŠä¼ãˆã§ãã¾ã™ã€‚'
  }
  
  // Generic helpful response
  return `ã€Œ${message}ã€ã«ã¤ã„ã¦æ‰¿ã‚Šã¾ã—ãŸã€‚\n\nã‚ˆã‚Šè©³ç´°ã§ãŠå½¹ã«ç«‹ã¤å›ç­”ã‚’ã™ã‚‹ãŸã‚ã«ã€é–¢é€£ã™ã‚‹è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ãŸã ãã‹ã€å…·ä½“çš„ãªè³ªå•ã‚’ãŠèã‹ã›ãã ã•ã„ã€‚\n\nä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ãŠèã‹ã›ãã ã•ã„ã€‚`
}
