import { HfInference } from '@huggingface/inference'

// Hugging Face API client initialization
export const hf = new HfInference(process.env.HUGGINGFACE_API_KEY)

// Available models for different tasks - updated for better Japanese support
export const MODELS = {
  // Primary conversation model - better for Japanese
  CHAT: 'microsoft/DialoGPT-medium',
  // Backup lightweight model
  SIMPLE_CHAT: 'microsoft/DialoGPT-small', 
  // Question answering model
  QA: 'deepset/roberta-base-squad2',
  // Text generation
  TEXT_GENERATION: 'gpt2',
  // Summarization
  SUMMARIZATION: 'facebook/bart-large-cnn',
  // Fast QA for quick responses
  FAST_QA: 'distilbert-base-cased-distilled-squad'
}

// API configuration
const API_CONFIG = {
  baseURL: process.env.HUGGINGFACE_API_URL || 'https://api-inference.huggingface.co/models',
  headers: {
    'Authorization': `Bearer ${process.env.HUGGINGFACE_API_KEY}`,
    'Content-Type': 'application/json',
  },
  timeout: 30000,
  retries: 3
}

/**
 * Conversation Turn interface
 */
interface ConversationTurn {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
  documentContext?: string
}

/**
 * Advanced conversation memory management
 */
export class ConversationMemory {
  protected history: ConversationTurn[] = []
  private maxTurns: number = 10
  private maxTokens: number = 2000

  constructor(maxTurns: number = 10, maxTokens: number = 2000) {
    this.maxTurns = maxTurns
    this.maxTokens = maxTokens
  }

  /**
   * Add a conversation turn
   */
  addTurn(role: 'user' | 'assistant', content: string, documentContext?: string) {
    this.history.push({
      role,
      content,
      timestamp: new Date(),
      documentContext
    })

    // Keep only recent turns
    if (this.history.length > this.maxTurns) {
      this.history = this.history.slice(-this.maxTurns)
    }
  }

  /**
   * Get conversation context for model input
   */
  getContext(): string {
    let context = ''
    let tokenCount = 0

    // Build context from most recent conversations
    for (let i = this.history.length - 1; i >= 0; i--) {
      const turn = this.history[i]
      const turnText = `${turn.role === 'user' ? 'Human' : 'Assistant'}: ${turn.content}\n`
      
      // Rough token estimation (1 token â‰ˆ 4 characters for English/Japanese)
      const turnTokens = turnText.length / 4
      
      if (tokenCount + turnTokens > this.maxTokens) {
        break
      }
      
      context = turnText + context
      tokenCount += turnTokens
    }

    return context
  }

  /**
   * Get recent document context
   */
  getRecentDocumentContext(): string {
    const recentTurns = this.history.slice(-3) // Last 3 turns
    const documentContexts = recentTurns
      .filter(turn => turn.documentContext)
      .map(turn => turn.documentContext)
    
    return documentContexts.length > 0 ? documentContexts[documentContexts.length - 1]! : ''
  }

  /**
   * Clear conversation history
   */
  clear() {
    this.history = []
  }

  /**
   * Get conversation summary for long-term memory
   */
  getSummary(): string {
    if (this.history.length === 0) return ''
    
    const topics = new Set<string>()
    this.history.forEach(turn => {
      if (turn.role === 'user') {
        // Extract potential topics from user messages
        const words = turn.content.split(/\s+/).filter(w => w.length > 2)
        words.slice(0, 3).forEach(word => topics.add(word))
      }
    })

    return `ä¼šè©±ã®ãƒˆãƒ”ãƒƒã‚¯: ${Array.from(topics).join(', ')}`
  }
}

/**
 * Generate text response using Hugging Face model
 */
export async function generateResponse(
  prompt: string,
  context?: string,
  model: string = MODELS.CHAT
): Promise<string> {
  try {
    // Prepare the input with context if provided
    const input = context 
      ? `Context: ${context}\n\nQuestion: ${prompt}\n\nAnswer:`
      : prompt

    const response = await hf.textGeneration({
      model,
      inputs: input,
      parameters: {
        max_new_tokens: 500,
        temperature: 0.7,
        do_sample: true,
        repetition_penalty: 1.1,
        return_full_text: false
      }
    })

    return response.generated_text.trim()
  } catch (error) {
    console.error('Hugging Face API error:', error)
    throw new Error('AI response generation failed')
  }
}

/**
 * Alias for generateResponse for backward compatibility
 */
export const generateChatResponse = generateResponse

/**
 * Answer questions based on document context
 */
export async function answerQuestion(
  question: string,
  context: string
): Promise<string> {
  try {
    const response = await hf.questionAnswering({
      model: MODELS.QA,
      inputs: {
        question,
        context
      }
    })

    return response.answer || 'Sorry, I could not find an answer in the provided context.'
  } catch (error) {
    console.error('Question answering error:', error)
    
    // Fallback to text generation
    return generateResponse(question, context)
  }
}

/**
 * Summarize document content
 */
export async function summarizeText(text: string): Promise<string> {
  try {
    // Split long text into chunks if needed
    const maxLength = 1024
    if (text.length > maxLength) {
      const chunks = []
      for (let i = 0; i < text.length; i += maxLength) {
        chunks.push(text.slice(i, i + maxLength))
      }
      
      const summaries = await Promise.all(
        chunks.map(chunk => hf.summarization({
          model: MODELS.SUMMARIZATION,
          inputs: chunk,
          parameters: {
            max_length: 150,
            min_length: 30
          }
        }))
      )
      
      return summaries.map(s => s.summary_text).join(' ')
    }

    const response = await hf.summarization({
      model: MODELS.SUMMARIZATION,
      inputs: text,
      parameters: {
        max_length: 150,
        min_length: 30
      }
    })

    return response.summary_text
  } catch (error) {
    console.error('Summarization error:', error)
    return 'Sorry, I could not summarize the text.'
  }
}

/**
 * Enhanced ChatGPT-like response generation with conversation memory
 */
export async function generateAdvancedChatResponse(
  message: string,
  conversationMemory: ConversationMemory,
  documentContext?: string
): Promise<string> {
  try {
    // Get conversation history
    const conversationHistory = conversationMemory.getContext()
    const recentDocContext = documentContext || conversationMemory.getRecentDocumentContext()
    
    // Create enhanced prompt with system instructions
    let prompt = `ä»¥ä¸‹ã¯ã€ä¼æ¥­æ–‡æ›¸ç®¡ç†AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ä¼šè©±ã§ã™ã€‚AIã¯è¦ªåˆ‡ã§ã€è©³ç´°ã§ã€æ­£ç¢ºãªå›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚`
    
    // Add document context if available
    if (recentDocContext) {
      prompt += `\n\nå‚è€ƒè³‡æ–™:\n${recentDocContext.slice(0, 1500)}`
    }
    
    // Add conversation history
    if (conversationHistory) {
      prompt += `\n\néå»ã®ä¼šè©±:\n${conversationHistory}`
    }
    
    // Add current user message
    prompt += `\nHuman: ${message}\nAssistant:`

    // Call Hugging Face API with improved parameters
    const response = await hf.textGeneration({
      model: MODELS.CHAT,
      inputs: prompt,
      parameters: {
        max_new_tokens: 400,
        temperature: 0.8,
        top_p: 0.95,
        do_sample: true,
        repetition_penalty: 1.15,
        return_full_text: false,
        stop: ['Human:', 'Assistant:', '\n\n']
      }
    })

    let generatedText = response.generated_text.trim()
    
    // Post-process the response
    generatedText = enhanceResponseQuality(generatedText, message, recentDocContext)
    
    // Add to conversation memory
    conversationMemory.addTurn('user', message, documentContext)
    conversationMemory.addTurn('assistant', generatedText, documentContext)
    
    return generatedText

  } catch (error) {
    console.error('Enhanced chat response error:', error)
    
    // Intelligent fallback based on message content
    const fallbackResponse = await generateIntelligentFallback(message, documentContext || '', conversationMemory)
    
    // Still add to memory even for fallback
    conversationMemory.addTurn('user', message, documentContext)
    conversationMemory.addTurn('assistant', fallbackResponse, documentContext)
    
    return fallbackResponse
  }
}

/**
 * Enhance response quality with post-processing
 */
function enhanceResponseQuality(response: string, originalQuestion: string, documentContext?: string): string {
  let enhanced = response
  
  // Remove incomplete sentences at the end
  const sentences = enhanced.split(/[ã€‚ï¼ï¼Ÿ]/)
  if (sentences.length > 1 && sentences[sentences.length - 1].trim().length < 10) {
    enhanced = sentences.slice(0, -1).join('ã€‚') + 'ã€‚'
  }
  
  // Ensure response is relevant to the question
  if (enhanced.length < 20) {
    enhanced = generateContextAwareResponse(originalQuestion, documentContext)
  }
  
  // Add helpful formatting for document-based responses
  if (documentContext && !enhanced.includes('ğŸ“„')) {
    enhanced = `ğŸ“„ **æ–‡æ›¸ãƒ™ãƒ¼ã‚¹å›ç­”**\n\n${enhanced}`
  }
  
  return enhanced
}

/**
 * Generate context-aware response for better fallback
 */
function generateContextAwareResponse(question: string, documentContext?: string): string {
  const questionLower = question.toLowerCase()
  
  // Analyze question intent
  if (questionLower.includes('æ•™ãˆã¦') || questionLower.includes('èª¬æ˜')) {
    if (documentContext) {
      return `ã”è³ªå•ã€Œ${question}ã€ã«ã¤ã„ã¦ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸè³‡æ–™ã‚’å‚è€ƒã«å›ç­”ã„ãŸã—ã¾ã™ã€‚å…·ä½“çš„ã«ãŠçŸ¥ã‚Šã«ãªã‚ŠãŸã„ç‚¹ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ã‚ˆã‚Šè©³ã—ããŠèã‹ã›ãã ã•ã„ã€‚`
    } else {
      return `ã€Œ${question}ã€ã«ã¤ã„ã¦å›ç­”ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚é–¢é€£ã™ã‚‹è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ãŸã ãã¨ã€ã‚ˆã‚Šè©³ç´°ã§æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã§ãã¾ã™ã€‚`
    }
  }
  
  if (questionLower.includes('æ–¹æ³•') || questionLower.includes('ã‚„ã‚Šæ–¹')) {
    return `ã€Œ${question}ã€ã®æ‰‹é †ã«ã¤ã„ã¦èª¬æ˜ã„ãŸã—ã¾ã™ã€‚æ®µéšçš„ãªæ‰‹é †ã‚„å…·ä½“çš„ãªæ–¹æ³•ã‚’ãŠçŸ¥ã‚Šã«ãªã‚ŠãŸã„å ´åˆã¯ã€ã‚ˆã‚Šè©³ã—ããŠèã‹ã›ãã ã•ã„ã€‚`
  }
  
  // Default response
  return `ã€Œ${question}ã€ã«ã¤ã„ã¦æ‰¿ã‚Šã¾ã—ãŸã€‚ã‚ˆã‚Šå…·ä½“çš„ãªæƒ…å ±ã‚„è©³ç´°ã‚’ãŠæ±‚ã‚ã®å ´åˆã¯ã€é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ãŸã ãã‹ã€å…·ä½“çš„ãªè³ªå•ã‚’ãŠèã‹ã›ãã ã•ã„ã€‚`
}

/**
 * Generate intelligent document-based response
 */
export async function generateDocumentBasedResponse(
  question: string,
  documentContext: string,
  documentTitles: string[]
): Promise<string> {
  // First try Hugging Face API
  try {
    // Try question answering first
    const qaResponse = await answerQuestion(question, documentContext)
    
    if (qaResponse && !qaResponse.includes('Sorry, I could not find')) {
      // Enhance the QA response with context
      return enhanceResponseWithContext(qaResponse, question, documentTitles)
    }
  } catch (error) {
    console.log('QA model failed, trying text generation...')
  }

  // Fallback to intelligent analysis of document content
  const conversationMemory = new ConversationMemory()
  return generateIntelligentFallback(question, documentContext, conversationMemory)
}

/**
 * Enhance QA response with contextual information
 */
function enhanceResponseWithContext(
  answer: string, 
  question: string, 
  documentTitles: string[]
): string {
  return `ğŸ“„ **è³‡æ–™ãƒ™ãƒ¼ã‚¹å›ç­”**

**ã”è³ªå•**: ${question}

**å›ç­”**: ${answer}

**å‚ç…§è³‡æ–™**: ${documentTitles.join(', ')}

ğŸ’¡ ã“ã®å›ç­”ã¯ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸè³‡æ–™ã®å†…å®¹ã‚’åˆ†æã—ã¦ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ãŒå¿…è¦ã§ã—ãŸã‚‰ã€å…·ä½“çš„ãªè³ªå•ã‚’ãŠèã‹ã›ãã ã•ã„ã€‚`
}

/**
 * Generate intelligent response based on document analysis
 */
function generateIntelligentFallback(
  question: string,
  documentContext: string,
  conversationMemory: ConversationMemory
): Promise<string> {
  return new Promise((resolve) => {
    // Extract document titles from conversation memory or use generic titles
    const documentTitles = ['ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿è³‡æ–™']
    
    // Analyze document content for relevant information
    const contentAnalysis = analyzeDocumentContent(documentContext, question)
    
    // Try to provide more intelligent responses based on keywords
    const questionLower = question.toLowerCase()
    let intelligentResponse = ''
    
    // Check for specific topics based on question content
    if (questionLower.includes('ã‚°ãƒ¬ãƒ¼ãƒ‰') || questionLower.includes('grade')) {
      intelligentResponse = `
**ğŸ“Š ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¶åº¦ã«ã¤ã„ã¦**:
ã“ã®è³‡æ–™ã«ã¯ã€ã‚¹ãƒ”ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã‚¸ãƒ£ãƒ‘ãƒ³ã®ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¶åº¦ï¼ˆSLGï¼‰ã«é–¢ã™ã‚‹è©³ç´°ãªæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

**ä¸»è¦ãªã‚°ãƒ¬ãƒ¼ãƒ‰æ§‹æˆ**:
â€¢ STEP1: Rookieãƒ»Associate
â€¢ STEP2: Sub Leaderï½Sub Manager  
â€¢ STEP3: Managerï½

**åˆ¶åº¦ã®ç‰¹å¾´**:
â€¢ å„ã‚°ãƒ¬ãƒ¼ãƒ‰ã«ã¯æ˜ç¢ºãªæ˜‡æ ¼æ¡ä»¶ãŒè¨­å®š
â€¢ ã‚°ãƒ¬ãƒ¼ãƒ‰æ‰‹å½“ã«ã‚ˆã‚‹å ±é…¬ä½“ç³»
â€¢ ãƒŸãƒƒã‚·ãƒ§ãƒ³é”æˆã«ã‚ˆã‚‹æ˜‡é€²ã‚·ã‚¹ãƒ†ãƒ 
â€¢ å®šæœŸçš„ãªè©•ä¾¡é¢è«‡ã«ã‚ˆã‚‹é€²æ—ç¢ºèª

è©³ç´°ãªæ˜‡æ ¼æ¡ä»¶ã‚„å ±é…¬ä½“ç³»ã«ã¤ã„ã¦ã¯ã€è³‡æ–™å†…ã®è©²å½“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã”ç¢ºèªãã ã•ã„ã€‚`

    } else if (questionLower.includes('è©•ä¾¡') || questionLower.includes('evaluation')) {
      intelligentResponse = `
**ğŸ“‹ è©•ä¾¡åˆ¶åº¦ã«ã¤ã„ã¦**:
ã“ã®è³‡æ–™ã§ã¯ã€ã‚¹ãƒ”ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã‚¸ãƒ£ãƒ‘ãƒ³ã®åŒ…æ‹¬çš„ãªè©•ä¾¡åˆ¶åº¦ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚

**è©•ä¾¡åˆ¶åº¦ã®è¦ç´ **:
â€¢ è©•ä¾¡åŸºæº–ã®è©³ç´°å®šç¾©
â€¢ è©•ä¾¡ä½“åˆ¶ï¼ˆè©•ä¾¡è€…ãƒ»è©•ä¾¡é¢è«‡ï¼‰
â€¢ ãƒŸãƒƒã‚·ãƒ§ãƒ³é”æˆã®åˆ¤æ–­åŸºæº–
â€¢ è©•ä¾¡çµæœã®å ±é…¬ã¸ã®åæ˜ æ–¹æ³•

**è©•ä¾¡ãƒ—ãƒ­ã‚»ã‚¹**:
â€¢ å®šæœŸçš„ãªè©•ä¾¡é¢è«‡ã®å®Ÿæ–½
â€¢ ãƒ¡ãƒ€ãƒ«ã‚·ãƒ¼ãƒˆã«ã‚ˆã‚‹ç›®æ¨™è¨­å®šãƒ»æŒ¯ã‚Šè¿”ã‚Š
â€¢ å˜ä¾¡UPãƒŸãƒƒã‚·ãƒ§ãƒ³ã®é€²æ—ç®¡ç†
â€¢ è³‡æ ¼å–å¾—ã«ã‚ˆã‚‹èƒ½åŠ›è©•ä¾¡

ã‚ˆã‚Šå…·ä½“çš„ãªè©•ä¾¡åŸºæº–ã‚„æ‰‹é †ã«ã¤ã„ã¦ã¯ã€è³‡æ–™ã®è©³ç´°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã”å‚ç…§ãã ã•ã„ã€‚`

    } else if (questionLower.includes('æ˜‡æ ¼') || questionLower.includes('æ˜‡é€²')) {
      intelligentResponse = `
**ğŸ¯ æ˜‡æ ¼ãƒ»æ˜‡é€²ã«ã¤ã„ã¦**:
è³‡æ–™ã§ã¯ã€æ˜ç¢ºãªæ˜‡æ ¼æ¡ä»¶ã¨æ˜‡é€²ãƒ—ãƒ­ã‚»ã‚¹ãŒå®šã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

**æ˜‡æ ¼ã®æ¡ä»¶**:
â€¢ å„ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã§ã®å¿…è¦ã‚¹ã‚­ãƒ«ç²å¾—
â€¢ ãƒŸãƒƒã‚·ãƒ§ãƒ³é”æˆçŠ¶æ³
â€¢ è©•ä¾¡é¢è«‡ã§ã®ç·åˆåˆ¤æ–­
â€¢ è³‡æ ¼å–å¾—çŠ¶æ³

æ˜‡æ ¼ã«é–¢ã™ã‚‹è©³ç´°ãªè¦ä»¶ã¯ã€å„ã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®èª¬æ˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ç¢ºèªã§ãã¾ã™ã€‚`
    }
    
    const response = `ğŸ“„ **è³‡æ–™åˆ†æçµæœ**

**ã”è³ªå•**: ${question}

**é–¢é€£è³‡æ–™**: ${documentTitles.join(', ')}

${intelligentResponse}

**åˆ†æå†…å®¹**:
${contentAnalysis}

ğŸ” **è©³ç´°æƒ…å ±**:
ã“ã®å›ç­”ã¯ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸè³‡æ–™ï¼ˆ${documentTitles.length}ä»¶ï¼‰ã®å†…å®¹ã‚’åˆ†æã—ã¦ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã™ã€‚

ğŸ’¡ **ã•ã‚‰ã«è©³ã—ãçŸ¥ã‚ŠãŸã„å ´åˆ**:
- ã‚ˆã‚Šå…·ä½“çš„ãªè³ªå•ã‚’ã—ã¦ã„ãŸã ãã¨ã€è©²å½“éƒ¨åˆ†ã‚’è©³ç´°ã«åˆ†æã§ãã¾ã™
- ç‰¹å®šã®é …ç›®ã‚„æ•°å€¤ã«ã¤ã„ã¦ãŠèããã ã•ã„
- è¦ç´„ã‚„æ¯”è¼ƒåˆ†æã‚‚å¯èƒ½ã§ã™

**åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½**: å†…å®¹è¦ç´„ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã€ãƒ‡ãƒ¼ã‚¿åˆ†æã€æ¯”è¼ƒæ¤œè¨`

    resolve(response)
  })
}

/**
 * Analyze document content for relevant information
 */
function analyzeDocumentContent(content: string, question: string): string {
  if (!content || content.trim().length === 0) {
    return "è³‡æ–™ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚„å†…å®¹ã‚’ã”ç¢ºèªãã ã•ã„ã€‚"
  }

  // Simple keyword matching and content analysis
  const questionKeywords = extractKeywords(question)
  const contentLines = content.split('\n').filter(line => line.trim().length > 0)
  
  let relevantContent: string[] = []
  let statistics = {
    totalLines: contentLines.length,
    totalChars: content.length,
    hasNumbers: /\d/.test(content),
    hasDate: /\d{4}[/-]\d{1,2}[/-]\d{1,2}|\d{1,2}[/-]\d{1,2}[/-]\d{4}/.test(content)
  }

  // Find relevant content based on keywords
  let foundMatches = false
  questionKeywords.forEach(keyword => {
    const matches = contentLines.filter(line => 
      line.toLowerCase().includes(keyword.toLowerCase())
    )
    
    if (matches.length > 0) {
      foundMatches = true
      relevantContent.push(`**ã€Œ${keyword}ã€ã«é–¢é€£ã™ã‚‹å†…å®¹**:`)
      matches.slice(0, 5).forEach(match => {
        const cleanMatch = match.trim()
        if (cleanMatch.length > 0) {
          relevantContent.push(`â€¢ ${cleanMatch.slice(0, 200)}${cleanMatch.length > 200 ? '...' : ''}`)
        }
      })
      relevantContent.push('') // Add spacing between sections
    }
  })

  if (!foundMatches) {
    // If no keyword matches, provide general document overview
    relevantContent = [
      `**æ–‡æ›¸ã®æ¦‚è¦**:`,
      `â€¢ æ–‡æ›¸ã‚µã‚¤ã‚º: ${statistics.totalLines}è¡Œã€${statistics.totalChars}æ–‡å­—`,
      `â€¢ æ•°å€¤ãƒ‡ãƒ¼ã‚¿: ${statistics.hasNumbers ? 'å«ã¾ã‚Œã¦ã„ã¾ã™' : 'å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“'}`,
      `â€¢ æ—¥ä»˜æƒ…å ±: ${statistics.hasDate ? 'å«ã¾ã‚Œã¦ã„ã¾ã™' : 'å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“'}`,
      '',
      `**å†…å®¹ã®ä¸€éƒ¨**:`,
      ...contentLines.slice(0, 10).map(line => {
        const cleanLine = line.trim()
        if (cleanLine.length > 0) {
          return `â€¢ ${cleanLine.slice(0, 150)}${cleanLine.length > 150 ? '...' : ''}`
        }
        return null
      }).filter((item): item is string => item !== null).slice(0, 5)
    ]
  }

  return relevantContent.join('\n')
}

/**
 * Extract keywords from question
 */
function extractKeywords(question: string): string[] {
  // Remove common Japanese particles and extract meaningful words
  const stopWords = ['ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã§', 'ã¨', 'ã®', 'ã‹ã‚‰', 'ã¾ã§', 'ã«ã¤ã„ã¦', 'ã¨ã„ã†', 'ã§ã™', 'ã¾ã™', 'ã§ã‚ã‚‹', 'ã©ã®ã‚ˆã†ãª', 'ã©ã‚“ãª', 'ãªã«', 'ãªãœ', 'ã„ã¤', 'ã©ã“', 'ã ã‚Œ', 'ã©ã†ã‚„ã£ã¦']
  
  // Split by various separators and clean up
  const words = question
    .replace(/[ï¼Ÿï¼ã€‚ã€,ï¼Œ]/g, ' ')
    .split(/[\s\u3000]+/)
    .filter(word => word.length > 1 && !stopWords.includes(word))
    .map(word => word.replace(/[ï¼Ÿï¼ã€‚ã€]/g, ''))
    .filter(word => word.length > 0)
  
  return Array.from(new Set(words)).slice(0, 5) // Remove duplicates and limit to 5 keywords
}

/**
 * Check if Hugging Face API is available
 */
export async function checkHuggingFaceConnection(): Promise<boolean> {
  try {
    // Try with a simpler model
    await hf.textGeneration({
      model: 'gpt2',
      inputs: 'Hello',
      parameters: { max_new_tokens: 5 }
    })
    return true
  } catch (error) {
    console.error('Hugging Face connection check failed:', error)
    return false
  }
}

/**
 * Enhanced conversation management with streaming support
 */
export class EnhancedConversationMemory extends ConversationMemory {
  private contextSummary: string = ''
  
  constructor(maxTurns: number = 15, maxTokens: number = 3000) {
    super(maxTurns, maxTokens)
  }

  /**
   * Get optimized context for ChatGPT-like responses
   */
  getChatGPTContext(): string {
    const recentContext = this.getContext()
    const summary = this.getSummary()
    
    if (this.contextSummary && recentContext.length < 500) {
      return `éå»ã®ä¼šè©±æ¦‚è¦: ${this.contextSummary}\n\næœ€è¿‘ã®ä¼šè©±:\n${recentContext}`
    }
    
    return recentContext
  }

  /**
   * Update context summary for long conversations
   */
  updateContextSummary(summary: string) {
    this.contextSummary = summary
  }

  /**
   * Analyze conversation patterns for better responses
   */
  getConversationPatterns(): { 
    frequentTopics: string[], 
    userPreferences: string[], 
    conversationStyle: string 
  } {
    const allUserMessages = this.history
      .filter(turn => turn.role === 'user')
      .map(turn => turn.content.toLowerCase())
    
    // Simple keyword extraction for topics
    const words = allUserMessages.join(' ').split(/\s+/)
    const wordCounts = words.reduce((acc, word) => {
      if (word.length > 3) {
        acc[word] = (acc[word] || 0) + 1
      }
      return acc
    }, {} as Record<string, number>)
    
    const frequentTopics = Object.entries(wordCounts)
      .sort(([,a], [,b]) => b - a)
      .slice(0, 5)
      .map(([word]) => word)
    
    // Determine conversation style
    const hasPoliteLanguage = allUserMessages.some(msg => 
      msg.includes('ãŠé¡˜ã„') || msg.includes('ãã ã•ã„') || msg.includes('ã‚ã‚ŠãŒã¨ã†')
    )
    
    const hasInformalLanguage = allUserMessages.some(msg =>
      msg.includes('ã ã‚ˆã­') || msg.includes('ã£ã¦ã„ã†') || msg.includes('ã‚„ã°ã„')
    )
    
    let conversationStyle = 'neutral'
    if (hasPoliteLanguage && !hasInformalLanguage) conversationStyle = 'formal'
    if (hasInformalLanguage && !hasPoliteLanguage) conversationStyle = 'casual'
    
    return {
      frequentTopics,
      userPreferences: [], // Could be enhanced based on user feedback
      conversationStyle
    }
  }
}

/**
 * ChatGPT-like streaming response generation
 */
export async function* generateStreamingResponse(
  message: string,
  conversationMemory: EnhancedConversationMemory,
  documentContext?: string
): AsyncGenerator<string, void, unknown> {
  try {
    // Get conversation patterns for personalized responses
    const patterns = conversationMemory.getConversationPatterns()
    const chatContext = conversationMemory.getChatGPTContext()
    
    // Create enhanced prompt
    let systemPrompt = `ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚`
    
    // Adapt response style based on conversation patterns
    switch (patterns.conversationStyle) {
      case 'formal':
        systemPrompt += `ä¸å¯§ã§æ ¼å¼ã‚ã‚‹æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`
        break
      case 'casual':
        systemPrompt += `ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§è¦ªã—ã¿ã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`
        break
      default:
        systemPrompt += `è‡ªç„¶ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`
    }
    
    if (documentContext) {
      systemPrompt += `\n\nå‚è€ƒè³‡æ–™:\n${documentContext.slice(0, 2000)}`
    }
    
    if (chatContext) {
      systemPrompt += `\n\nä¼šè©±å±¥æ­´:\n${chatContext}`
    }
    
    const fullPrompt = `${systemPrompt}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼: ${message}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:`
    
    // Use Hugging Face streaming if available
    const response = await hf.textGeneration({
      model: MODELS.CHAT,
      inputs: fullPrompt,
      parameters: {
        max_new_tokens: 500,
        temperature: 0.7,
        top_p: 0.9,
        do_sample: true,
        repetition_penalty: 1.1,
        return_full_text: false,
        stop: ['ãƒ¦ãƒ¼ã‚¶ãƒ¼:', 'User:', 'Human:']
      }
    })
    
    // Simulate streaming by yielding chunks of the response
    const fullResponse = response.generated_text.trim()
    const chunks = fullResponse.split(/([ã€‚ï¼ï¼Ÿ])/g)
    
    let accumulatedResponse = ''
    for (const chunk of chunks) {
      if (chunk.trim()) {
        accumulatedResponse += chunk
        yield accumulatedResponse
        
        // Add small delay to simulate real streaming
        await new Promise(resolve => setTimeout(resolve, 50))
      }
    }
    
    // Add to conversation memory
    conversationMemory.addTurn('user', message, documentContext)
    conversationMemory.addTurn('assistant', fullResponse, documentContext)
    
  } catch (error) {
    console.error('Streaming response error:', error)
    
    // Fallback to non-streaming intelligent response
    const fallbackResponse = await generateIntelligentFallback(message, documentContext || '', conversationMemory)
    yield fallbackResponse
  }
}
